<!DOCTYPE HTML>
<html>

<head>

  <meta charset="utf-8">
  
  <title>Andrew&#39;s Blog</title>
  <meta name="author" content="Andrew">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Andrew&#39;s Blog"/>

  
    <meta property="og:image" content=""/>
  

  
  
    <link href="/favicon.png" rel="icon">
  
  
  <link rel="stylesheet" href="/css/bootstrap.min.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/notebook.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/tabs.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>
  <script src="/js/tabs.js"></script>

  <!-- analytics -->
  



<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML-full,Safe"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    init_mathjax = function() {
        if (window.MathJax) {
        // MathJax loaded
            MathJax.Hub.Config({
                TeX: {
                    equationNumbers: {
                    autoNumber: "AMS",
                    useLabelIds: true
                    }
                },
                tex2jax: {
                    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                    processEscapes: true,
                    processEnvironments: true
                },
                displayAlign: 'center',
                CommonHTML: {
                    linebreaks: {
                    automatic: true
                    }
                }
            });

            MathJax.Hub.Queue(["Typeset", MathJax.Hub]);
        }
    }
    init_mathjax();
    </script>
    <!-- End of mathjax configuration -->

<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <nav id="main-nav" class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
		<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
	  <a class="navbar-brand" href="/">Andrew&#39;s Blog</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 <div class="page-header">
  <h1>Andrew&#39;s Blog</h1>
</div>

<div class="row page">

	
	<div class="col-md-9">
	

		<div class="slogan">
      <i class="fa fa-heart"></i>
      PURSUE ULTIMATE FACTS
</div>    

		<div class="mypage">
		
		<!-- title and entry -->
		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-12 </div>
			<div class="article-title"><a href="/2023/04/12/LangChain-Embeddings/" >LangChain Embeddings</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Open-Text-Embeddings">Open Text Embeddings<a class="anchor-link" href="#Open-Text-Embeddings">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LangChain-Embeddings"><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/reference/modules/embeddings.html">LangChain Embeddings</a><a class="anchor-link" href="#LangChain-Embeddings">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="LLaMA-Embeddings"><a target="_blank" rel="noopener" href="https://huggingface.co/shalomma/llama-7b-embeddings">LLaMA Embeddings</a><a class="anchor-link" href="#LLaMA-Embeddings">&#182;</a></h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>llama-cpp-python
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Successfully installed llama-cpp-python-0.1.32 typing-extensions-4.5.0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><a target="_blank" rel="noopener" href="https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml">GPT4 x Alpaca without LoRA ggml</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>git+https://github.com/huggingface/transformers
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">tokenizers</span><span class="o">==</span><span class="m">0</span>.13.3<span class="w"> </span><span class="nv">protobuf</span><span class="o">==</span><span class="m">3</span>.20.*
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Requirement already satisfied: tokenizers==0.13.3 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (0.13.3)
Requirement already satisfied: protobuf==3.20.* in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (3.20.3)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/welcome">HuggingFace Welcome</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;chavinlo/gpt4-x-alpaca&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;chavinlo/gpt4-x-alpaca&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Downloading (…)l-00001-of-00006.bin:  69%|######9   | 6.91G/9.96G [00:00&lt;?, ?B/s]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/snapshots/6a571f458cab9a23d14324ec63e0abd1744c8353
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>total 0
drwxr-xr-x  14 saintway  staff   448B Apr 14 14:21 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x   3 saintway  staff    96B Apr 12 21:08 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:34 <span class="ansi-magenta-fg">added_tokens.json</span> -&gt; ../../blobs/3e03d5f619baf8592fb936d63d05366f9304f7b2
lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:43 <span class="ansi-magenta-fg">config.json</span> -&gt; ../../blobs/849ee4b803bc92eb21e60c3946d20e4cbc69eefa
lrwxr-xr-x   1 saintway  staff    76B Apr 13 02:27 <span class="ansi-magenta-fg">pytorch_model-00001-of-00006.bin</span> -&gt; ../../blobs/c022dd1d22c5ed2501abdb220f8315e6f51a5197026ed72bdbd2fdbac641d27b
lrwxr-xr-x   1 saintway  staff    76B Apr 13 13:32 <span class="ansi-magenta-fg">pytorch_model-00002-of-00006.bin</span> -&gt; ../../blobs/5481821b5869b58b15c3175e712e41cd6b7b5596557b10aa2c2655a4cf019a7a
lrwxr-xr-x   1 saintway  staff    76B Apr 14 11:53 <span class="ansi-magenta-fg">pytorch_model-00003-of-00006.bin</span> -&gt; ../../blobs/df46de31831a882cd57c9beefdad97e1ae442fe071871bad60223b23c1a08df9
lrwxr-xr-x   1 saintway  staff    76B Apr 14 13:21 <span class="ansi-magenta-fg">pytorch_model-00004-of-00006.bin</span> -&gt; ../../blobs/0e5f42d9943bdbc6e12288733a65d6e337c2cc1a3ff90654cdf96df3f43437ee
lrwxr-xr-x   1 saintway  staff    76B Apr 14 14:06 <span class="ansi-magenta-fg">pytorch_model-00005-of-00006.bin</span> -&gt; ../../blobs/6149b601c773fce7642e3424878c2c8182a221a2723e93d3da10e0f28850d00e
lrwxr-xr-x   1 saintway  staff    76B Apr 14 14:21 <span class="ansi-magenta-fg">pytorch_model-00006-of-00006.bin</span> -&gt; ../../blobs/1b02c47b8a6151783c6ab90a8e5acba320940d2197cff255cf8f23eab10f8180
lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:43 <span class="ansi-magenta-fg">pytorch_model.bin.index.json</span> -&gt; ../../blobs/eb488e9b33396741832583081e6ca45eb6f4de49
lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:34 <span class="ansi-magenta-fg">special_tokens_map.json</span> -&gt; ../../blobs/318f9131477d72be713dcfee9da3a2e43d7ac8ad
lrwxr-xr-x   1 saintway  staff    76B Apr 12 21:34 <span class="ansi-magenta-fg">tokenizer.model</span> -&gt; ../../blobs/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347
lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:08 <span class="ansi-magenta-fg">tokenizer_config.json</span> -&gt; ../../blobs/8edc6b4c1db134f5d717a6a4f271dfa3194f2295
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/blobs
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>total 84439400
drwxr-xr-x  14 saintway  staff   448B Apr 14 18:53 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x   7 saintway  staff   224B Apr 14 14:21 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
-rw-r--r--   1 saintway  staff   9.2G Apr 14 13:21 0e5f42d9943bdbc6e12288733a65d6e337c2cc1a3ff90654cdf96df3f43437ee
-rw-r--r--   1 saintway  staff   2.3G Apr 14 14:21 1b02c47b8a6151783c6ab90a8e5acba320940d2197cff255cf8f23eab10f8180
-rw-r--r--   1 saintway  staff    96B Apr 12 21:34 318f9131477d72be713dcfee9da3a2e43d7ac8ad
-rw-r--r--   1 saintway  staff    20B Apr 12 21:34 3e03d5f619baf8592fb936d63d05366f9304f7b2
-rw-r--r--   1 saintway  staff   4.6G Apr 13 13:32 5481821b5869b58b15c3175e712e41cd6b7b5596557b10aa2c2655a4cf019a7a.incomplete
-rw-r--r--   1 saintway  staff   9.2G Apr 14 14:06 6149b601c773fce7642e3424878c2c8182a221a2723e93d3da10e0f28850d00e
-rw-r--r--   1 saintway  staff   535B Apr 12 21:43 849ee4b803bc92eb21e60c3946d20e4cbc69eefa
-rw-r--r--   1 saintway  staff   329B Apr 12 21:08 8edc6b4c1db134f5d717a6a4f271dfa3194f2295
-rw-r--r--   1 saintway  staff   488K Apr 12 21:34 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347
-rw-r--r--   1 saintway  staff   5.7G Apr 13 02:27 c022dd1d22c5ed2501abdb220f8315e6f51a5197026ed72bdbd2fdbac641d27b.incomplete
-rw-r--r--   1 saintway  staff   9.3G Apr 14 11:53 df46de31831a882cd57c9beefdad97e1ae442fe071871bad60223b23c1a08df9
-rw-r--r--   1 saintway  staff    33K Apr 12 21:43 eb488e9b33396741832583081e6ca45eb6f4de49
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">LlamaCppEmbeddings</span>
<span class="n">alpaca_embeddings</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/snapshots/6a571f458cab9a23d14324ec63e0abd1744c8353/model.bin&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 准备文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;这是一个测试文档。&#39;</span>

<span class="c1"># 使用 HuggingFaceEmbeddings 生成文本嵌入</span>
<span class="n">query_result</span> <span class="o">=</span> <span class="n">alpaca_embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">doc_result</span> <span class="o">=</span> <span class="n">alpaca_embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span><span class="n">text</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">query_result</span><span class="p">))</span>
<span class="c1"># print(query_result)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># print(doc_result)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/huggingface_hub/guides/download">Download files from the HuggingFace Hub</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s1">&#39;Pi3141/gpt4-x-alpaca-native-13B-ggml&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;consolidated.00.pth&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9/consolidated.00.pth&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 用 Python 去掉文件中最后一个字节</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/blobs/8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55.incomplete&#39;</span><span class="p">),</span> <span class="s1">&#39;rb+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">filehandle</span><span class="p">:</span>
    <span class="n">filehandle</span><span class="o">.</span><span class="n">seek</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">SEEK_END</span><span class="p">)</span>
    <span class="n">filehandle</span><span class="o">.</span><span class="n">truncate</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s1">&#39;Pi3141/gpt4-x-alpaca-native-13B-ggml&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;ggml-model-q4_1.bin&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9/ggml-model-q4_1.bin&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">snapshot_download</span>
<span class="n">snapshot_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s1">&#39;Pi3141/gpt4-x-alpaca-native-13B-ggml&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Fetching 4 files:   0%|          | 0/4 [00:00&lt;?, ?it/s]</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>total 0
drwxr-xr-x  6 saintway  staff   192B Apr 14 11:20 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x  3 saintway  staff    96B Apr 12 23:05 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
lrwxr-xr-x  1 saintway  staff    52B Apr 12 23:05 <span class="ansi-magenta-fg">.gitattributes</span> -&gt; ../../blobs/c7d9f3332a950355d5a77d85000f05e6f45435ea
lrwxr-xr-x  1 saintway  staff    52B Apr 12 23:05 <span class="ansi-magenta-fg">README.md</span> -&gt; ../../blobs/03dbe88acfdc7f800acf2423960468e1c852c9ba
lrwxr-xr-x  1 saintway  staff    76B Apr 14 11:20 <span class="ansi-magenta-fg">consolidated.00.pth</span> -&gt; ../../blobs/fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499
lrwxr-xr-x  1 saintway  staff    76B Apr 13 09:49 <span class="ansi-magenta-fg">ggml-model-q4_1.bin</span> -&gt; ../../blobs/8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/blobs
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>total 90434208
drwxr-xr-x  9 saintway  staff   288B Apr 14 11:20 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x  6 saintway  staff   192B Apr 14 11:20 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
-rw-r--r--  1 saintway  staff   254B Apr 13 09:53 03dbe88acfdc7f800acf2423960468e1c852c9ba
-rw-r--r--@ 1 saintway  staff   9.1G Apr 13 09:53 8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55
-rw-r--r--  1 saintway  staff   2.1G Apr 13 09:53 8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55.incomplete.retry
-rw-r--r--  1 saintway  staff   1.4K Apr 13 09:53 c7d9f3332a950355d5a77d85000f05e6f45435ea
-rw-r--r--  1 saintway  staff    24G Apr 14 11:20 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499
-rw-------  1 saintway  staff   6.2G Apr 13 09:53 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499.incomplete.retry
-rw-r--r--  1 saintway  staff   1.5G Apr 13 09:53 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499.incomplete.tempfile
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">LlamaCppEmbeddings</span>
<span class="n">alpaca_embeddings</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/ggml-model-q4_1.bin&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  =  800.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 准备文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;这是一个测试文档。&#39;</span>

<span class="c1"># 使用 HuggingFaceInstructEmbeddings 生成文本嵌入</span>
<span class="n">query_result</span> <span class="o">=</span> <span class="n">alpaca_embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">doc_result</span> <span class="o">=</span> <span class="n">alpaca_embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span><span class="n">text</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">query_result</span><span class="p">))</span>
<span class="c1"># print(query_result)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># print(doc_result)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 15205.17 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 28693.65 ms /    12 tokens ( 2391.14 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 28701.20 ms
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>5120
1
5120
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 15205.17 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time =  3616.78 ms /    12 tokens (  301.40 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time =  3628.49 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://sj-langchain.readthedocs.io/en/latest/ecosystem/huggingface.html">LangChain with Hugging Face</a><ul>
<li><a target="_blank" rel="noopener" href="https://sj-langchain.readthedocs.io/en/latest/modules/llms/integrations/huggingface_hub.html">LLM from Hugging Face Hub</a></li>
<li><a target="_blank" rel="noopener" href="https://sj-langchain.readthedocs.io/en/latest/modules/indexes/examples/textsplitter.html">LangChain Text Splitter</a></li>
<li><a target="_blank" rel="noopener" href="https://sj-langchain.readthedocs.io/en/latest/use_cases/evaluation/huggingface_datasets.html">Evaluate Models using HuggingFace Datasets</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span><span class="p">,</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="c1"># (1) Import a series of documents.</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span> <span class="n">silent_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">raw_documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="c1"># (2) Split them into small chunks.</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>26</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>Document(page_content=&#39;title: Neuroscience\ndate: 2021-10-14 16:30:20\ntags: Neuro\n---\n\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\n\n---\n\n&gt; \u3000\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\n\u3000\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\n\u3000\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。&#39;, metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>Document(page_content=&#39;&gt; ² =&gt;\n\u3000\u3000有关文献包括：\n* Moore, Philosophical Studies (1922)\n* Grossmann, &#34;Are current concepts and methods in neuroscience inadequate for studying the neural basis of consciousness and mental activity?&#34; (1980)\n* Baars, A Cognitive Theory of Consciousness (1988)\n* Baars, &#34;Surprisingly small subcortical structures are needed for the state of waking consciousness, while cortical projection areas seem to provide perceptual contents of consciousness,&#34; (1995)\n* Bogen, &#34;On the neurophysiology of consciousness: I. An overview,&#34;(1995a)\n* Searle, &#34;The Mystery of Consciousness&#34;, (2000)\n\n&gt; ³ =&gt;\n\u3000\u3000冥想的技巧就在于排除万念而只集中于一个想法、观念或者知觉。这要经过多年的修炼，才能遏制注意力的不断转换（第9章），把注意力长时间集中在一件事上而又不昏昏入睡。由于神经的适应性无时不在，对单件事的觉知会逐渐消退，使得脑中一片空白，主观上没有任何意识内容，但人还是清醒的。\n\n---&#39;, metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>Document(page_content=&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\n&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\n&gt; 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\n&gt; ……&#39;, metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>Document(page_content=&#39;&gt; ……\n&gt; ……诚然，可以按照某种任意的尺度使一些数与心理的量值相配列。但是这些量值并没有就归结为某种别的东西，因而彼此仍然是互不联系的。所以，我们不能说就知道了某种东西的性质或本质。这种情况与我们上面谈到的物理学上的例子完全相同。如果我们对“温度”本身的测量仅仅依据任意的标尺来对数进行配列，那么我们仍然没有知道“温度”的性质。但是热力学理论引入分子的平均运动能量来代替温度，同时也就为排除任何任意因素的量的处理方法提供了自然的原则。只有当量的关系不是单纯地反映一种任意的约定，而是从事物的本性中产生并且从事物本性中觉察到的时候，这种关系才真的是代表了一种**本质**的知识。正像温度在这里归结为力学的规定性，同样，意识的材料如果要真正地被认知，一般也必须依据自然的原则归结为物理的规定性。就温度的情况来说（也就是热的客观的质），只有通过物质的分子结构的假设才可能把它归结为力学的规定；同样，对主观的心理的质的知识需要有深入研究大脑过程本质的生理学假设。遗憾的是，这种研究的现状还不容许我们以实现心理学最终目标所需要的精确性来构述这种假设。\nーー《普通认识论》（Ｍ．石里克），31&#39;, metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;})</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">LlamaCppEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.faiss</span> <span class="kn">import</span> <span class="n">FAISS</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">ingest_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)</span>
    <span class="c1"># (3) Create embeddings for each document (using text-embedding-ada-002).</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/ggml-model-q4_1.bin&#39;</span><span class="p">),</span> <span class="n">n_ctx</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">ingest_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  = 3200.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1234747.80 ms /   607 tokens ( 2034.18 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1234897.68 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1014295.96 ms /   384 tokens ( 2641.40 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1014467.79 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 3282788.17 ms /  1245 tokens ( 2636.78 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 3283401.24 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1918145.27 ms /   648 tokens ( 2960.10 ms per token)
llama_print_timings:        eval time = 20809.58 ms /     1 runs   (20809.58 ms per run)
llama_print_timings:       total time = 1939184.53 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 96082.05 ms /    30 tokens ( 3202.73 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 96150.74 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 507646.90 ms /   208 tokens ( 2440.61 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 507789.71 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 276020.03 ms /   115 tokens ( 2400.17 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 276108.72 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2019785.65 ms /   672 tokens ( 3005.63 ms per token)
llama_print_timings:        eval time = 21867.42 ms /     1 runs   (21867.42 ms per run)
llama_print_timings:       total time = 2041848.65 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 3602106.63 ms /  1131 tokens ( 3184.89 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 3602439.90 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 3230592.03 ms /  1040 tokens ( 3106.34 ms per token)
llama_print_timings:        eval time = 22766.44 ms /     1 runs   (22766.44 ms per run)
llama_print_timings:       total time = 3253751.32 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2692559.48 ms /  1530 tokens ( 1759.84 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2692893.27 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2117241.21 ms /  1428 tokens ( 1482.66 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2117414.25 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1295672.21 ms /  1255 tokens ( 1032.41 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1295802.91 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2584795.09 ms /  1406 tokens ( 1838.40 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2585014.00 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2833173.90 ms /  1514 tokens ( 1871.32 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2833446.52 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2497129.27 ms /  1459 tokens ( 1711.53 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2497330.28 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2323893.79 ms /  1448 tokens ( 1604.90 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2324101.06 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 424718.68 ms /   549 tokens (  773.62 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 424798.69 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2024465.75 ms /  1456 tokens ( 1390.43 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2024680.15 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 203939.74 ms /   407 tokens (  501.08 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 204026.72 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2015409.22 ms /  1524 tokens ( 1322.45 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2015592.15 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 147731.28 ms /   397 tokens (  372.12 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 147780.64 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 477176.94 ms /  1092 tokens (  436.98 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 477262.18 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 900018.45 ms /  1332 tokens (  675.69 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 900122.17 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 133290.49 ms /   490 tokens (  272.02 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 133351.61 ms

llama_print_timings:        load time = 12248.83 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 13613.44 ms /    53 tokens (  256.86 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 13634.46 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Save vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_13B_2048.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vectorstore</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_13B_2048.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  = 3200.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;你知道什么？&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get context related to the question from the embedding model</span>
<span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>page_content=&#39;title: Neuroscience\ndate: 2021-10-14 16:30:20\ntags: Neuro\n---\n\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\n\n---\n\n&gt; \u3000\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\n\u3000\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\n\u3000\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;有意注意，是指，对于某次效果的注意。\n无意注意，是指，对于某次非效果的注意。\n\n目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。\n意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。\n\n懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。\n焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。\n\n对于某次功效的目标，联结着，对于此次功效的原因。\n对于某种功效的概括，联结着，对于此种功效的原因。\n\n兴趣，是指，联结着某次快乐的识。\n荒诞，是指，联结着某次乏味的识。\n苦毒，是指，联结着某次痛苦的识。\n\n慾望，是指，对于某次兴趣的表征。\n妄想，是指，对于某次荒诞的表征。？\n苦观，是指，对于某次苦毒的表征。\n\n苦观，分为，记忆苦观、拟构苦观。弊害，…、…\n\n有趣注意，是指，对于某次兴趣的注意。\n无趣注意，是指，对于某次荒诞的注意。\n\n意义，是指，值得的注意。\n神圣，是指，极其丰富的意义。\n积极的态度，是指，充满对于某种意义的信心。\n消极的态度，是指，缺乏对于某种意义的信心。\n积极的注意，导致着，快乐。\n消极的注意，导致着，乏味。\n对于某种意义的怀疑，是指，对于某种意义的信心的减弱。\n对于某种意义的确定，是指，对于某种意义的信心的增强。\n对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。\n对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。\n静思、禅修、祷告，都是，某种定觉练习。\n\n---\n\n&gt; 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。\n(罗马书 8:24-25 吕振中)\n\n&gt; 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。\n(提摩太后书 2:22 吕振中)\n\n向内往最深处去：净心、呼求主名、并且、等待回应。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

page_content=&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\n&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\n&gt; 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\n&gt; ……&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;客体方式，导致着、联结着，主体方式、机体状态\n形体，导致着、联结着，身体、快乐、痛苦\n轻蔑、轻视他人对自己的态度，损害着，羞耻心\n羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？\n羞耻、光荣，重视他人对自己的态度、敬重\n恥辱、傲慢，轻视他人对自己的态度、轻蔑\n羞耻、羞辱，在含义上，有所不同吗？\n单方的轻视、双方的轻视？\n一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。\n狭隘、愚蠢、固执，轻视他人的信念\n开明、智慧、变通，重视他人的信念&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time =  6111.23 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time =  6109.85 ms /     8 tokens (  763.73 ms per token)
llama_print_timings:        eval time = 10089.46 ms /     1 runs   (10089.46 ms per run)
llama_print_timings:       total time = 16205.01 ms
</pre>
</div>
</div>

</div>
</div>

</div>
 




<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Open-Text-Embeddings">Open Text Embeddings<a class="anchor-link" href="#Open-Text-Embeddings">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="LangChain-Embeddings"><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/reference/modules/embeddings.html">LangChain Embeddings</a><a class="anchor-link" href="#LangChain-Embeddings">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Hugging-Face-Embeddings">Hugging Face Embeddings<a class="anchor-link" href="#Hugging-Face-Embeddings">&#182;</a></h4>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>sentence-transformers
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Successfully installed nltk-3.8.1 scikit-learn-1.2.2 scipy-1.10.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 threadpoolctl-3.1.0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceEmbeddings</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">HuggingFaceEmbeddings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">HuggingFaceEmbeddings</span><span class="o">.</span><span class="fm">__init__</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Help on function __init__ in module langchain.embeddings.huggingface:

__init__(self, **kwargs: Any)
    Initialize the sentence_transformer.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">HuggingFaceEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;sentence-transformers/all-mpnet-base-v2&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hf_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceEmbeddings</span><span class="p">()</span>

<span class="c1"># 准备文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;这是一个测试文档。&#39;</span>

<span class="c1"># 使用 HuggingFaceEmbeddings 生成文本嵌入</span>
<span class="n">query_result</span> <span class="o">=</span> <span class="n">hf_embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">doc_result</span> <span class="o">=</span> <span class="n">hf_embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span><span class="n">text</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">query_result</span><span class="p">))</span>
<span class="c1"># print(query_result)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># print(doc_result)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>768
1
768
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hf_embeddings</span><span class="o">.</span><span class="n">model_name</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;sentence-transformers/all-mpnet-base-v2&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
total 865816
drwxr-xr-x  16 saintway  staff   512B Apr 12 14:31 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x   3 saintway  staff    96B Apr 12 14:31 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
-rw-r--r--   1 saintway  staff   1.1K Apr 12 14:28 .gitattributes
drwxr-xr-x   3 saintway  staff    96B Apr 12 14:28 <span class="ansi-cyan-intense-fg ansi-bold">1_Pooling</span>
-rw-r--r--   1 saintway  staff    10K Apr 12 14:28 README.md
-rw-r--r--   1 saintway  staff   571B Apr 12 14:28 config.json
-rw-r--r--   1 saintway  staff   116B Apr 12 14:28 config_sentence_transformers.json
-rw-r--r--   1 saintway  staff    38K Apr 12 14:28 data_config.json
-rw-r--r--   1 saintway  staff   349B Apr 12 14:31 modules.json
-rw-r--r--   1 saintway  staff   418M Apr 12 14:31 pytorch_model.bin
-rw-r--r--   1 saintway  staff    53B Apr 12 14:31 sentence_bert_config.json
-rw-r--r--   1 saintway  staff   239B Apr 12 14:31 special_tokens_map.json
-rw-r--r--   1 saintway  staff   455K Apr 12 14:31 tokenizer.json
-rw-r--r--   1 saintway  staff   363B Apr 12 14:31 tokenizer_config.json
-rw-r--r--   1 saintway  staff    13K Apr 12 14:31 train_script.py
-rw-r--r--   1 saintway  staff   226K Apr 12 14:31 vocab.txt
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
du<span class="w"> </span>-sh<span class="w"> </span>~/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
423M	/Users/saintway/.cache/torch/sentence_transformers/sentence-transformers_all-mpnet-base-v2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Hugging-Face-Instruct-Embeddings"><a target="_blank" rel="noopener" href="https://huggingface.co/datasets/calmgoose/book-embeddings">Hugging Face Instruct Embeddings</a><a class="anchor-link" href="#Hugging-Face-Instruct-Embeddings">&#182;</a></h4><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/UKPLab/sentence-transformers">https://github.com/UKPLab/sentence-transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/HKUNLP/instructor-embedding">https://github.com/HKUNLP/instructor-embedding</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>InstructorEmbedding
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Successfully installed InstructorEmbedding-1.0.0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/basujindal/chatPDF">https://github.com/basujindal/chatPDF</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceInstructEmbeddings</span>
<span class="n">hfi_embeddings</span> <span class="o">=</span> <span class="n">HuggingFaceInstructEmbeddings</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;hkunlp/instructor-large&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>load INSTRUCTOR_Transformer
max_seq_length  512
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hfi_embeddings</span><span class="o">.</span><span class="n">model_name</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;hkunlp/instructor-large&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/torch/sentence_transformers/hkunlp_instructor-large
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
total 2640208
drwxr-xr-x  15 saintway  staff   480B Apr 12 15:19 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x   4 saintway  staff   128B Apr 12 15:19 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
-rw-r--r--   1 saintway  staff   1.4K Apr 12 15:07 .gitattributes
drwxr-xr-x   3 saintway  staff    96B Apr 12 15:07 <span class="ansi-cyan-intense-fg ansi-bold">1_Pooling</span>
drwxr-xr-x   4 saintway  staff   128B Apr 12 15:08 <span class="ansi-cyan-intense-fg ansi-bold">2_Dense</span>
-rw-r--r--   1 saintway  staff    65K Apr 12 15:08 README.md
-rw-r--r--   1 saintway  staff   1.5K Apr 12 15:08 config.json
-rw-r--r--   1 saintway  staff   122B Apr 12 15:08 config_sentence_transformers.json
-rw-r--r--   1 saintway  staff   461B Apr 12 15:19 modules.json
-rw-r--r--   1 saintway  staff   1.2G Apr 12 15:19 pytorch_model.bin
-rw-r--r--   1 saintway  staff    53B Apr 12 15:19 sentence_bert_config.json
-rw-r--r--   1 saintway  staff   2.1K Apr 12 15:19 special_tokens_map.json
-rw-r--r--   1 saintway  staff   773K Apr 12 15:19 spiece.model
-rw-r--r--   1 saintway  staff   2.3M Apr 12 15:19 tokenizer.json
-rw-r--r--   1 saintway  staff   2.4K Apr 12 15:19 tokenizer_config.json
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
du<span class="w"> </span>-sh<span class="w"> </span>~/.cache/torch/sentence_transformers/hkunlp_instructor-large
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
1.3G	/Users/saintway/.cache/torch/sentence_transformers/hkunlp_instructor-large
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 准备文本</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;这是一个测试文档。&#39;</span>

<span class="c1"># 使用 HuggingFaceInstructEmbeddings 生成文本嵌入</span>
<span class="n">query_result</span> <span class="o">=</span> <span class="n">hfi_embeddings</span><span class="o">.</span><span class="n">embed_query</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="n">doc_result</span> <span class="o">=</span> <span class="n">hfi_embeddings</span><span class="o">.</span><span class="n">embed_documents</span><span class="p">([</span><span class="n">text</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">query_result</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">query_result</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">doc_result</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doc_result</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>768
[-0.022137142717838287, -0.019943105056881905, 0.009940845891833305, 0.029961414635181427, 0.0015559268649667501, -0.0010082109365612268, 0.004636477679014206, 0.006970031186938286, -0.039788346737623215, 0.028241422027349472, -1.5192752471193671e-05, -0.008512390777468681, 0.04590446129441261, 0.03056621551513672, -0.030894720926880836, -0.02884022891521454, -0.023664429783821106, -0.010090871714055538, -0.036661747843027115, -0.001970992423593998, 0.05847157910466194, 0.008038687519729137, -0.012776742689311504, 0.05411699786782265, 0.01262636948376894, 0.016430772840976715, -0.04767526313662529, 0.01811787858605385, 0.04832480102777481, -0.0647105798125267, 0.03377210721373558, -0.04854683578014374, -0.040563128888607025, -0.04772289842367172, -0.018774421885609627, 0.020985594019293785, 0.025719504803419113, 0.027344582602381706, 0.026014933362603188, 0.055159278213977814, -0.01577085256576538, 0.01060266699641943, -0.0031603227835148573, -0.039208076894283295, 0.03614024817943573, 0.009471523575484753, -0.025426877662539482, -0.04541698843240738, 0.026563631370663643, -0.03881140425801277, -0.03588118404150009, -0.057559046894311905, -0.007960007525980473, 0.012319786474108696, -0.0029835468158125877, -0.029109695926308632, -0.06043725088238716, 0.03710782155394554, 0.08494839072227478, -0.054077211767435074, -0.03525502607226372, -0.0031806030310690403, -0.09065768867731094, 0.023320553824305534, 0.02501724287867546, -0.05140731483697891, 0.048127785325050354, -0.05498746410012245, 0.029325366020202637, -0.04640709608793259, 0.01205480471253395, -0.047244541347026825, 0.00035423680674284697, -0.09959323704242706, -0.027633335441350937, 0.001402342109940946, 0.02929595485329628, 0.046018004417419434, -0.05788029357790947, 0.042901281267404556, 0.03905021399259567, 0.0020306624937802553, 0.048880625516176224, -0.0019414519192650914, -0.033322807401418686, 0.028527161106467247, -0.005001544952392578, 0.019440239295363426, 0.0041367351077497005, 0.041833482682704926, -0.03431558609008789, -0.0706053078174591, -0.01964596100151539, 0.00529050687327981, -0.004017329774796963, 0.020387377589941025, 0.0496586374938488, 0.006946606561541557, 0.03991807624697685, 0.037570007145404816, 0.03404153883457184, 0.05588211491703987, -0.02905808761715889, 0.03623465821146965, -0.013191419653594494, 0.009090606123209, -0.020825188606977463, -0.02675699256360531, -0.04974988102912903, -0.0004641334235202521, -0.016248611733317375, -0.055453505367040634, -0.014421780593693256, 0.038791216909885406, -0.003007616614922881, -0.05522274225950241, 0.06346995383501053, -0.054133057594299316, -0.06531116366386414, -0.02393488958477974, 0.027049822732806206, 0.021163685247302055, -0.045149073004722595, -0.005699407774955034, -0.0549631305038929, 0.019174829125404358, -0.020559104159474373, 0.0040987106040120125, -0.01622997410595417, 0.003300424898043275, -0.010149079374969006, 0.021996449679136276, 0.041733454912900925, -0.09496010094881058, 0.021827906370162964, 0.014840630814433098, -0.04588484764099121, 0.02394992485642433, 0.016791993752121925, 0.08624919503927231, -0.06361847370862961, -0.03944281488656998, -0.04442731291055679, 0.007785744499415159, -0.023762937635183334, -0.0110867815092206, 0.01138637587428093, -0.05897051468491554, -0.04304634779691696, -0.0173543319106102, 0.06624708324670792, -0.0437123104929924, 0.004276968538761139, 0.07888749241828918, -0.0071301888674497604, 0.024873679503798485, -0.018245670944452286, 0.004486299119889736, 0.00582241453230381, 0.02243458852171898, -0.030916478484869003, 0.049587175250053406, -0.010419673286378384, -0.022187191992998123, -0.0791892409324646, -0.02702951990067959, -0.0035843446385115385, 0.05750065669417381, -0.018682042136788368, -0.030490878969430923, -0.08072890341281891, 0.024044500663876534, 0.05379054695367813, 0.01158835832029581, -0.02660636231303215, 0.03985058143734932, 0.03334967792034149, 0.030472831800580025, -0.02080536261200905, 0.04899463802576065, 0.010174624621868134, -0.015453080646693707, -0.029648398980498314, 0.04518602415919304, 0.0644007995724678, -0.015453620813786983, 0.012724599801003933, 0.02410477213561535, 0.021669277921319008, -0.047304801642894745, -0.0030988911166787148, 0.06250063329935074, -0.037959348410367966, -0.016027355566620827, 0.03403116390109062, -0.0007538921781815588, -0.04373054951429367, 0.024864956736564636, -0.017527837306261063, -0.004101598169654608, -0.0481080487370491, 0.010937296785414219, 0.02215939201414585, 0.042132746428251266, -0.005298169795423746, 0.05001835525035858, -0.03381647542119026, 0.07707470655441284, -0.01247261743992567, 0.015081333927810192, -0.04821961373090744, -0.05602756887674332, 0.002172428648918867, 0.03414832800626755, 0.05385158583521843, 0.03951353579759598, -0.03862477093935013, -0.06857028603553772, 0.05580616369843483, 0.047364167869091034, 0.04966306313872337, 0.00995559711009264, -0.033690739423036575, 0.011581477709114552, 0.035535167902708054, 0.03085923381149769, -0.04816819354891777, -0.03495897352695465, 0.006372313015162945, 0.05013415589928627, -0.029227256774902344, 0.0053755310364067554, -0.019459571689367294, 0.024346565827727318, -0.034451521933078766, -0.0677531510591507, 0.03487487509846687, 0.04172320291399956, 0.010180668905377388, 0.016491739079356194, -0.01668640412390232, 0.03754301741719246, 0.023817863315343857, 0.021770311519503593, 0.02320024184882641, -0.03048897720873356, 0.023136703297495842, -0.019154028967022896, 0.06983145326375961, -0.013741375878453255, 0.03929886966943741, 0.012652753852307796, 0.015791112557053566, 0.007288077380508184, -0.04030032828450203, 0.020244285464286804, 0.0701761543750763, 0.014144702814519405, -0.0366959422826767, 0.034101251512765884, 0.027012642472982407, 0.04800959303975105, 0.07189490646123886, 0.00042301067151129246, 0.04226808249950409, -0.007224685046821833, 0.03213008865714073, 0.03385363519191742, 0.009528609924018383, 0.013251561671495438, 0.025025293231010437, 0.08515191823244095, -0.004974443931132555, -0.01735675148665905, 0.0720532163977623, -0.03935912624001503, 0.004844623617827892, -0.04394184425473213, 0.011392495594918728, -0.03961816802620888, -0.021686410531401634, 0.0632035881280899, -2.6600875571602955e-05, -0.018001483753323555, 0.0002045980654656887, -0.014556610025465488, 0.009118364192545414, 0.025560518726706505, -0.007447301875799894, 0.019972093403339386, -0.015712067484855652, -0.024550966918468475, 0.023652231320738792, -0.0584896020591259, -0.01404705923050642, -0.017441358417272568, -0.040668584406375885, 0.03344985097646713, -0.06545151770114899, -0.0015352212358266115, -0.059208810329437256, -0.034206390380859375, 0.031709667295217514, 0.031845979392528534, 0.017346983775496483, -0.0192494485527277, 0.04217034950852394, -0.04041285440325737, -0.05436360463500023, -0.03852096572518349, -0.04090946912765503, -0.044468097388744354, -0.05580539256334305, 0.04195259511470795, 0.04524538293480873, -0.001072112238034606, 0.05045463517308235, -0.007403041701763868, -0.011037559248507023, -0.0481581874191761, 0.05748680979013443, 0.021998926997184753, -0.0114741837605834, -0.023216141387820244, 0.02764948643743992, 0.0033738191705197096, -0.015397194772958755, -0.04408087953925133, -0.025693349540233612, 0.08350582420825958, 0.08186513930559158, 0.02589094638824463, 0.031298864632844925, 0.022997794672846794, -0.0705040693283081, -0.017929619178175926, -0.0386652797460556, -0.021253539249300957, -0.011709196493029594, -0.04891839995980263, -0.034260012209415436, -0.010942338034510612, -0.019566839560866356, -0.011726225726306438, -0.0073863305151462555, -0.021809587255120277, 0.031185846775770187, 0.036898598074913025, 0.03579287230968475, 0.03630955517292023, -0.008991124108433723, -0.009002245031297207, 0.08667944371700287, 0.0010887794196605682, -0.032698702067136765, -0.06335387378931046, -0.048804596066474915, 0.02329985983669758, 0.005369881168007851, 0.018586451187729836, -0.051516350358724594, 0.04263807460665703, -0.0006810427876189351, -0.001103260088711977, 0.02041923999786377, -0.02518085390329361, 0.007012072950601578, 0.0016074466984719038, 0.010585247538983822, -0.01600584201514721, -0.06097882241010666, 0.006132303737103939, -0.02095993608236313, 0.03893598914146423, -0.05109530687332153, -0.01899784617125988, -0.011300088837742805, 0.009710361249744892, 0.011983739212155342, -0.013006223365664482, -0.04143975302577019, 0.010285450145602226, -0.06305649876594543, -0.03500263765454292, -0.016994699835777283, 0.019823139533400536, 0.010417548008263111, 0.08123686909675598, 0.028307151049375534, -0.00701523432508111, 0.03513439744710922, 0.002012860495597124, 0.05926254391670227, 0.041931524872779846, -0.014769182540476322, 0.03460005670785904, -0.019610265269875526, 0.027883131057024002, 0.013702291995286942, -0.016595499590039253, -0.03288355842232704, -0.06714218854904175, -0.051795538514852524, 0.032413337379693985, -0.013448472134768963, -0.002694027731195092, 0.04136023297905922, -0.059142980724573135, -0.01380129437893629, -0.0022579259239137173, 0.0219892431050539, 0.021225525066256523, -0.04053009673953056, 0.03726652264595032, -0.021588211879134178, 0.02056734822690487, -0.02374904789030552, 0.003405689960345626, -0.03571395203471184, -0.030117657035589218, 0.006589301861822605, 0.04827360436320305, 0.011746781878173351, 0.047028761357069016, 0.07872718572616577, 0.00854527484625578, -0.031543463468551636, 0.046509627252817154, 0.05302605777978897, 0.02241320163011551, -0.05144788697361946, 0.00531784538179636, -0.008528214879333973, -0.010467768646776676, -0.017910946160554886, -0.0448242723941803, 0.003639540169388056, 0.055717598646879196, -0.005372322164475918, -0.01859535463154316, 1.9491570128593594e-05, -0.017520388588309288, -0.02086714655160904, -0.06756243854761124, 0.016911156475543976, 0.020075716078281403, -0.028387082740664482, -0.014232601039111614, 0.06091458722949028, 0.015551713295280933, 0.05874437093734741, 0.011747990734875202, -0.039615631103515625, -0.04280305281281471, 0.029752986505627632, 0.009051498025655746, -0.062423039227724075, -0.03705782815814018, -0.040770962834358215, 0.030891701579093933, 0.030155127868056297, -0.007816113531589508, -0.0807504653930664, 0.017831768840551376, 0.05006144940853119, -0.05361318588256836, 0.062341079115867615, 0.025491494685411453, 0.048408396542072296, 0.029084276407957077, -0.015447879210114479, -0.00034940679324790835, -0.017930610105395317, -0.006148740649223328, -0.009926981292665005, 0.0582754872739315, -0.015238925814628601, -0.03550595045089722, -0.03390232101082802, 0.03024483472108841, 0.006002285983413458, 0.03796408697962761, 0.019458049908280373, -0.031131630763411522, -0.04120856150984764, 0.010978765785694122, 0.0333918035030365, -0.04785642772912979, 0.022198036313056946, 0.04413451626896858, 0.019193124026060104, -0.03626841679215431, -0.031137170270085335, -0.002764541655778885, 0.007364119868725538, 0.06619369983673096, -0.028123170137405396, -0.035441718995571136, -0.042127810418605804, 0.01750461757183075, 0.008765813894569874, -0.046932898461818695, -0.09832718968391418, 0.028079211711883545, -0.001491512986831367, -0.03492061793804169, 0.05971066281199455, 0.02804477885365486, 0.024293120950460434, 0.08744291216135025, -0.008789492771029472, 0.01302084419876337, -0.020782649517059326, 0.04509878158569336, -0.029434096068143845, -0.03194120153784752, 0.01338726095855236, -0.05421733483672142, -0.02326781488955021, -0.041257284581661224, -0.04912155494093895, -0.031217290088534355, 0.006082997191697359, 0.033354438841342926, -0.0216062068939209, 0.010425982065498829, 0.01690390706062317, 0.01642853394150734, 0.0015028663910925388, -0.05256250873208046, -0.008604401722550392, -0.04340226203203201, -0.06464898586273193, -0.04076193645596504, 0.03596508502960205, -0.01954132877290249, 0.02018481120467186, 0.011362768709659576, 0.02293892204761505, 0.015474352054297924, -0.011335867457091808, -0.02486458234488964, 0.026126177981495857, 0.02133898064494133, -0.04850659891963005, -0.045282673090696335, -0.030667219310998917, 0.008212651126086712, 0.01518244668841362, -0.04165206849575043, 0.03790992125868797, 0.02218039147555828, -0.01681869477033615, -0.02027173899114132, 0.009450569748878479, 0.015175608918070793, -0.04803943634033203, -0.06358246505260468, -0.013711309060454369, 0.009996723383665085, 0.040636055171489716, -0.042011044919490814, -0.011027892120182514, 0.02691529504954815, 0.057163577526807785, 0.03753253072500229, 0.022043783217668533, -0.0021431820932775736, 0.04917208105325699, -0.04179786145687103, -0.005483817774802446, -0.08106370270252228, 0.024761244654655457, -0.016964280977845192, 0.03641534224152565, -0.010911267250776291, -0.0011809802381321788, -0.05593414604663849, 0.04463743418455124, -0.04375195503234863, -0.037942975759506226, -0.003339756280183792, 0.014220676384866238, -0.04019850865006447, 0.053015731275081635, -0.028724318370223045, 0.003802355146035552, -0.037122998386621475, 0.030996421352028847, -0.03507940098643303, -0.0007456461898982525, -0.006219014525413513, -0.0005068734171800315, -0.06874105334281921, -0.027668355032801628, -0.015170485712587833, -0.0672307014465332, 0.0110006770119071, 0.04290778934955597, 0.0037627213168889284, 0.01036884170025587, -0.007260350044816732, -0.014498177915811539, 0.008817058056592941, -0.047402523458004, -0.01808319240808487, -0.05033589527010918, -0.028884392231702805, 0.0035344050265848637, 0.03218654543161392, 0.03320618346333504, -0.05054805800318718, -0.0503070168197155, 0.048324212431907654, 0.045269548892974854, 0.006230846047401428, 0.0028933598659932613, 0.03576548025012016, 0.039641764014959335, 0.04235326126217842, 0.00390684325248003, 0.017620764672756195, -0.05768784508109093, 0.005895737558603287, 0.0004468218539841473, -0.006375355180352926, 0.0018062518211081624, -0.01394017692655325, -0.05188938230276108, -0.018782146275043488, 0.09841680526733398, 0.03368517756462097, 0.02949652634561062, -0.02045777440071106, -0.05439259484410286, 0.04644351080060005, 0.02802385576069355, -0.031084785237908363, 0.018647707998752594, 0.015535857528448105, -0.0347856730222702, 0.07113273441791534, 0.02331412024796009, 0.03137088567018509, 0.045221082866191864, 0.01769883558154106, -0.02390470542013645, 0.02507965639233589, -0.03268289566040039, -0.0027856382075697184, 0.03365938365459442, -0.05175941064953804, 0.006154587958008051, -0.033265549689531326, 0.05281004682183266, -0.0404675267636776, 0.0657331719994545, 0.05071451887488365, 0.0020178519189357758, 0.014635175466537476, -0.03720288723707199, -0.010401709005236626, 0.03344612568616867, -0.010997913777828217, 0.06176922470331192, -0.016880199313163757, -0.07420120388269424, 0.04998021200299263, 0.03931588679552078, -0.07584240287542343, 0.023533020168542862, 0.0006756161455996335, 0.02090786024928093, 0.036075837910175323, 0.03659137338399887, -0.05161881819367409, -0.006765023805201054, -0.005993164610117674, 0.013982019387185574, -0.020381171256303787, -0.029386788606643677, 0.04889058321714401, -0.00013371290697250515, -0.0002964060113299638, 0.027174945920705795, -0.009697571396827698, 0.028293093666434288, 0.0374593585729599, -0.04518287256360054, -0.06050867959856987, -0.00014245744387153536, 0.057110074907541275, 0.030268797650933266, -0.013529627583920956, -0.04629375785589218, -0.012579434551298618, 0.018368467688560486, -0.009889695793390274, -0.01691138558089733, 0.03825466334819794, 0.0271073579788208, -0.1041674092411995, -0.014870252460241318, 0.028485944494605064, 0.0070266807451844215, -0.03262393921613693, 0.024559883400797844, 0.0045441146939992905, 0.012745088897645473, 0.021893462166190147, 0.014667946845293045, 0.001110888086259365, -0.06492006778717041, -0.004571379162371159, 0.02366933599114418, -0.015817731618881226, 0.05720985680818558, -0.0345175638794899, 0.018073854967951775, 0.050241053104400635, 0.03106319159269333, 0.0066062589175999165, -0.0469040647149086, -0.027500491589307785, 0.045247793197631836, -0.02852327562868595, 0.040982939302921295, -0.02894440107047558, -0.04443144053220749, -0.03902950510382652, -0.020365344360470772, -0.026738805696368217, 0.05663229525089264, -0.010026874020695686, 0.01433494221419096, 0.011053822934627533, 0.013605833984911442, -0.0018017073161900043, -0.06102275103330612, -0.03922444209456444, -0.024675380438566208, 0.05290922522544861, 0.021725371479988098, -0.01934276521205902, 0.009532574564218521, -0.03275094926357269, -0.03986525163054466, 0.03821403905749321, -0.009230101481080055, -0.04589630663394928, 0.06575308740139008, 0.02526622824370861, -0.018659353256225586, 0.008876781910657883, 0.03926151990890503, -0.05208025500178337, 0.0474589541554451, 0.0033570746891200542, 0.016553008928894997, 0.03175811842083931, 0.0395858995616436, 0.00479521369561553, -0.028426123782992363, -0.04252200201153755, -0.01386924460530281, -0.013864289969205856, 0.0007772607496008277, 0.07288770377635956]
1
768
[[-0.017321424558758736, -0.0290683601051569, 0.02144867181777954, 0.03564919903874397, 0.007519469130784273, -0.0020619011484086514, 0.01159549318253994, 0.0033334267791360617, -0.030980847775936127, 0.028360769152641296, -0.00923326425254345, -0.015223197638988495, 0.045116547495126724, 0.029102467000484467, -0.034634821116924286, -0.02428201586008072, -0.02622298151254654, -0.012027820572257042, -0.033619582653045654, -0.006393300835043192, 0.04940227046608925, -0.005081124138087034, -0.013001572340726852, 0.04863433539867401, 0.01769079640507698, 0.019589051604270935, -0.05099540203809738, 0.02014349400997162, 0.07345512509346008, -0.056142907589673996, 0.017525048926472664, -0.04323125630617142, -0.03591267392039299, -0.04471318796277046, -0.03387963026762009, 0.02250732108950615, 0.0260605551302433, 0.03198987990617752, 0.015925999730825424, 0.055788323283195496, -0.008521814830601215, 0.003774485783651471, -0.004999945871531963, -0.032853759825229645, 0.03646484762430191, 0.010934969410300255, -0.02773832529783249, -0.040138907730579376, 0.03286226838827133, -0.04289257898926735, -0.04034288600087166, -0.05297926440834999, -0.010307238437235355, 0.01675456389784813, 0.002258037682622671, -0.010667218826711178, -0.05834063142538071, 0.03491596132516861, 0.08479320257902145, -0.06158952787518501, -0.032164279371500015, -0.010000646114349365, -0.08898387849330902, 0.02972586452960968, 0.021418822929263115, -0.054490238428115845, 0.0574653334915638, -0.05757038667798042, 0.03782184422016144, -0.047682881355285645, 0.008862263523042202, -0.05072873830795288, 0.004423295613378286, -0.10649670660495758, -0.037948817014694214, -0.003030016552656889, 0.03342246636748314, 0.043445296585559845, -0.05618777126073837, 0.056365132331848145, 0.037387412041425705, 0.002760133007541299, 0.04104584828019142, -0.002229600679129362, -0.030764112249016762, 0.032726626843214035, -0.008634235709905624, 0.02173653617501259, 0.0011450621532276273, 0.04624494910240173, -0.03307458013296127, -0.07201699912548065, -0.016203274950385094, 0.0036587673239409924, -0.012880930677056313, 0.019049828872084618, 0.04802519455552101, 0.012124101631343365, 0.04760000854730606, 0.050861284136772156, 0.038192279636859894, 0.061423029750585556, -0.017844833433628082, 0.026627566665410995, -0.0229574516415596, 0.006922928616404533, -0.03528516739606857, -0.0240724328905344, -0.040312256664037704, -0.0032211754005402327, -0.012484926730394363, -0.0663076713681221, -0.007122499402612448, 0.037984441965818405, 0.0038152653723955154, -0.06272412836551666, 0.0509425513446331, -0.056290652602910995, -0.058687008917331696, -0.010993730276823044, 0.0430663600564003, 0.02154427394270897, -0.041093580424785614, -0.014876669272780418, -0.060619112104177475, 0.024312550202012062, -0.024698927998542786, 0.008390418253839016, -0.019487500190734863, 0.007395193446427584, -0.01375834085047245, 0.011960526928305626, 0.031002424657344818, -0.08624697476625443, 0.024247542023658752, 0.006527060177177191, -0.03665762394666672, 0.02803284116089344, 0.006271459627896547, 0.07319948822259903, -0.05682418495416641, -0.04574257507920265, -0.03672082722187042, 0.015206302516162395, -0.02461088076233864, -0.01586904563009739, 0.013223697431385517, -0.054297711700201035, -0.04598791524767876, -0.006337896920740604, 0.06237014755606651, -0.03651244938373566, 0.01589328609406948, 0.07579127699136734, -0.013197096064686775, 0.026848727837204933, -0.01778201386332512, 0.002971116453409195, 0.008328345604240894, 0.025022976100444794, -0.024580515921115875, 0.034560929983854294, 0.0050260028801858425, -0.02787385880947113, -0.08644828200340271, -0.03169411048293114, 0.005510836839675903, 0.04482191801071167, -0.023682281374931335, -0.0391206219792366, -0.06080467253923416, 0.027954647317528725, 0.03046696074306965, 0.015648001804947853, -0.017990151420235634, 0.04091879352927208, 0.03362458944320679, 0.037195030599832535, -0.0177246555685997, 0.054683029651641846, 0.02061222307384014, -0.025283208116889, -0.028483398258686066, 0.03470484912395477, 0.05267150327563286, -0.010124912485480309, 0.021229730919003487, 0.023600108921527863, 0.014927630312740803, -0.04385334625840187, 0.009330634027719498, 0.06787162274122238, -0.04097283259034157, -0.009420155547559261, 0.036641813814640045, -0.010596320964396, -0.03766893595457077, 0.03683076798915863, -0.028229041025042534, -0.008987233974039555, -0.05227290093898773, 0.03991328924894333, 0.020183095708489418, 0.050611864775419235, -0.004223272204399109, 0.04868282377719879, -0.03727664053440094, 0.07120431214570999, -0.005240161437541246, 0.027111586183309555, -0.05283592268824577, -0.061826545745134354, 0.015926817432045937, 0.02243269421160221, 0.05361457169055939, 0.03672531619668007, -0.027527185156941414, -0.07340686023235321, 0.04527905583381653, 0.04849943891167641, 0.04911305755376816, 0.00674306508153677, -0.04111838340759277, 0.007636277470737696, 0.027770960703492165, 0.03094053640961647, -0.055380359292030334, -0.032657623291015625, 0.018110627308487892, 0.055499594658613205, -0.023379191756248474, 0.006783255375921726, -0.022936547175049782, 0.019551491364836693, -0.03582318127155304, -0.06713026762008667, 0.027420353144407272, 0.03327900543808937, 0.005401282571256161, 0.0005863559781573713, -0.018303031101822853, 0.026252977550029755, 0.02077643573284149, 0.02444145828485489, 0.030188167467713356, -0.03149719163775444, 0.012608859688043594, -0.02100216969847679, 0.08470612019300461, -0.0031517825555056334, 0.033449966460466385, 0.013242308981716633, 0.02110837772488594, 0.014618230983614922, -0.04304170981049538, 0.02197984606027603, 0.06688559055328369, 0.02574421651661396, -0.045794527977705, 0.0373358353972435, 0.03567683696746826, 0.044516921043395996, 0.07325885444879532, 0.007987595163285732, 0.040773455053567886, -0.0027179326862096786, 0.01602841727435589, 0.030576495453715324, 0.013823426328599453, 0.008543203584849834, 0.020804863423109055, 0.08582372218370438, 0.0004137080395594239, -0.01901276409626007, 0.07436161488294601, -0.03382265940308571, 0.007069360930472612, -0.053395360708236694, 0.024815915152430534, -0.0338716134428978, -0.022816475480794907, 0.04400728642940521, -0.020293278619647026, -0.014036102220416069, -0.0034727640450000763, -0.010488650761544704, 0.008831663988530636, 0.0338919535279274, -0.010221480391919613, 0.024804800748825073, -0.004506160970777273, -0.026584165170788765, 0.028500426560640335, -0.06343217194080353, -0.024766407907009125, -0.02698618732392788, -0.04278045892715454, 0.023514093831181526, -0.06462697684764862, 0.00019009616516996175, -0.041659679263830185, -0.02747407555580139, 0.030351657420396805, 0.021620070561766624, 0.02664831466972828, -0.017507608979940414, 0.04152553528547287, -0.04807824641466141, -0.0418708473443985, -0.031889189034700394, -0.04912397265434265, -0.045796893537044525, -0.047752924263477325, 0.037040527909994125, 0.03883790597319603, -0.010969972237944603, 0.052010659128427505, -0.0028239635284990072, 0.010785464197397232, -0.0499294176697731, 0.06617559492588043, 0.01020990964025259, -0.0059270779602229595, -0.008470469154417515, 0.020733419805765152, 0.003880891017615795, -0.03046511486172676, -0.038246043026447296, -0.023029915988445282, 0.08330415189266205, 0.0814347043633461, 0.019334042444825172, 0.01608927547931671, 0.0231016892939806, -0.059429723769426346, -0.017135992646217346, -0.04106093570590019, -0.012111494317650795, -0.007100872695446014, -0.047069329768419266, -0.03065279684960842, -0.015734704211354256, -0.012285885401070118, -0.01509094052016735, -0.006573914550244808, -0.01767726242542267, 0.03516869619488716, 0.03823452070355415, 0.047458309680223465, 0.04796659201383591, 0.0036093818489462137, 0.0051482380367815495, 0.09279636293649673, -0.009012808091938496, -0.029566612094640732, -0.07162266224622726, -0.04701884463429451, 0.018646517768502235, -0.012380776926875114, 0.01323506236076355, -0.05949578434228897, 0.04261186718940735, -0.0020745713263750076, -0.0008975438540801406, 0.027776023373007774, -0.022997431457042694, 0.008152569644153118, -0.003328507300466299, 0.02153967134654522, -0.0015762682305648923, -0.060422927141189575, 0.013903005048632622, -0.020790306851267815, 0.03715454414486885, -0.04409933462738991, -0.027633583173155785, -0.017945939674973488, 0.013620913028717041, 0.002300640568137169, -0.01987757720053196, -0.042235616594552994, 0.010321836918592453, -0.06352412700653076, -0.03793075680732727, -0.009916971437633038, 0.009865921922028065, 0.013699888251721859, 0.07334297895431519, 0.028970519080758095, -0.008001064881682396, 0.037864528596401215, -0.0046511320397257805, 0.05371379479765892, 0.04236424341797829, -0.005195035599172115, 0.045150261372327805, -0.024089565500617027, 0.02996688149869442, 0.01702144183218479, -0.024357546120882034, -0.02874450944364071, -0.07051112502813339, -0.043213099241256714, 0.015171256847679615, -0.013019931502640247, 0.007322291377931833, 0.032377343624830246, -0.0761328786611557, -0.006147494073957205, -0.009803448803722858, 0.026521947234869003, 0.02876337245106697, -0.03936163708567619, 0.04309429973363876, -0.02219192497432232, 0.028185781091451645, -0.020020579919219017, 0.007225584704428911, -0.039547450840473175, -0.038863398134708405, 0.0023863662499934435, 0.03486962988972664, 0.009217005223035812, 0.04753095656633377, 0.07488342374563217, 0.021021869033575058, -0.019261261448264122, 0.05075135827064514, 0.05234203487634659, 0.042802054435014725, -0.058917783200740814, 0.009169568307697773, -0.0114184794947505, -0.024603283032774925, -0.020432887598872185, -0.03730656951665878, -0.011245569214224815, 0.0518949069082737, -0.01777232438325882, -0.020640768110752106, 0.015455231070518494, -0.009173426777124405, -0.019948413595557213, -0.07887320220470428, 0.010364311747252941, 0.023990197107195854, -0.02183448150753975, -0.016735829412937164, 0.04933293163776398, 0.010621244087815285, 0.053837575018405914, 0.019890988245606422, -0.02331245131790638, -0.042438603937625885, 0.0376339852809906, 0.011187724769115448, -0.0675845816731453, -0.04671342670917511, -0.05316471308469772, 0.03220190852880478, 0.02316640503704548, -0.010591902770102024, -0.08743032068014145, 0.017634905874729156, 0.0520271360874176, -0.0533728264272213, 0.06312556564807892, 0.03298362344503403, 0.04579544439911842, 0.04387025162577629, -0.0067642005160450935, -0.0012411015341058373, -0.0220523402094841, -0.015545527450740337, -0.004620107356458902, 0.04921107366681099, -0.01686077192425728, -0.02403855137526989, -0.036483075469732285, 0.034233104437589645, -0.0004134571354370564, 0.03333095461130142, 0.023780155926942825, -0.0215507410466671, -0.039365991950035095, 0.007088224403560162, 0.021579977124929428, -0.04926331341266632, 0.015588230453431606, 0.0431477315723896, 0.01689556986093521, -0.02139599435031414, -0.025761902332305908, -0.005895566660910845, 0.009362280368804932, 0.06592248380184174, -0.024581845849752426, -0.033219024538993835, -0.03883035480976105, 0.022612683475017548, 0.004720636177808046, -0.04392965883016586, -0.10522866994142532, 0.03612978011369705, -0.007931091822683811, -0.02656685747206211, 0.0595211498439312, 0.02757774479687214, 0.023897986859083176, 0.07394728809595108, 0.0013310438953340054, 0.015569751150906086, -0.03303242474794388, 0.04341736435890198, -0.03539205342531204, -0.036442358046770096, 0.011251267977058887, -0.05132952705025673, -0.01252642460167408, -0.040165532380342484, -0.044165316969156265, -0.0377374067902565, 0.01601918786764145, 0.033709876239299774, -0.029712719842791557, 0.01371624507009983, 0.012021202594041824, 0.009144358336925507, -0.0012294809566810727, -0.054170094430446625, -0.012410640716552734, -0.03506080433726311, -0.059657350182533264, -0.04365936294198036, 0.03791392594575882, -0.005035681184381247, 0.023932164534926414, 0.0034728029277175665, 0.02458377555012703, 0.013101131655275822, -0.019378934055566788, -0.017877968028187752, 0.032196931540966034, 0.02618904784321785, -0.044426120817661285, -0.04397771507501602, -0.04404795542359352, 0.012077024206519127, 0.0009185854578390718, -0.057090867310762405, 0.03635062649846077, 0.02196238934993744, -0.021085225045681, -0.02153061516582966, 0.013250293210148811, 0.00024128140648826957, -0.046743229031562805, -0.05875478312373161, 0.0014907746808603406, 0.01107202097773552, 0.0355990007519722, -0.044903725385665894, -0.00744067644700408, 0.039901454001665115, 0.054436638951301575, 0.03989597037434578, 0.020156705752015114, -0.0024347302969545126, 0.05408094823360443, -0.03394201770424843, 0.01124644186347723, -0.08133042603731155, 0.012479742057621479, -0.014537785202264786, 0.03361954540014267, -0.016587598249316216, -0.0019532712176442146, -0.04333049803972244, 0.03773229196667671, -0.03504340723156929, -0.02680058404803276, 0.009273001924157143, 0.020638667047023773, -0.03398161754012108, 0.04324514791369438, -0.032252904027700424, 0.012472523376345634, -0.04653674736618996, 0.021308109164237976, -0.03609234094619751, -0.007463605143129826, 0.002886619418859482, -0.010017814114689827, -0.05408637970685959, -0.03729449212551117, -0.018613724038004875, -0.0747760459780693, 0.014677428640425205, 0.04456208646297455, -0.0037362310104072094, 0.01652003638446331, -0.02049092948436737, -0.009004614315927029, 0.01310324389487505, -0.04954605549573898, -0.024893861263990402, -0.054506320506334305, -0.02564423345029354, 0.0038152928464114666, 0.02247799001634121, 0.03442836552858353, -0.04935676231980324, -0.04851234331727028, 0.06551472097635269, 0.04748581722378731, 0.012762893922626972, 0.012478840537369251, 0.03356518596410751, 0.029689347371459007, 0.03162391856312752, 0.0009990492835640907, 0.015171544626355171, -0.039270542562007904, 0.019663330167531967, -0.0032285084016621113, 0.0005036802613176405, 0.005587312392890453, -0.029122715815901756, -0.04797721281647682, -0.029384853318333626, 0.09772323071956635, 0.04215091094374657, 0.036107707768678665, -0.0059586623683571815, -0.06001514196395874, 0.04123353213071823, 0.02582753449678421, -0.033240944147109985, 0.019794894382357597, 0.00762584013864398, -0.04011582210659981, 0.06632877886295319, 0.025751780718564987, 0.0337512381374836, 0.022395649924874306, 0.014725248329341412, -0.026703786104917526, 0.03363873064517975, -0.04476390779018402, -0.0031432872638106346, 0.022113017737865448, -0.04401383548974991, 0.0010328451171517372, -0.03826238587498665, 0.049495622515678406, -0.03614891692996025, 0.04957938194274902, 0.050102073699235916, -9.874166426016018e-05, 0.011222000233829021, -0.028685754165053368, -0.010983431711792946, 0.031677842140197754, -0.010753041133284569, 0.06186556816101074, -0.0074374014511704445, -0.07745931297540665, 0.03900652006268501, 0.03108547255396843, -0.06512308120727539, 0.031722452491521835, 0.008549943566322327, 0.012647946365177631, 0.03415451943874359, 0.04522695019841194, -0.05784319341182709, -0.01139136590063572, -0.014637447893619537, 0.013122860342264175, -0.03127758577466011, -0.02670898847281933, 0.051023080945014954, 0.010094310157001019, -0.009224277921020985, 0.03171323239803314, -0.007032867521047592, 0.02815721184015274, 0.03412032499909401, -0.04599647969007492, -0.05517728999257088, 0.006577468477189541, 0.050992149859666824, 0.03993885591626167, -0.009872814640402794, -0.04027444124221802, -0.006794418208301067, 0.016102338209748268, -0.01627134159207344, -0.03268791362643242, 0.061120036989450455, 0.03230159357190132, -0.11263767629861832, 0.0009169777040369809, 0.036213599145412445, 0.0032208524644374847, -0.035493154078722, 0.01669081673026085, 0.0012780106626451015, 0.007710043806582689, 0.015535444021224976, 0.015169922262430191, 0.0034496274311095476, -0.07034318149089813, -0.009966891258955002, 0.023845499381422997, -0.0037595026660710573, 0.04338093847036362, -0.028243770822882652, 0.013600415550172329, 0.06672463566064835, 0.025279821828007698, 0.012037968263030052, -0.05715629830956459, -0.030268756672739983, 0.03982356935739517, -0.02968951314687729, 0.043989308178424835, -0.03443540632724762, -0.042146142572164536, -0.051622238010168076, -0.01609404757618904, -0.01765049248933792, 0.06530895829200745, -0.006290975026786327, 0.016796844080090523, 0.01698577031493187, 0.00958422850817442, 0.00293816183693707, -0.05678679049015045, -0.03955871984362602, -0.03187083080410957, 0.05141907185316086, 0.019684238359332085, -0.024625755846500397, 0.018303697928786278, -0.03491023927927017, -0.04864603281021118, 0.03176124766469002, -0.011285300366580486, -0.028160076588392258, 0.07350596785545349, 0.02604042924940586, -0.021007336676120758, 0.00442914292216301, 0.034913986921310425, -0.057340387254953384, 0.03061915747821331, 0.004473674576729536, 0.011480627581477165, 0.025294611230492592, 0.03858784958720207, 0.02081691473722458, -0.02084927447140217, -0.034105028957128525, -0.018360333517193794, -0.003101494861766696, 0.0009713417966850102, 0.07005392760038376]]
</pre>
</div>
</div>

</div>
</div>

</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/12/LangChain-Embeddings/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-12 </div>
			<div class="article-title"><a href="/2023/04/12/Text-Embeddings/" >Text Embedding</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Text-Embeddings">Text Embeddings<a class="anchor-link" href="#Text-Embeddings">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://openai.com/blog/new-and-improved-embedding-model">OpenAI Embedding Model</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">EdgeGPT</span> <span class="kn">import</span> <span class="n">Chatbot</span> <span class="k">as</span> <span class="n">Bing</span><span class="p">,</span> <span class="n">ConversationStyle</span>

<span class="n">bing</span> <span class="o">=</span> <span class="n">Bing</span><span class="p">(</span><span class="n">cookiePath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.config/EdgeGPT/cookies.json&#39;</span><span class="p">))</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="k">await</span> <span class="n">bing</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="n">conversation_style</span> <span class="o">=</span> <span class="n">ConversationStyle</span><span class="o">.</span><span class="n">balanced</span><span class="p">,</span>
    <span class="p">))[</span><span class="s1">&#39;item&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;adaptiveCards&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;body&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">text-embedding-ada-002 是什么？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>text-embedding-ada-002 是 OpenAI 提供的一种嵌入模型，用于将文本转换为向量表示。[^1^] 你可以将文本字符串发送到嵌入 API 端点，同时选择嵌入模型 ID（例如 text-embedding-ada-002），响应将包含一个嵌入，你可以提取、保存和使用。[^1^] 

text-embedding-ada-002 模型是 OpenAI 提供的第二代嵌入模型之一，它比第一代模型更好、更便宜、更简单易用。[^5^] 

如果你想了解更多关于 text-embedding-ada-002 的信息，可以参考 OpenAI 的官方文档。[^5^]

---

[1]: https://platform.openai.com/docs/guides/embeddings/second-generation-models &#34;Embeddings - OpenAI API&#34;
[2]: https://juejin.cn/post/7216319218505220151 &#34;OpenAI的Embedding - 掘金 - 稀土掘金&#34;
[3]: https://cloud.tencent.com/developer/article/2222888 &#34;GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试 - 腾讯云开发者社区-腾讯云&#34;
[4]: https://openai.com/blog/new-and-improved-embedding-model/ &#34;New and improved embedding model - openai.com&#34;
[5]: https://platform.openai.com/docs/guides/embeddings &#34;Embeddings - OpenAI API&#34;

text-embedding-ada-002 是 OpenAI 提供的一种嵌入模型，用于将文本转换为向量表示。[^1^][1] 你可以将文本字符串发送到嵌入 API 端点，同时选择嵌入模型 ID（例如 text-embedding-ada-002），响应将包含一个嵌入，你可以提取、保存和使用。[^1^][1] 

text-embedding-ada-002 模型是 OpenAI 提供的第二代嵌入模型之一，它比第一代模型更好、更便宜、更简单易用。[^2^][5] 

如果你想了解更多关于 text-embedding-ada-002 的信息，可以参考 OpenAI 的官方文档。[^2^][5]

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">比较 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入、MPNet 嵌入</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入都是常见的文本嵌入模型。其中，GPT-3 是 OpenAI 开发的一种大型语言模型，可以生成自然语言文本[^1^]。GloVe 和 Word2vec 都是基于神经网络的词向量模型，用于将单词转换为向量表示，以便计算机可以更好地理解自然语言[^1^] [^2^]。MPNet 是一种基于多任务学习的预训练语言模型，可以用于各种自然语言处理任务[^1^] [^4^]。

据腾讯云开发者社区的一篇文章所述，该作者使用了 text-embedding-ada-002（GPT-3 的一个 Embeddings）与三种传统文本嵌入技术生成的嵌入进行比较，即 GloVe、Word2vec 和 MPNet。结果显示，GPT-3 嵌入在所有模型中获得了最高的精度[^1^]。但是，这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！

---

[1]: https://cloud.tencent.com/developer/article/2222888 &#34;GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试 - 腾讯云开发者社区-腾讯云&#34;
[2]: https://developer.aliyun.com/article/1159268 &#34;GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试-阿里云开发者社区&#34;
[3]: https://new.qq.com/rain/a/20230212A01IWP00 &#34;GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试_腾讯新闻&#34;
[4]: https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/129153025 &#34;GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试&#34;
[5]: https://zhuanlan.zhihu.com/p/605603003 &#34;GPT-3 vs Bert vs GloVe 文本嵌入技术的性能对比测试 - 知乎&#34;

GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入都是常见的文本嵌入模型。其中，GPT-3 是 OpenAI 开发的一种大型语言模型，可以生成自然语言文本[^1^][1]。GloVe 和 Word2vec 都是基于神经网络的词向量模型，用于将单词转换为向量表示，以便计算机可以更好地理解自然语言[^1^][1] [^2^][2]。MPNet 是一种基于多任务学习的预训练语言模型，可以用于各种自然语言处理任务[^1^][1] [^3^][4]。

据腾讯云开发者社区的一篇文章所述，该作者使用了 text-embedding-ada-002（GPT-3 的一个 Embeddings）与三种传统文本嵌入技术生成的嵌入进行比较，即 GloVe、Word2vec 和 MPNet。结果显示，GPT-3 嵌入在所有模型中获得了最高的精度[^1^][1]。但是，这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">还有哪些文本嵌入模型？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>除了 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入之外，还有许多其他文本嵌入模型。例如，ELMo、Skip-Thoughts、Quick-Thoughts、InferSent、MILA/MSR 的通用句子表示和 Google 的通用句子编码器等模型都是最先进的模型之一[^1^]。这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！

---

[1]: https://cloud.tencent.com/developer/article/1167972 &#34;干货 | 文本嵌入的经典模型与最新进展 - 腾讯云开发者社区-腾讯云&#34;
[2]: https://blog.csdn.net/lingpy/article/details/80809926 &#34;文本嵌入的经典模型与最新进展 - CSDN博客&#34;
[3]: https://zhuanlan.zhihu.com/p/459295186 &#34;5分钟 NLP系列—— 11 个词嵌入模型总结 - 知乎 - 知乎专栏&#34;

除了 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入之外，还有许多其他文本嵌入模型。例如，ELMo、Skip-Thoughts、Quick-Thoughts、InferSent、MILA/MSR 的通用句子表示和 Google 的通用句子编码器等模型都是最先进的模型之一[^1^][1]。这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">文本嵌入模型的英文？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>The English term for &#34;文本嵌入模型&#34; is &#34;text embedding models&#34;. Some of the most common text embedding models include Word2Vec, GloVe, FastText and Embeddings from Language Models (ELMo)[^1^] [^2^]. If you need more information about these models or any other text embedding models, please let me know!

---

[1]: https://techblog.ezra.com/different-embedding-models-7874197dc410 &#34;What are the common word embeddings? | The Ezra Tech Blog - Medium&#34;
[2]: https://openai.com/blog/introducing-text-and-code-embeddings/ &#34;Introducing text and code embeddings - OpenAI&#34;
[3]: https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings &#34;The Beginner’s Guide to Text Embeddings | deepset&#34;
[4]: https://platform.openai.com/docs/guides/embeddings/embedding-models &#34;Embeddings - OpenAI API&#34;

The English term for &#34;文本嵌入模型&#34; is &#34;text embedding models&#34;. Some of the most common text embedding models include Word2Vec, GloVe, FastText and Embeddings from Language Models (ELMo)[^1^][1] [^2^][2]. If you need more information about these models or any other text embedding models, please let me know!

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">InstructorEmbedding 是什么？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>InstructorEmbedding is a text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning[^1^]. It is a single embedder that can generate text embeddings tailored to different downstream tasks and domains[^1^]. InstructorEmbedding is available as a Python package[^2^]. If you need more information about InstructorEmbedding or any other text embedding models, please let me know!

---

[1]: https://instructor-embedding.github.io/ &#34;Instructor Text Embedding&#34;
[2]: https://pypi.org/project/InstructorEmbedding/ &#34;InstructorEmbedding · PyPI&#34;
[3]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html &#34;langchain.embeddings.huggingface — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.135&#34;

InstructorEmbedding is a text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning[^1^][1]. It is a single embedder that can generate text embeddings tailored to different downstream tasks and domains[^1^][1]. InstructorEmbedding is available as a Python package[^2^][2]. If you need more information about InstructorEmbedding or any other text embedding models, please let me know!

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">比较 HuggingFaceInstructEmbeddings 和 HuggingFaceHubEmbeddings</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>HuggingFaceInstructEmbeddings and HuggingFaceHubEmbeddings are both wrappers around sentence_transformers embedding models[^2^] [^3^]. HuggingFaceInstructEmbeddings is a wrapper around InstructorEmbedding[^2^], while HuggingFaceHubEmbeddings is a wrapper around Hugging Face Hub embedding models[^3^]. To use HuggingFaceInstructEmbeddings, you should have the `sentence_transformers` and `InstructorEmbedding` Python packages installed[^2^]. To use HuggingFaceHubEmbeddings, you should have the `huggingface_hub` Python package installed and the environment variable `HUGGINGFACEHUB_API_TOKEN` set with your API token, or pass it as a named parameter to the constructor[^3^]. If you need more information about HuggingFaceInstructEmbeddings or HuggingFaceHubEmbeddings or any other text embedding models, please let me know!

---

[1]: https://huggingface.co/blog/getting-started-with-embeddings &#34;Getting Started With Embeddings - Hugging Face&#34;
[2]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html &#34;langchain.embeddings.huggingface — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.135&#34;
[3]: https://sj-langchain.readthedocs.io/en/latest/reference/modules/embeddings.html &#34;Embeddings — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.94&#34;
[4]: https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline &#34;machine learning - Getting sentence embedding from huggingface Feature ...&#34;

HuggingFaceInstructEmbeddings and HuggingFaceHubEmbeddings are both wrappers around sentence_transformers embedding models[^1^][2] [^2^][3]. HuggingFaceInstructEmbeddings is a wrapper around InstructorEmbedding[^1^][2], while HuggingFaceHubEmbeddings is a wrapper around Hugging Face Hub embedding models[^2^][3]. To use HuggingFaceInstructEmbeddings, you should have the `sentence_transformers` and `InstructorEmbedding` Python packages installed[^1^][2]. To use HuggingFaceHubEmbeddings, you should have the `huggingface_hub` Python package installed and the environment variable `HUGGINGFACEHUB_API_TOKEN` set with your API token, or pass it as a named parameter to the constructor[^2^][3]. If you need more information about HuggingFaceInstructEmbeddings or HuggingFaceHubEmbeddings or any other text embedding models, please let me know!

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">sentence_transformers 的 GitHub 链接？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Here is the GitHub link for sentence_transformers: https://github.com/UKPLab/sentence-transformers [^1^] [^5^]. You can find more information about sentence_transformers and how to use it in your project by visiting this link. Let me know if you have any other questions!

---

[1]: https://github.com/taishan1994/sbert_text_similarity &#34;GitHub - taishan1994/sbert_text_similarity: 使用sentence-transformers ...&#34;
[2]: https://github.com/zhangyi24/sentence_transformer_zh &#34;GitHub - zhangyi24/sentence_transformer_zh&#34;
[3]: https://blog.csdn.net/RinaWong/article/details/122890394 &#34;安装sentence-transformers_竹霖聴雨的博客-CSDN博客&#34;
[4]: https://github.com/UKPLab/sentence-transformers &#34;UKPLab/sentence-transformers - Github&#34;
[5]: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained_models.md &#34;sentence-transformers/pretrained_models.md at master · UKPLab ... - GitHub&#34;
[6]: https://github.com/huggingface/blog/blob/main/how-to-train-sentence-transformers.md &#34;blog/how-to-train-sentence-transformers.md at main - Github&#34;

Here is the GitHub link for sentence_transformers: https://github.com/UKPLab/sentence-transformers [^1^][1] [^2^][5]. You can find more information about sentence_transformers and how to use it in your project by visiting this link. Let me know if you have any other questions!

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">InstructorEmbedding 的 GitHub 链接？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Here is the GitHub link for InstructorEmbedding: https://github.com/HKUNLP/instructor-embedding [^1^] [^2^]. You can find more information about InstructorEmbedding and how to use it in your project by visiting this link. Let me know if you have any other questions!

---

[1]: https://pypi.org/project/InstructorEmbedding/ &#34;InstructorEmbedding · PyPI&#34;
[2]: https://github.com/HKUNLP/instructor-embedding &#34;GitHub - HKUNLP/instructor-embedding: One Embedder, Any Task ...&#34;
[3]: https://github.com/basujindal/chatPDF &#34;GitHub - basujindal/chatPDF: chat with PDF&#34;

Here is the GitHub link for InstructorEmbedding: https://github.com/HKUNLP/instructor-embedding [^1^][1] [^2^][2]. You can find more information about InstructorEmbedding and how to use it in your project by visiting this link. Let me know if you have any other questions!

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">比较 HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings 都是用于自然语言处理的嵌入式模型。HuggingFaceInstructEmbeddings 是 sentence_transformers 的包装器，而 LlamaCppEmbeddings 是 llama.cpp 嵌入式模型的包装器。这两个模型都可以用于生成文本嵌入，但是它们的实现方式和性能可能有所不同。如果您需要更多的信息，请访问它们的 GitHub 页面：https://github.com/HKUNLP/instructor-embedding [^1^] [^2^] 和 https://github.com/ggerganov/llama.cpp [^3^] 。

---

[1]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/llamacpp.html &#34;langchain.embeddings.llamacpp — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.136&#34;
[2]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html &#34;langchain.embeddings.huggingface — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.135&#34;
[3]: https://sj-langchain.readthedocs.io/en/latest/_modules/langchain/embeddings/huggingface.html &#34;langchain.embeddings.huggingface — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.94&#34;

HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings 都是用于自然语言处理的嵌入式模型。HuggingFaceInstructEmbeddings 是 sentence_transformers 的包装器，而 LlamaCppEmbeddings 是 llama.cpp 嵌入式模型的包装器。这两个模型都可以用于生成文本嵌入，但是它们的实现方式和性能可能有所不同。如果您需要更多的信息，请访问它们的 GitHub 页面：https://github.com/HKUNLP/instructor-embedding [^1^][1] [^2^][2] 和 https://github.com/ggerganov/llama.cpp [^3^][3] 。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">执行下面这段代码之后，HuggingFaceEmbeddings 的模型下载到哪里了？</span>
<span class="s1">```</span>
<span class="s1">from langchain.embeddings import HuggingFaceEmbeddings</span>

<span class="s1"># 准备文本</span>
<span class="s1">text = &#39;这是一个测试文档。&#39;</span>

<span class="s1"># 使用 HuggingFace 生成文本嵌入</span>
<span class="s1">embeddings = HuggingFaceEmbeddings()</span>
<span class="s1">query_result = embeddings.embed_query(text)</span>
<span class="s1">doc_result = embeddings.embed_documents([text])</span>
<span class="s1">```</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>HuggingFaceEmbeddings 的模型下载路径是本地缓存。当您第一次使用该模型时，它会自动缓存在本地。如果您想查看缓存的位置，可以运行以下代码：
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
print(tokenizer.cache_dir)
```
如果您想将模型下载到特定的文件夹中，可以使用以下代码：
```
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModel.from_pretrained(&#34;bert-base-uncased&#34;, cache_dir=&#34;my_folder&#34;)
```
希望这可以帮助到您！

---

[1]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html &#34;langchain.embeddings.huggingface — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.135&#34;
[2]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface &#34;How to download model from huggingface? - Stack Overflow&#34;
[3]: https://github.com/hwchase17/langchain/blob/master/langchain/embeddings/huggingface.py &#34;langchain/huggingface.py at master · hwchase17/langchain&#34;

HuggingFaceEmbeddings 的模型下载路径是本地缓存。当您第一次使用该模型时，它会自动缓存在本地。如果您想查看缓存的位置，可以运行以下代码：
```
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
print(tokenizer.cache_dir)
```
如果您想将模型下载到特定的文件夹中，可以使用以下代码：
```
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModel.from_pretrained(&#34;bert-base-uncased&#34;, cache_dir=&#34;my_folder&#34;)
```
希望这可以帮助到您！

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="nb">dir</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">name_or_path</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;bert-base-uncased&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--bert-base-uncased
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
total 0
drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">.no_exist</span>
drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">blobs</span>
drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">refs</span>
drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 <span class="ansi-cyan-intense-fg ansi-bold">snapshots</span>
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
du<span class="w"> </span>-sh<span class="w"> </span>~/.cache/huggingface/hub/models--bert-base-uncased
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
696K	/Users/saintway/.cache/huggingface/hub/models--bert-base-uncased
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p>If you're talking about vector database embeddings, check out <a target="_blank" rel="noopener" href="https://github.com/chroma-core/chroma">Chroma</a>.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.trychroma.com/">https://www.trychroma.com/</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.trychroma.com/embeddings">https://docs.trychroma.com/embeddings</a></li>
</ul>
<hr />
<p>If you're talking about fine tuning, check out <a target="_blank" rel="noopener" href="https://github.com/tloen/alpaca-lora/blob/main/finetune.py">alpaca-lora finetuning</a>.</p>
<p>I recommend using <a target="_blank" rel="noopener" href="https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml">gpt4xalpaca</a> as a base model as it doesn't have the same level of censorship as vicuna. However, if you're using it purely for ethical tasks, vicuna is definitely better.</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/chavinlo/gpt4-x-alpaca">GPT4 x Alpaca</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Chroma Embedding Database 是什么</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Chroma Embedding Database 是一个开源的嵌入式数据库，用于存储人工智能模型的嵌入[^1^]。嵌入是向量的集合。Chroma 的数据库专门用于存储 AI 模型的嵌入[^2^]。它允许您通过最近邻搜索而不是子字符串搜索来搜索嵌入[^3^]。默认情况下，Chroma 使用 Sentence Transformers 进行嵌入，但您也可以使用 OpenAI 嵌入、Cohere（多语言）嵌入或自己的嵌入[^3^]。

希望这可以帮助您了解 Chroma Embedding Database。

---

[1]: https://docs.trychroma.com/ &#34;\uD83C\uDFE1 Home | Chroma&#34;
[2]: https://siliconangle.com/2023/04/06/chroma-bags-18m-speed-ai-models-embedding-database/ &#34;Chroma bags $18M to speed up AI models with its embedding database&#34;
[3]: https://pypi.org/project/chromadb/ &#34;chromadb · PyPI&#34;
[4]: https://www.trychroma.com/ &#34;Chroma - the AI-native open-source embedding database&#34;
[5]: https://github.com/chroma-core/chroma &#34;GitHub - chroma-core/chroma: the open source embedding database&#34;

Chroma Embedding Database 是一个开源的嵌入式数据库，用于存储人工智能模型的嵌入[^1^][1]。嵌入是向量的集合。Chroma 的数据库专门用于存储 AI 模型的嵌入[^2^][2]。它允许您通过最近邻搜索而不是子字符串搜索来搜索嵌入[^3^][3]。默认情况下，Chroma 使用 Sentence Transformers 进行嵌入，但您也可以使用 OpenAI 嵌入、Cohere（多语言）嵌入或自己的嵌入[^3^][3]。

希望这可以帮助您了解 Chroma Embedding Database。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">ggml 是什么</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ggml 是一个用于机器学习的张量库。该项目正在开发中，尚未准备好用于生产。它是用 C 语言编写的，支持 16 位浮点数和 4 位整数量化。它还支持自动微分（正在进行中）ADAM 和 L ...[^3^]。

希望这可以帮助到您！

---

[1]: https://gitee.com/mirrors/ggml &#34;ggml: ggml 是一个用于机器学习的张量库&#34;
[2]: https://www.oschina.net/p/ggml &#34;ggml首页、文档和下载 - 机器学习的张量库 - OSCHINA - 中文开源技术交流社区&#34;
[3]: https://github.com/ggerganov/ggml &#34;GitHub - ggerganov/ggml: Tensor library for machine learning&#34;
[4]: https://zhuanlan.zhihu.com/p/616037509 &#34;介绍大模型LLaMa及周边 - 知乎 - 知乎专栏&#34;
[5]: https://github.com/ggerganov/whisper.cpp &#34;ggerganov/whisper.cpp: Port of OpenAI&#39;s Whisper model in C/C++ - Github&#34;

ggml 是一个用于机器学习的张量库。该项目正在开发中，尚未准备好用于生产。它是用 C 语言编写的，支持 16 位浮点数和 4 位整数量化。它还支持自动微分（正在进行中）ADAM 和 L ...[^1^][3]。

希望这可以帮助到您！

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">HUGGINGFACEHUB_API_TOKEN 有什么用？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>HUGGINGFACEHUB_API_TOKEN 是一个环境变量，用于验证您的 API 令牌。您可以使用它来访问 Hugging Face Hub API，以便在您的模型中使用预训练模型、上传和下载模型等。[^1^] [^3^]

希望这可以帮助到您！

---

[1]: https://sj-langchain.readthedocs.io/en/latest/ecosystem/huggingface.html &#34;Hugging Face — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.94&#34;
[2]: https://zhuanlan.zhihu.com/p/344173246 &#34;用户身份验证的令牌——Token - 知乎 - 知乎专栏&#34;
[3]: https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_hub.py &#34;langchain/huggingface_hub.py at master · hwchase17/langchain&#34;

HUGGINGFACEHUB_API_TOKEN 是一个环境变量，用于验证您的 API 令牌。您可以使用它来访问 Hugging Face Hub API，以便在您的模型中使用预训练模型、上传和下载模型等。[^1^][1] [^2^][3]

希望这可以帮助到您！

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Hugging Face Hub API 有什么用？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Hugging Face Hub API 是一个模型和数据集的存储库，可以让您轻松地共享、发现和使用模型和数据集。它提供了一组 API，可以让您从 Python 中上传、下载和管理模型和数据集。您可以使用 Hugging Face Hub API 来存储、共享和管理您的模型和数据集，也可以使用它来发现其他人共享的模型和数据集。[^1^]

Hugging Face Hub API 还提供了一些端点，您可以使用这些端点从 Hub 中检索信息，也可以执行某些操作，例如创建模型、数据集或 Space 存储库。[^1^]

---

[1]: https://huggingface.co/docs/hub/api &#34;https://\uE000huggingface\uE001.co/docs/\uE000hub\uE001/\uE000api\uE001&#34;

Hugging Face Hub API 是一个模型和数据集的存储库，可以让您轻松地共享、发现和使用模型和数据集。它提供了一组 API，可以让您从 Python 中上传、下载和管理模型和数据集。您可以使用 Hugging Face Hub API 来存储、共享和管理您的模型和数据集，也可以使用它来发现其他人共享的模型和数据集。[^1^][1]

Hugging Face Hub API 还提供了一些端点，您可以使用这些端点从 Hub 中检索信息，也可以执行某些操作，例如创建模型、数据集或 Space 存储库。[^1^][1]

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">没有 HUGGINGFACEHUB_API_TOKEN 就不能使用 Hugging Face Hub API 吗？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>是的，Hugging Face Hub API 需要 HUGGINGFACEHUB_API_TOKEN。您需要在 Hugging Face 帐户设置中获取用户访问或 API 令牌。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^] [^3^]

---

[1]: https://api-inference.huggingface.co/docs/python/html/quicktour.html &#34;Overview - Hugging Face&#34;
[2]: https://huggingface.co/docs/huggingface_hub/how-to-inference &#34;Access the Inference API - Hugging Face&#34;
[3]: https://python.langchain.com/en/latest/ecosystem/huggingface.html &#34;Hugging Face — \uD83E\uDD9C\uD83D\uDD17 LangChain 0.0.137&#34;
[4]: https://docs.fast.ai/huggingface.html &#34;fastai - Hugging Face Hub&#34;
[5]: https://zhuanlan.zhihu.com/p/619763221 &#34;HuggingGPT： 用ChatGPT和它的朋友在HuggingFace中解决AI任务 - 知乎&#34;

是的，Hugging Face Hub API 需要 HUGGINGFACEHUB_API_TOKEN。您需要在 Hugging Face 帐户设置中获取用户访问或 API 令牌。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^][1] [^2^][3]

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">没有 HUGGINGFACEHUB_API_TOKEN 就不能在 HuggingFace 下载模型吗？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>是的，您需要 HUGGINGFACEHUB_API_TOKEN 才能在 HuggingFace 下载模型。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^] [^3^]

---

[1]: https://huggingface.co/docs/hub/security-tokens &#34;User access tokens - Hugging Face&#34;
[2]: https://blog.csdn.net/qq_41895747/article/details/125232359 &#34;解决ValueError: You need to pass a valid `token` or login by using ...&#34;
[3]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface &#34;How to download model from huggingface? - Stack Overflow&#34;
[4]: https://zhuanlan.zhihu.com/p/475260268 &#34;如何优雅的下载huggingface-transformers模型 - 知乎 - 知乎专栏&#34;
[5]: https://juejin.cn/post/7220623225775095869 &#34;ILLA Cloud- 调用 Hugging Face Inference Endpoints，开启大模型世界之门 - 掘金&#34;

是的，您需要 HUGGINGFACEHUB_API_TOKEN 才能在 HuggingFace 下载模型。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^][1] [^2^][3]

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">在模型上运行推理是什么意思？训练模型是什么意思？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>在机器学习中，训练模型是指使用大量数据来训练模型，以便它可以自动地从新数据中进行预测。在训练期间，模型会根据输入数据进行调整，以便它可以更好地预测输出。训练完成后，模型就可以用于推理，也就是使用新数据进行预测。

推理是指使用训练好的模型来进行预测。在推理期间，模型会接收新的输入数据，并输出预测结果。推理通常比训练快得多，因为它不需要对模型进行调整。

希望这可以帮到你。如果你有其他问题，请随时问我。

---

在机器学习中，训练模型是指使用大量数据来训练模型，以便它可以自动地从新数据中进行预测。在训练期间，模型会根据输入数据进行调整，以便它可以更好地预测输出。训练完成后，模型就可以用于推理，也就是使用新数据进行预测。

推理是指使用训练好的模型来进行预测。在推理期间，模型会接收新的输入数据，并输出预测结果。推理通常比训练快得多，因为它不需要对模型进行调整。

希望这可以帮到你。如果你有其他问题，请随时问我。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">没有 HUGGINGFACEHUB_API_TOKEN 可以在 HuggingFace 下载他人公开的模型吗？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>HuggingFace 下载他人公开的模型不需要 HUGGINGFACEHUB_API_TOKEN。你可以通过以下方式下载他人公开的模型：

1. 通过 Hugging Face 官网下载，例如：https://huggingface.co/models
2. 通过 Transformers 库中的 from_pretrained 方法下载，例如：

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)
```

希望这可以帮到你。如果你有其他问题，请随时问我。

---

[1]: https://zhuanlan.zhihu.com/p/475260268 &#34;如何优雅的下载huggingface-transformers模型 - 知乎 - 知乎专栏&#34;
[2]: https://huggingface.co/inference-api &#34;Inference API - Hugging Face&#34;
[3]: https://zhuanlan.zhihu.com/p/448852278 &#34;【Huggingface Transformers】保姆级使用教程—上 - 知乎&#34;
[4]: https://pypi.org/project/huggingface-hub/ &#34;huggingface-hub · PyPI&#34;
[5]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface &#34;How to download model from huggingface? - Stack Overflow&#34;

HuggingFace 下载他人公开的模型不需要 HUGGINGFACEHUB_API_TOKEN。你可以通过以下方式下载他人公开的模型：

1. 通过 Hugging Face 官网下载，例如：https://huggingface.co/models
2. 通过 Transformers 库中的 from_pretrained 方法下载，例如：

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)
```

希望这可以帮到你。如果你有其他问题，请随时问我。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">怎么用 Python 在 HuggingFace 下载模型？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>您可以使用以下代码在 HuggingFace 中下载模型：
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)
```
这将下载 BERT 模型和 tokenizer。

---

您可以使用以下代码在 HuggingFace 中下载模型：
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;)
```
这将下载 BERT 模型和 tokenizer。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">在 HuggingFace 下载模型，如何断点续传？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>您可以使用以下代码在 HuggingFace 中断点续传下载模型：
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;, resume_download=True)
```
这将下载 BERT 模型和 tokenizer，并在下载过程中断点续传。

---

[1]: https://zhuanlan.zhihu.com/p/475260268 &#34;https://zhuanlan.zhihu.com/p/475260268&#34;

您可以使用以下代码在 HuggingFace 中断点续传下载模型：
```
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-uncased&#34;)
model = AutoModelForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;, resume_download=True)
```
这将下载 BERT 模型和 tokenizer，并在下载过程中断点续传。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">PyTorch 模型可以分成好几个 bin 文件吗？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>PyTorch 模型可以分成多个文件，其中包括模型权重和其他模型参数。如果您的模型是由多个子模型组成的，则可以将每个子模型的权重和参数保存在单独的文件中。这些文件可以使用 PyTorch 的 state_dict 和 optimizer 保存在同一个文件中[^9^]. 

您可以使用以下代码将模型保存到多个文件中：

```python
torch.save(model.state_dict(), PATH)
```

其中，`model` 是您要保存的 PyTorch 模型，`PATH` 是您要保存的文件路径。这将会把模型权重和其他参数保存到一个名为 `PATH` 的文件中。

如果您想要加载这些文件并恢复模型，则可以使用以下代码：

```python
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
```

其中，`TheModelClass` 是您要恢复的 PyTorch 模型类，`*args` 和 `**kwargs` 是用于初始化模型的参数。这将会从名为 `PATH` 的文件中加载模型权重和其他参数，并将它们加载到新创建的模型实例中。

希望这可以帮助您！请让我知道是否还有其他问题需要解答。

---

[1]: https://blog.csdn.net/qq_39686950/article/details/116090826 &#34;Windows系统下把PyTorch模型转为ncnn模型流程 - CSDN博客&#34;
[2]: https://blog.csdn.net/season77us/article/details/104311195 &#34;如何下载Hugging Face 模型（pytorch_model.bin, config.json, vocab.txt）以及如何在 ...&#34;
[3]: https://www.cnblogs.com/cxq1126/p/14277134.html &#34;将tensorflow版本的.ckpt模型转成pytorch的.bin模型 - 最咸的鱼 - 博客园&#34;
[4]: https://zhuanlan.zhihu.com/p/620688513 &#34;Pytorch格式 .pt .pth .bin 详解 - 知乎 - 知乎专栏&#34;
[5]: https://blog.csdn.net/znevegiveup1/article/details/118093294 &#34;读取pytorch.bin权重文件解读_pytorch_model.bin_唐僧爱吃唐僧肉的博客-CSDN博客&#34;
[6]: https://blog.csdn.net/A_water_/article/details/128197563 &#34;Pytorch框架的模型pth文件转换成C++ OpenVINO框架的bin和xml文件并运行_pth转bin_A_water_的博客-CSDN博客&#34;
[7]: https://zhuanlan.zhihu.com/p/113694038 &#34;PyTorch分布式训练简明教程(2022更新版) - 知乎 - 知乎专栏&#34;
[8]: https://zhuanlan.zhihu.com/p/82038049 &#34;PyTorch | 保存和加载模型 - 知乎&#34;
[9]: https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html &#34;Saving and loading multiple models in one file using PyTorch&#34;

PyTorch 模型可以分成多个文件，其中包括模型权重和其他模型参数。如果您的模型是由多个子模型组成的，则可以将每个子模型的权重和参数保存在单独的文件中。这些文件可以使用 PyTorch 的 state_dict 和 optimizer 保存在同一个文件中[^1^][9]. 

您可以使用以下代码将模型保存到多个文件中：

```python
torch.save(model.state_dict(), PATH)
```

其中，`model` 是您要保存的 PyTorch 模型，`PATH` 是您要保存的文件路径。这将会把模型权重和其他参数保存到一个名为 `PATH` 的文件中。

如果您想要加载这些文件并恢复模型，则可以使用以下代码：

```python
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
```

其中，`TheModelClass` 是您要恢复的 PyTorch 模型类，`*args` 和 `**kwargs` 是用于初始化模型的参数。这将会从名为 `PATH` 的文件中加载模型权重和其他参数，并将它们加载到新创建的模型实例中。

希望这可以帮助您！请让我知道是否还有其他问题需要解答。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span><span class="p">,</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">LlamaCppEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.faiss</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="k">def</span> <span class="nf">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="c1"># (1) Import a series of documents.</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span> <span class="n">silent_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">raw_documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="c1"># (2) Split them into small chunks.</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ingest_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)</span>
    <span class="c1"># (3) Create embeddings for each document (using text-embedding-ada-002).</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/ggml-model-q4_1.bin&#39;</span><span class="p">),</span> <span class="n">n_ctx</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">ingest_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 1024
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  = 1600.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 58231.14 ms /   136 tokens (  428.17 ms per token)
llama_print_timings:        eval time =  1743.42 ms /     1 runs   ( 1743.42 ms per run)
llama_print_timings:       total time = 60005.23 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 178540.67 ms /   472 tokens (  378.26 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 178591.84 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 41458.73 ms /   139 tokens (  298.26 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 41490.46 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 13792.71 ms /    51 tokens (  270.45 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 13806.36 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 64205.43 ms /   192 tokens (  334.40 ms per token)
llama_print_timings:        eval time =   393.32 ms /     1 runs   (  393.32 ms per run)
llama_print_timings:       total time = 64615.16 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 312641.94 ms /   458 tokens (  682.62 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 312700.26 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 428699.27 ms /   478 tokens (  896.86 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 428799.03 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 268943.46 ms /   309 tokens (  870.37 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 268984.75 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 428036.12 ms /   648 tokens (  660.55 ms per token)
llama_print_timings:        eval time =   426.99 ms /     1 runs   (  426.99 ms per run)
llama_print_timings:       total time = 428658.90 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 10771.61 ms /    30 tokens (  359.05 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 10808.14 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 77845.98 ms /   109 tokens (  714.18 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 77920.76 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 31191.36 ms /   103 tokens (  302.83 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 31233.61 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 125583.51 ms /   111 tokens ( 1131.38 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 125624.03 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 26138.27 ms /    68 tokens (  384.39 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 26169.78 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 12869.16 ms /    35 tokens (  367.69 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 12877.02 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 349920.02 ms /   504 tokens (  694.29 ms per token)
llama_print_timings:        eval time =  1198.45 ms /     1 runs   ( 1198.45 ms per run)
llama_print_timings:       total time = 351175.09 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 235384.79 ms /   158 tokens ( 1489.78 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 235445.03 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 540504.96 ms /   542 tokens (  997.24 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 540623.58 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 351235.10 ms /   719 tokens (  488.51 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 351388.48 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 50923.63 ms /   165 tokens (  308.63 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 51015.38 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 184992.04 ms /   272 tokens (  680.12 ms per token)
llama_print_timings:        eval time =   411.72 ms /     1 runs   (  411.72 ms per run)
llama_print_timings:       total time = 185451.72 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 194429.41 ms /   622 tokens (  312.59 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 194464.86 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 113118.77 ms /   366 tokens (  309.07 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 113145.92 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 687028.24 ms /   728 tokens (  943.72 ms per token)
llama_print_timings:        eval time =   439.94 ms /     1 runs   (  439.94 ms per run)
llama_print_timings:       total time = 687586.24 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 189149.69 ms /   533 tokens (  354.88 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 189193.56 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 191291.91 ms /   560 tokens (  341.59 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 191325.52 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 262665.41 ms /   725 tokens (  362.30 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 262699.98 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 446825.63 ms /   744 tokens (  600.57 ms per token)
llama_print_timings:        eval time =  2820.09 ms /     1 runs   ( 2820.09 ms per run)
llama_print_timings:       total time = 449699.31 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 370878.55 ms /   717 tokens (  517.26 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 370985.32 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 316657.40 ms /   647 tokens (  489.42 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 316701.04 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 355381.24 ms /   504 tokens (  705.12 ms per token)
llama_print_timings:        eval time =   426.32 ms /     1 runs   (  426.32 ms per run)
llama_print_timings:       total time = 355866.70 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 306588.42 ms /   559 tokens (  548.46 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 306644.46 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 388570.23 ms /   565 tokens (  687.73 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 388642.65 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 123319.83 ms /   318 tokens (  387.80 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 123356.07 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 329182.49 ms /   811 tokens (  405.90 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 329242.86 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 210174.59 ms /   584 tokens (  359.89 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 210206.57 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 81113.52 ms /   286 tokens (  283.61 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 81134.54 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 439253.03 ms /   727 tokens (  604.20 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 439387.22 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 286354.39 ms /   751 tokens (  381.30 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 286420.33 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 444201.68 ms /   675 tokens (  658.08 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 444267.37 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 607155.44 ms /   620 tokens (  979.28 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 607229.15 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 674110.08 ms /   708 tokens (  952.13 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 674200.48 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 329078.28 ms /   470 tokens (  700.17 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 329136.29 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 774982.77 ms /   734 tokens ( 1055.83 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 775084.90 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1736656.90 ms /   767 tokens ( 2264.22 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1736891.33 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1348035.02 ms /   608 tokens ( 2217.16 ms per token)
llama_print_timings:        eval time = 24584.54 ms /     1 runs   (24584.54 ms per run)
llama_print_timings:       total time = 1372887.22 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1395140.63 ms /   472 tokens ( 2955.81 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1395354.86 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2404735.92 ms /   808 tokens ( 2976.16 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2405157.08 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1788561.84 ms /   640 tokens ( 2794.63 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1788744.43 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2043317.53 ms /   690 tokens ( 2961.33 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2043549.39 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1295899.29 ms /   402 tokens ( 3223.63 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1296069.46 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1882245.78 ms /   592 tokens ( 3179.47 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1882491.04 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 2343689.96 ms /   741 tokens ( 3162.87 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 2343993.29 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 1234739.82 ms /   490 tokens ( 2519.88 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 1234957.50 ms

llama_print_timings:        load time =  8850.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 86554.06 ms /    53 tokens ( 1633.10 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 86652.29 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="c1"># Save vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_13B_1024.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vectorstore</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="c1"># Load vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_13B_1024.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 1024
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  = 1600.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;你知道什么？&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get context related to the question from the embedding model</span>
<span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>page_content=&#39;&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;态度：认同、欣赏、尊重、重视、轻视、忽视、鄙视、反对\n客体所做：我对你的态度、你对我的态度、你对他的态度\n主体所想：我对你的态度、你对我的态度\n主体所说：我对他的态度、他对我的态度、他对你的态度\n对于某些次态度的记忆、拟构；怀疑、与、确信；\n你对我的态度、有正有误；作为某种主体效果的客体态度，对于客体态度的目标、意向；对于态度的态度、有正有误；\n渴望被重视、被认可、被理解、被公正地对待；\n虚荣；自负，轻视，反对、有正有误，对于他人的误解；对于自己态度的温和的怀疑；苏格拉底式教学法；偏见、综合比较，是，某种轻视；\n我对客体的态度：你对我的态度、你的确信，我的确信、我对你的态度；确信、对于确信的态度；我对自己的态度，耻辱之恨、丑陋之恨\n对于某种经历的认同。对于某种人生概括的认同。省略主语、所有格。怕丑。\n注意，是，自由的。？我是谁？&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

page_content=&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;---\n\n&gt; \u3000\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\n\u3000\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\n\u3000\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 15863.30 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 15862.61 ms /     8 tokens ( 1982.83 ms per token)
llama_print_timings:        eval time = 23200.83 ms /     1 runs   (23200.83 ms per run)
llama_print_timings:       total time = 39067.66 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.base</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
<span class="kn">from</span> <span class="nn">langchain.chains.chat_vector_db.prompts</span> <span class="kn">import</span> <span class="n">CONDENSE_QUESTION_PROMPT</span><span class="p">,</span> <span class="n">QA_PROMPT</span>
<span class="kn">from</span> <span class="nn">langchain.chains.question_answering</span> <span class="kn">import</span> <span class="n">load_qa_chain</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.base</span> <span class="kn">import</span> <span class="n">VectorStore</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># Callback function to stream answers to stdout.</span>
<span class="n">manager</span> <span class="o">=</span> <span class="n">CallbackManager</span><span class="p">([</span><span class="n">StreamingStdOutCallbackHandler</span><span class="p">()])</span>

<span class="n">streaming_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">question_gen_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="c1"># Prompt to generate independent questions by incorporating chat history and a new question.</span>
<span class="n">question_generator</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">question_gen_llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">CONDENSE_QUESTION_PROMPT</span><span class="p">)</span>
<span class="c1"># Pass in documents and a standalone prompt to answer questions.</span>
<span class="n">doc_chain</span> <span class="o">=</span> <span class="n">load_qa_chain</span><span class="p">(</span><span class="n">streaming_llm</span><span class="p">,</span> <span class="n">chain_type</span><span class="o">=</span><span class="s1">&#39;stuff&#39;</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">QA_PROMPT</span><span class="p">)</span>
<span class="c1"># Generate prompts from embedding model.</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">ConversationalRetrievalChain</span><span class="p">(</span><span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span> <span class="n">combine_docs_chain</span><span class="o">=</span><span class="n">doc_chain</span><span class="p">,</span> <span class="n">question_generator</span><span class="o">=</span><span class="n">question_generator</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">QA_PROMPT</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>PromptTemplate(input_variables=[&#39;context&#39;, &#39;question&#39;], output_parser=None, partial_variables={}, template=&#34;Use the following pieces of context to answer the question at the end. If you don&#39;t know the answer, just say that you don&#39;t know, don&#39;t try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:&#34;, template_format=&#39;f-string&#39;, validate_template=True)</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p>Utilizing <a target="_blank" rel="noopener" href="https://github.com/seii-saintway/ipytest">IPyTest</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 PyTest 中，可以使用 <code>monkeypatch</code> 来替换导入的库，从而进行测试。 <code>monkeypatch</code> 是一个 pytest 内置的 fixture，用于替换运行时的变量和对象，以便在测试期间使用自定义值。</p>
<p>对于要替换的库，可以使用 <code>pytest_mock</code> fixture 来自动将其传递给 <code>monkeypatch</code>。如果没有使用 <code>pytest_mock</code>，则需要手动使用 <code>monkeypatch.setattr()</code> 方法来替换导入的库。</p>
<p>以下是一个示例，假设我们有一个名为 <code>example.py</code> 的模块，它导入了 <code>requests</code> 库，并使用了该库的 <code>get()</code> 方法发送网络请求：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>

<span class="k">def</span> <span class="nf">get_example_data</span><span class="p">():</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;https://example.com&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">content</span>
</pre></div>
<p>要测试这个函数，我们需要使用 Dummy 对象替换 requests 库，以便我们可以模拟网络请求的响应。可以使用以下代码进行测试：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">example</span>

<span class="k">class</span> <span class="nc">DummyResponse</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">content</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">content</span> <span class="o">=</span> <span class="n">content</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_requests</span><span class="p">(</span><span class="n">monkeypatch</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">mock_get</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DummyResponse</span><span class="p">(</span><span class="sa">b</span><span class="s1">&#39;Test data&#39;</span><span class="p">)</span>
    <span class="n">monkeypatch</span><span class="o">.</span><span class="n">setattr</span><span class="p">(</span><span class="n">requests</span><span class="p">,</span> <span class="s1">&#39;get&#39;</span><span class="p">,</span> <span class="n">mock_get</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_get_example_data</span><span class="p">(</span><span class="n">mock_requests</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">example</span><span class="o">.</span><span class="n">get_example_data</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">data</span> <span class="o">==</span> <span class="sa">b</span><span class="s1">&#39;Test data&#39;</span>
</pre></div>
<p>在这个示例中，我们首先定义了一个名为 <code>DummyResponse</code> 的类，它代表了一个虚拟的 requests 库的响应。然后我们定义了一个名为 <code>mock_requests</code> 的 fixture，它使用 <code>monkeypatch.setattr()</code> 方法替换了 requests 库的 <code>get()</code> 方法，使其返回我们定义的 DummyResponse 对象。</p>
<p>最后，我们定义了一个测试函数 <code>test_get_example_data</code>，并将 <code>mock_requests</code> fixture 作为参数传递给它。在测试函数内部，我们调用 <code>example.get_example_data()</code>，这将调用我们已经用 Dummy 对象替换的 requests 库的 <code>get()</code> 方法，最终返回我们定义的虚拟响应。</p>
<p>这样我们就可以在测试中使用 Dummy 对象替换任何库，以便更好地控制测试环境。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">openai</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">help</span><span class="p">(</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Help on method create in module openai.api_resources.chat_completion:

create(*args, **kwargs) method of builtins.type instance
    Creates a new chat completion for the provided messages and parameters.
    
    See https://platform.openai.com/docs/api-reference/chat-completions/create
    for a list of valid parameters.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">json</span><span class="o">,</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">revChatGPT.V1</span> <span class="kn">import</span> <span class="n">Chatbot</span><span class="p">,</span> <span class="n">configure</span>

<span class="c1"># Open the JSON file and read the conversation_id</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.config/revChatGPT/config.json&#39;</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;conversation_id&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">bot</span> <span class="o">=</span> <span class="n">Chatbot</span><span class="p">(</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configure</span><span class="p">(),</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="n">conversation_id</span><span class="p">,</span>
    <span class="n">lazy_loading</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>git+https://github.com/seii-saintway/ipytest
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">markdown</span>
<span class="kn">import</span> <span class="nn">IPython</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb">How to stream completions?</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">delta</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="c1"># IPython.display.display(IPython.core.display.Markdown(response[&#39;message&#39;]))</span>
        <span class="c1"># IPython.display.clear_output(wait=True)</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">):],</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">],</span>
        <span class="p">}</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">mock_create</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;user&#39;</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[{}],</span>
        <span class="p">}</span>

    <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stream&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">delta</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]):</span>
        <span class="c1"># IPython.display.display(IPython.core.display.Markdown(response[&#39;message&#39;]))</span>
        <span class="c1"># IPython.display.clear_output(wait=True)</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s1">&#39;finish_reason&#39;</span><span class="p">:</span> <span class="s1">&#39;stop&#39;</span><span class="p">,</span>
                <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s1">&#39;message&#39;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">],</span>
    <span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_openai</span><span class="p">(</span><span class="n">monkeypatch</span><span class="p">):</span>
    <span class="n">monkeypatch</span><span class="o">.</span><span class="n">setattr</span><span class="p">(</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">,</span> <span class="n">mock_create</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;终极真实是什么？&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">answer</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">test_qa</span><span class="p">(</span><span class="n">mock_openai</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">answer</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">qa</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[]})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ipytest</span> <span class="kn">import</span> <span class="n">do</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do</span><span class="p">(</span>
    <span class="n">mock_openai</span><span class="o">=</span><span class="n">mock_openai</span><span class="p">,</span>
    <span class="n">test_qa</span><span class="o">=</span><span class="n">test_qa</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
=&gt; no.0  ::source::test_qa  setup  passed

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 14550.38 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 23539.97 ms /    15 tokens ( 1569.33 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 23551.68 ms
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>The author argues that to study the interconnections between subjective experiences and physical processes, we need to move away from purely introspective methods and embrace physiological psychology. Only through this approach can we achieve a complete understanding of psychological phenomena. The author also suggests that consciousness is inextricably linked to physical processes and can be affected by them.

=&gt; no.0  ::source::test_qa  runtest  passed

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">answer</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;question&#39;: &#39;终极真实是什么？&#39;,
 &#39;chat_history&#39;: [],
 &#39;answer&#39;: &#39;The author argues that to study the interconnections between subjective experiences and physical processes, we need to move away from purely introspective methods and embrace physiological psychology. Only through this approach can we achieve a complete understanding of psychological phenomena. The author also suggests that consciousness is inextricably linked to physical processes and can be affected by them.&#39;}</pre>
</div>

</div>

</div>
</div>

</div>
 




<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/C9b_oZu9jpw0oObEuDmd9w">ColossalAI</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/hpcaitech/ColossalAI">https://github.com/hpcaitech/ColossalAI</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/LCGQyNA6sHcdfIIARSNlww">LMFlow</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/OptimalScale/LMFlow">https://github.com/OptimalScale/LMFlow</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://chat.lmsys.org/">Chat with Open Large Language Models</a><ul>
<li>Vicuna: a chat assistant fine-tuned from LLaMA on user-shared conversations. This one is expected to perform best according to our evaluation.</li>
<li>Koala: a chatbot fine-tuned from LLaMA on user-shared conversations and open-source datasets. This one performs similarly to Vicuna.</li>
<li>ChatGLM: an open bilingual dialogue language model | 开源双语对话语言模型</li>
<li>Alpaca: a model fine-tuned from LLaMA on 52K instruction-following demonstrations.</li>
<li>LLaMA: open and efficient foundation language models</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/llama">Inference code for LLaMA models</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/ggerganov/llama.cpp">LLaMA Cpp</a><ul>
<li><a target="_blank" rel="noopener" href="https://til.simonwillison.net/llms/llama-7b-m2">Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://vicuna.lmsys.org/">Vicuna: An Open-Source Chatbot</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/lmsys">Vicuna Models: 7b-delta-v0 and 13b-delta-v0</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat">FastChat: An open platform for training, serving, and evaluating large language model based chatbots.</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://crfm.stanford.edu/2023/03/13/alpaca.html">Alpaca: A Strong, Replicable Instruction-Following Model</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/tatsu-lab/stanford_alpaca">Train Stanford's Alpaca Models</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tloen/alpaca-lora">Instruct-tuned Alpaca-LoRA Models</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tloen/alpaca-lora/blob/main/finetune.py">Alpaca-LoRA Instruct-tuning</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/nomic-ai/gpt4all">GPT4All: an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMA</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I recommend using <a target="_blank" rel="noopener" href="https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml">gpt4 x alpaca ggml</a> as a base model as it doesn't have the same level of censorship as vicuna. However, if you're using it purely for ethical tasks, vicuna is definitely better.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/chavinlo/alpaca-native">Stanford Alpaca Model</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/chavinlo/alpaca-13b">Alpaca 13B without LoRA</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/chavinlo/gpt4-x-alpaca">GPT4 x Alpaca without LoRA</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s1">&#39;Pi3141/alpaca-lora-7B-ggml&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;ggml-model-q4_0.bin&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Downloading ggml-model-q4_0.bin:   0%|          | 0.00/4.21G [00:00&lt;?, ?B/s]</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;/Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_0.bin&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>
<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s1">&#39;Pi3141/alpaca-lora-7B-ggml&#39;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;ggml-model-q4_1.bin&#39;</span><span class="p">,</span> <span class="n">resume_download</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>Downloading ggml-model-q4_1.bin:   0%|          | 0.00/5.06G [00:00&lt;?, ?B/s]</pre>
</div>

</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>&#39;/Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin&#39;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python<span class="o">[</span>server<span class="o">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
<span class="nb">export</span><span class="w"> </span><span class="nv">MODEL</span><span class="o">=</span>~/ggml-model-q4_1.bin
python3<span class="w"> </span>-m<span class="w"> </span>llama_cpp.server
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 1608.00 MB per state)
llama_init_from_file: kv self size  = 1600.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
INFO:     Started server process [41176]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:     ::1:61438 - &#34;GET / HTTP/1.1&#34; 404 Not Found
INFO:     ::1:61438 - &#34;GET /docs HTTP/1.1&#34; 200 OK
INFO:     ::1:61438 - &#34;GET /openapi.json HTTP/1.1&#34; 200 OK
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 11679.89 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 240847.82 ms /   478 tokens (  503.87 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 240873.74 ms
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:     ::1:61439 - &#34;POST /v1/embeddings HTTP/1.1&#34; 200 OK
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 11679.89 ms
llama_print_timings:      sample time =    33.43 ms /    16 runs   (    2.09 ms per run)
llama_print_timings: prompt eval time = 255305.21 ms /   478 tokens (  534.11 ms per token)
llama_print_timings:        eval time = 28069.18 ms /    15 runs   ( 1871.28 ms per run)
llama_print_timings:       total time = 283470.31 ms
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>INFO:     ::1:61681 - &#34;POST /v1/completions HTTP/1.1&#34; 200 OK
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/ggml-model-q4_1.bin&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 512
llama_model_load_internal: n_embd     = 5120
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 40
llama_model_load_internal: n_layer    = 40
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 13824
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 13B
llama_model_load_internal: ggml ctx size =  73.73 KB
llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)
llama_init_from_file: kv self size  =  800.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">llm</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？&#39;&#39;&#39;</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf8&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">llm</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。&#39;&#39;&#39;</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;utf8&#39;</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span><span class="p">,</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="c1"># (1) Import a series of documents.</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span> <span class="n">silent_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">raw_documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="c1"># (2) Split them into small chunks.</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">LlamaCppEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.faiss</span> <span class="kn">import</span> <span class="n">FAISS</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">ingest_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)</span>
    <span class="c1"># (3) Create embeddings for each document (using text-embedding-ada-002).</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">LlamaCppEmbeddings</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span>
        <span class="s1">&#39;~/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin&#39;</span>
    <span class="p">),</span> <span class="n">n_ctx</span><span class="o">=</span><span class="mi">2048</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">ingest_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 6612.57 MB (+ 2052.00 MB per state)
llama_init_from_file: kv self size  = 2048.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 121567.27 ms /   607 tokens (  200.28 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 121578.08 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 55477.99 ms /   384 tokens (  144.47 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 55490.11 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 185983.50 ms /  1245 tokens (  149.38 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 185993.29 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 99895.20 ms /   648 tokens (  154.16 ms per token)
llama_print_timings:        eval time =   274.67 ms /     1 runs   (  274.67 ms per run)
llama_print_timings:       total time = 100174.12 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time =  4480.73 ms /    30 tokens (  149.36 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time =  4483.89 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 34421.56 ms /   208 tokens (  165.49 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 34428.46 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 18916.80 ms /   115 tokens (  164.49 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 18922.95 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 109097.79 ms /   672 tokens (  162.35 ms per token)
llama_print_timings:        eval time =   322.11 ms /     1 runs   (  322.11 ms per run)
llama_print_timings:       total time = 109426.49 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 182994.18 ms /  1131 tokens (  161.80 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 183004.98 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 159320.01 ms /  1040 tokens (  153.19 ms per token)
llama_print_timings:        eval time =   239.12 ms /     1 runs   (  239.12 ms per run)
llama_print_timings:       total time = 159568.61 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 239876.70 ms /  1530 tokens (  156.78 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 239888.82 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 217347.31 ms /  1428 tokens (  152.20 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 217358.50 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 191811.75 ms /  1255 tokens (  152.84 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 191821.81 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 226394.03 ms /  1406 tokens (  161.02 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 226403.94 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 250274.44 ms /  1514 tokens (  165.31 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 250290.90 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 245848.18 ms /  1459 tokens (  168.50 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 245869.09 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 241036.46 ms /  1448 tokens (  166.46 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 241046.69 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 90638.96 ms /   549 tokens (  165.10 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 90648.73 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 243383.44 ms /  1456 tokens (  167.16 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 243395.79 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 66549.32 ms /   407 tokens (  163.51 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 66557.07 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 254780.55 ms /  1524 tokens (  167.18 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 254791.22 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 63349.78 ms /   397 tokens (  159.57 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 63354.36 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 174274.05 ms /  1092 tokens (  159.59 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 174282.70 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 194346.63 ms /  1332 tokens (  145.91 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 194358.84 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 69352.89 ms /   490 tokens (  141.54 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 69357.43 ms

llama_print_timings:        load time =  9764.51 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time =  7361.54 ms /    53 tokens (  138.90 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time =  7371.77 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I need a big memory to accelerate LLM inference.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Save vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_7B_2048.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">vectorstore</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load vectorstore</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;vectorstore_7B_2048.pkl&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">vectorstore</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>llama.cpp: loading model from /Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin
llama_model_load_internal: format     = ggjt v1 (latest)
llama_model_load_internal: n_vocab    = 32000
llama_model_load_internal: n_ctx      = 2048
llama_model_load_internal: n_embd     = 4096
llama_model_load_internal: n_mult     = 256
llama_model_load_internal: n_head     = 32
llama_model_load_internal: n_layer    = 32
llama_model_load_internal: n_rot      = 128
llama_model_load_internal: f16        = 3
llama_model_load_internal: n_ff       = 11008
llama_model_load_internal: n_parts    = 1
llama_model_load_internal: model size = 7B
llama_model_load_internal: ggml ctx size =  59.11 KB
llama_model_load_internal: mem required  = 6612.57 MB (+ 2052.00 MB per state)
llama_init_from_file: kv self size  = 2048.00 MB
AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;你知道什么？&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get context related to the question from the embedding model</span>
<span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>page_content=&#39;title: Neuroscience\ndate: 2021-10-14 16:30:20\ntags: Neuro\n---\n\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\n\n---\n\n&gt; \u3000\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\n\u3000\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\n\u3000\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;有意注意，是指，对于某次效果的注意。\n无意注意，是指，对于某次非效果的注意。\n\n目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。\n意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。\n\n懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。\n焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。\n\n对于某次功效的目标，联结着，对于此次功效的原因。\n对于某种功效的概括，联结着，对于此种功效的原因。\n\n兴趣，是指，联结着某次快乐的识。\n荒诞，是指，联结着某次乏味的识。\n苦毒，是指，联结着某次痛苦的识。\n\n慾望，是指，对于某次兴趣的表征。\n妄想，是指，对于某次荒诞的表征。？\n苦观，是指，对于某次苦毒的表征。\n\n苦观，分为，记忆苦观、拟构苦观。弊害，…、…\n\n有趣注意，是指，对于某次兴趣的注意。\n无趣注意，是指，对于某次荒诞的注意。\n\n意义，是指，值得的注意。\n神圣，是指，极其丰富的意义。\n积极的态度，是指，充满对于某种意义的信心。\n消极的态度，是指，缺乏对于某种意义的信心。\n积极的注意，导致着，快乐。\n消极的注意，导致着，乏味。\n对于某种意义的怀疑，是指，对于某种意义的信心的减弱。\n对于某种意义的确定，是指，对于某种意义的信心的增强。\n对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。\n对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。\n静思、禅修、祷告，都是，某种定觉练习。\n\n---\n\n&gt; 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。\n(罗马书 8:24-25 吕振中)\n\n&gt; 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。\n(提摩太后书 2:22 吕振中)\n\n向内往最深处去：净心、呼求主名、并且、等待回应。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

page_content=&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\n&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\n&gt; 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\n&gt; ……&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;客体方式，导致着、联结着，主体方式、机体状态\n形体，导致着、联结着，身体、快乐、痛苦\n轻蔑、轻视他人对自己的态度，损害着，羞耻心\n羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？\n羞耻、光荣，重视他人对自己的态度、敬重\n恥辱、傲慢，轻视他人对自己的态度、轻蔑\n羞耻、羞辱，在含义上，有所不同吗？\n单方的轻视、双方的轻视？\n一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。\n狭隘、愚蠢、固执，轻视他人的信念\n开明、智慧、变通，重视他人的信念&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 14467.91 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 14467.34 ms /     8 tokens ( 1808.42 ms per token)
llama_print_timings:        eval time = 14635.33 ms /     1 runs   (14635.33 ms per run)
llama_print_timings:       total time = 29115.01 ms
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>

<span class="n">CONDENSE_QUESTION_PROMPT</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;chat_history&#39;</span><span class="p">,</span> <span class="s1">&#39;question&#39;</span><span class="p">],</span>
    <span class="n">output_parser</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partial_variables</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">template</span><span class="o">=</span><span class="s1">&#39;给定以下对话和后续问题，请重新表述后续问题以成为一个独立问题。</span><span class="se">\n\n</span><span class="s1">聊天记录：</span><span class="se">\n</span><span class="si">{chat_history}</span><span class="se">\n</span><span class="s1">后续问题：</span><span class="si">{question}</span><span class="se">\n</span><span class="s1">独立问题：&#39;</span><span class="p">,</span>
    <span class="n">template_format</span><span class="o">=</span><span class="s1">&#39;f-string&#39;</span><span class="p">,</span>
    <span class="n">validate_template</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">QA_PROMPT</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="p">(</span>
    <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;context&#39;</span><span class="p">,</span> <span class="s1">&#39;question&#39;</span><span class="p">],</span>
    <span class="n">output_parser</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">partial_variables</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">template</span><span class="o">=</span><span class="s1">&#39;使用下面的背景信息回答最后的问题。如果您不知道答案，请直接说您不知道，不要试图编造一个答案。</span><span class="se">\n\n</span><span class="s1">背景信息：</span><span class="se">\n</span><span class="si">{context}</span><span class="se">\n\n</span><span class="s1">问题：</span><span class="si">{question}</span><span class="se">\n</span><span class="s1">有用的答案：&#39;</span><span class="p">,</span>
    <span class="n">template_format</span><span class="o">=</span><span class="s1">&#39;f-string&#39;</span><span class="p">,</span>
    <span class="n">validate_template</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.base</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
<span class="kn">from</span> <span class="nn">langchain.chains.question_answering</span> <span class="kn">import</span> <span class="n">load_qa_chain</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.base</span> <span class="kn">import</span> <span class="n">VectorStore</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># Callback function to stream answers to stdout.</span>
<span class="n">manager</span> <span class="o">=</span> <span class="n">CallbackManager</span><span class="p">([</span><span class="n">StreamingStdOutCallbackHandler</span><span class="p">()])</span>

<span class="n">streaming_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">question_gen_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="c1"># Prompt to generate independent questions by incorporating chat history and a new question.</span>
<span class="n">question_generator</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">question_gen_llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">CONDENSE_QUESTION_PROMPT</span><span class="p">)</span>
<span class="c1"># Pass in documents and a standalone prompt to answer questions.</span>
<span class="n">doc_chain</span> <span class="o">=</span> <span class="n">load_qa_chain</span><span class="p">(</span><span class="n">streaming_llm</span><span class="p">,</span> <span class="n">chain_type</span><span class="o">=</span><span class="s1">&#39;stuff&#39;</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">QA_PROMPT</span><span class="p">)</span>
<span class="c1"># Generate prompts from embedding model.</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">ConversationalRetrievalChain</span><span class="p">(</span><span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span> <span class="n">combine_docs_chain</span><span class="o">=</span><span class="n">doc_chain</span><span class="p">,</span> <span class="n">question_generator</span><span class="o">=</span><span class="n">question_generator</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">openai</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">json</span><span class="o">,</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">revChatGPT.V1</span> <span class="kn">import</span> <span class="n">Chatbot</span><span class="p">,</span> <span class="n">configure</span>

<span class="c1"># Open the JSON file and read the conversation_id</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.config/revChatGPT/config.json&#39;</span><span class="p">),</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;conversation_id&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">bot</span> <span class="o">=</span> <span class="n">Chatbot</span><span class="p">(</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configure</span><span class="p">(),</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="n">conversation_id</span><span class="p">,</span>
    <span class="n">lazy_loading</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">delta</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                    <span class="s1">&#39;delta&#39;</span><span class="p">:</span> <span class="p">{</span>
                        <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">res</span><span class="p">):],</span>
                    <span class="p">}</span>
                <span class="p">}</span>
            <span class="p">],</span>
        <span class="p">}</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">mock_create</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">message</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">[</span><span class="s1">&#39;messages&#39;</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">message</span><span class="p">[</span><span class="s1">&#39;role&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="s1">&#39;user&#39;</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[{}],</span>
        <span class="p">}</span>

    <span class="k">if</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;stream&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">delta</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">message</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]):</span>
        <span class="k">pass</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s1">&#39;choices&#39;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s1">&#39;finish_reason&#39;</span><span class="p">:</span> <span class="s1">&#39;stop&#39;</span><span class="p">,</span>
                <span class="s1">&#39;index&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
                <span class="s1">&#39;message&#39;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">],</span>
                    <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">],</span>
    <span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_openai</span><span class="p">(</span><span class="n">monkeypatch</span><span class="p">):</span>
    <span class="n">monkeypatch</span><span class="o">.</span><span class="n">setattr</span><span class="p">(</span><span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="p">,</span> <span class="s1">&#39;create&#39;</span><span class="p">,</span> <span class="n">mock_create</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;终极真实是什么？&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">answer</span> <span class="o">=</span> <span class="p">{}</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">test_qa</span><span class="p">(</span><span class="n">mock_openai</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">answer</span>
    <span class="n">answer</span> <span class="o">=</span> <span class="n">qa</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[]})</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">answer</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">ipytest</span> <span class="kn">import</span> <span class="n">do</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">do</span><span class="p">(</span>
    <span class="n">mock_openai</span><span class="o">=</span><span class="n">mock_openai</span><span class="p">,</span>
    <span class="n">test_qa</span><span class="o">=</span><span class="n">test_qa</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
=&gt; no.0  ::source::test_qa  setup  passed

</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>
llama_print_timings:        load time = 14467.91 ms
llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings: prompt eval time = 22490.19 ms /    15 tokens ( 1499.35 ms per token)
llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)
llama_print_timings:       total time = 22496.21 ms
</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>根据提供的背景信息，以下是问题的答案：

此处引用的文章是哪位心理学家写的？ 

这篇文章是由德国心理学家威廉·威廉德撰写的，标题是《心理学的目标和方法》。

=&gt; no.0  ::source::test_qa  runtest  passed

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">answer</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;question&#39;: &#39;终极真实是什么？&#39;,
 &#39;chat_history&#39;: [],
 &#39;answer&#39;: &#39;根据提供的背景信息，以下是问题的答案：\n\n此处引用的文章是哪位心理学家写的？ \n\n这篇文章是由德国心理学家威廉·威廉德撰写的，标题是《心理学的目标和方法》。&#39;}</pre>
</div>

</div>

</div>
</div>

</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/12/Text-Embeddings/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-11 </div>
			<div class="article-title"><a href="/2023/04/11/LLaMA-Index/" >LLaMA Index</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LLaMA-Index"><a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index">LLaMA Index</a><a class="anchor-link" href="#LLaMA-Index">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>llama-index
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Installing collected packages: llama-index
Successfully installed llama-index-0.5.12
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Using-OpenAI-Embedding-Model">Using <a target="_blank" rel="noopener" href="https://openai.com/blog/new-and-improved-embedding-model">OpenAI Embedding Model</a><a class="anchor-link" href="#Using-OpenAI-Embedding-Model">&#182;</a></h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tea-artist/llama_index_example/blob/main/llama_index_example.py">Llama_Index_Example</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index_name</span> <span class="o">=</span> <span class="s1">&#39;./index.json&#39;</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">GPTSimpleVectorIndex</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">GPTSimpleVectorIndex</span><span class="o">.</span><span class="n">load_from_disk</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">GPTSimpleVectorIndex</span><span class="p">([])</span>
<span class="n">index</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">llama_index</span> <span class="kn">import</span> <span class="n">SimpleDirectoryReader</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s1">&#39;../_posts/ultimate-facts&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[Document(text=&#39;\n\nSystems Neuroscience\n\n**Systems neuroscience** is a subdiscipline of neuroscience and systems biology that studies the structure and function of neural circuits and systems. Systems neuroscience encompasses a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks. At this level of analysis, neuroscientists study how different neural circuits analyze sensory information, form perceptions of the external world, make decisions, and execute movements. Researchers in systems neuroscience are concerned with the relation between molecular and cellular approaches to understanding brain structure and function, as well as with the study of high-level mental functions such as language, memory, and self-awareness (which are the purview of behavioral and cognitive neuroscience). Systems neuroscientists typically employ techniques for understanding networks of neurons as they are seen to function, by way of electrophysiology using either single-unit recording or multi-electrode recording, functional magnetic resonance imaging (fMRI), and PET scans. The term is commonly used in an educational framework: a common sequence of graduate school neuroscience courses consists of cellular/molecular neuroscience for the first semester, then systems neuroscience for the second semester. It is also sometimes used to distinguish a subdivision within a neuroscience department at an academic institution.\n\n&#39;, doc_id=&#39;3fd9301a-cfdf-49e9-a534-4aaf48f58f8f&#39;, embedding=None, doc_hash=&#39;19f1397f0b2cd476a1fd12c789321a4414f9e09be786df63f46892adee2c3b99&#39;, extra_info=None), Document(text=&#39;\n\nシステム神経科学\n\n**システム神経科学**（しすてむしんけいかがく、英: **Systems neuroscience**）は神経科学の下位分野で、覚醒し、行動する正常な生物の神経回路や神経システムの機能を研究する。システム神経科学は、神経細胞が結合してニューラル・ネットワークを作った時にどのようにふるまうのか (例えば、視覚や自発運動など) を研究する様々な分野の総称である。この分野では、神経科学者は異なる神経回路がどのようにして感覚情報を分析し、外的環境を認識し、意思決定を行い、運動を実行するかを研究している。システム神経科学に関心がある研究者は脳に対する分子神経科学や細胞神経科学的なアプローチと、行動神経科学や認知神経科学のおもな守備範囲である言語や、記憶、自己認識のような高次精神活動の間にある大きな溝に着目している。自らの主な分野をシステム神経科学としている研究者は少なく、たいていの場合はより詳細に &#34;行動神経科学&#34; や &#34;認知神経生理学&#34; としている。この用語は主に、大学院の神経科学コースなどで、1学期の講義は分子/細胞神経科学、2学期の講義はシステム神経科学というように教育分野で使われる。また、時には学術機関の神経科学科内で分野を区別するために使われることもある。\n\n&#39;, doc_id=&#39;24bff9fe-e8dd-425e-b62e-daca67473e78&#39;, embedding=None, doc_hash=&#39;81e85f6b10f98753bf6d06b5aa1a31e8d88632e8e10d1b4e0831f353c0c8d79a&#39;, extra_info=None), Document(text=&#39;title: 研究机构date: 2008-08-20 18:05:20tags:---{% asset_ipynb 研究机构.ipynb %}&#39;, doc_id=&#39;e6b10168-64ff-41f6-ba7a-6635f190405c&#39;, embedding=None, doc_hash=&#39;3ab4d5b4234162b96b8634c7686454f32a9cb9635567917e1a563b20a2085944&#39;, extra_info=None), Document(text=&#39;title: 从「意义疗法」开始追求「终极真实」date: 2015-09-23 13:40:00tags: UltimateFacts---* 自从受浸以来，我总是会动弹不得而禁食。为此，我感到非常羞耻。有一次禁食后，我下床昏倒。醒来后，我心里有声音提醒我说，我还要去解答那三个关于自我意识的问题。一听到这声音，我就得平安并且开始恢复吃饭。其实，这三个问题也是我之前一直思考并且想要解答的问题。只是，我还是犹豫不决，因为我并不知道这声音从谁而来。我看到的解答是对宗教核心的摧毁。－－凡所信的都将被实证。－－我想，贵主若是意愿如此，那就让事情这么成就吧。* 孜孜不倦地追求着终极真实。  孜孜不倦地追问着终极问题：我是谁？我从哪里来？我往哪里去？  在爱里寻找着终极解答：我就是爱；我从爱里来，我往爱里去。* 我们借用数学在异质的真实中发现同质的真实；而终极的真实却是异质的。* 追求「幸福」？还是，追求「真实」？  * 你是赌徒。你是冒险者。难道还追求幸运吗？不是应该相信幸运和爱吗？要用智慧行事。要争取时机。  * 你们若追求终极真实而受苦，你们也必被断为配得这份终极荣耀。你们既在追求终极真实的受苦上有多少分儿，就该有多少喜乐，使你们在这份终极荣耀显示的时候也可以欢喜快乐。你们若因做终极冒死者而受苦，就别惭愧了，也别灰心丧志了，倒要因有这名而荣耀永恒主呢。* 这是一份草稿，记录着我这几年以来思考：---&gt; 自从造天地以来，\u3000神那看不见的永能和神性是明明可觉的，乃是藉着所造之物就可以晓得，叫人无可推诿。 (罗马书 1:20)&gt; 至於他的事、人所不能見的——他永久的能力及神性——從世界之創立以來，又是藉着他所造之物以被了悟而看清楚，使人無可推諉。 (羅馬書 1:20 呂振中)&gt; For the invisible things of Him, both His eternal power and divine characteristics, have been clearly seen since the creation of the world, being perceived by the things made, so that they would be without excuse; (Romans 1:20 Recovery)&gt; For since the creation of the world His invisible attributes, that is, His eternal power and divine nature, have been clearly perceived, being understood by what has been made, so that they are without excuse. (Romans 1:20 NASB)真理、真实；神造真实、人造真实；真实，想象；记忆，拟构。如果哲学更像「真理」，那么各类科学就更像「真实」。如果物理学更像「真理」，那么化学就更像「真实」。如果化学更像「真理」，那么生物学、生理学就更像「真实」。如果生理学更像「真理」，那么脑科学、神经科学就更像「真实」。如果理科更像「神造真实」，那么工科就更像「人造真实」。如果生理学更像「神造真实」，那么医学、药学就更像「人造真实」。---&gt; 我只是一个碳族生物；一个土生土长的地球人¹。贵主耶稣是灵族人吧。而木星上的风暴²可以拥有怎样的生命和意识呢？贵主耶稣在木星上能否与木星人一起生活和交往呢？&gt; ¹ 地球人 =&gt; 《费曼讲座：宇称不守恒定律和如何与外星人交流》&gt; ² 风暴 =&gt;&gt; 当那一天，到了傍晚，耶稣对他们说：『我们渡到那边去吧。』他们就离开群众，照他在船上的情况把他带走；还有别的船也跟他在一起。当下起了**大暴风**，波浪泼进船内，甚至船简直满了！耶稣竟在船尾上靠着枕头睡觉呢；门徒就叫醒了他，对他说：『老师，我们丧命，你不在意么？』耶稣醒起来，斥责³那风，向海说：『不要作声！噤默罢！』那风不狂吹，便大大平静了。耶稣对他们说：『为什么这么胆怯呢？怎么没有信心呢？』他们就大起了敬畏的心，直彼此说：『这个人到底是谁？连风和海也听从他！』 (马可福音 4:35-41 吕振中)&gt; ³ 斥责 =&gt; 『ワンパンマン』：サイタマ？キング？↓↓--------------------继续修订--------------------↓↓圣经信仰之神经心理学实证纲领父的自我信息，是指，对于圣灵的表征。纯粹的圣经，含于，父的自我信息。从「纯粹的基督徒」到「超基督徒」「超级赌徒」吾否认圣经中上帝的名，因为那是人们创造的。超越神论，不可知论；信仰；宁死不屈，抗争到底对于神圣生命的信心？或，亲密关系？坚贞，「甘愿承担自己的罪罚」是《古兰经》的价值所在。真诚、勇敢、坚贞，耶稣的「甘愿承担」是《圣经》的价值所在。吾，若不是因为怯懦，又怎么会祷告呢？所以，吾，应该要，放弃，那种、对于其他的心灵的畏惧、所联结着的祷告。以耶稣为榜样。人子要经受三日地狱之火的洗。罪全部被烧尽了后，第三日复活。我所爱慕的必定是父所喜爱的，因为父从始至终都在吸引我、塑造我的爱慕。我所爱慕的若是父所不喜爱的，父必定会改变我。所以，我总是晓得父的喜爱。人子，与父和好，与父为友，爱父并顺从祂。与父同在，就有勇气。与父同行，就有希望。子永远与父同在，从未分离。「吾要成为超人。」「在吾的生活中显明父的荣耀。」祷告，是，对于子灵的表征。感，分为，虚感、实感。虚感，分为，信（？）、思感、愉快感、位置感。实感，分为，色感、声感、香感、味感、触感、缩紧感、疼痛感、瘙痒感、冷热感。体，是指，广延。感、体，平行地，变化。感、体，分割的，平行性原理感的统合，预示着，体的核心。体的核心：在感的集合中占比最大的体。信，是一种，感。（联结？极深的记忆？）灵，是指，具有自我独特视角的、体。 =&gt; “我是谁？”《圣经》说：信、灵，平行地，变化。在苦难中持守坚忍为何能增加信心呢？魂，是指，具有私有视角（记忆？）的、体。（联结？） =&gt; “我在哪里？” =&gt; 寻找已睡了的灵（如，保罗） =&gt; 进入他们的梦境虚感、魂，平行地，变化。实感、身体（？），平行地，变化。识，是指，某些感的杂合、杂合体。已察，是指，主体所注意过的识。未察，是指，主体所没注意过的识。机体状态，是指，某些愉快感、位置感、疼痛感、瘙痒感、冷热感的杂合体。快乐，是指，某些愉快感的杂合。乏味，是指，某些非愉快感、位置感、缩紧感的杂合体。痛苦，是指，某些位置感、疼痛感的杂合体。舒适，是指，某些位置感、非疼痛感的杂合体。忧郁，是指，乏味痛苦。幸福，是指，快乐舒适。主体方式，分为，心思、身体。心思，是指，某些思感的杂合。身体，是指，某些位置感、缩紧感的杂合体。表征，是指，表征着某次识的心思。记忆，是指，对于某次已察的表征。 =&gt; “我在哪里？”拟构，是指，对于某次未察的表征。概括，是指，概括着某种识的心思。过去，是指，对于记忆的概括。未来，是指，对于拟构的概括。客体方式，是指，某些色感、声感、香感、味感、触感、冷热感的杂合体。形体，是指，某些位置感、色感的杂合体。声音，是指，某些位置感、声感的杂合体。气味，是指，某些位置感、香感的杂合体。口味，是指，某些位置感、味感的杂合体。质地，是指，某些位置感、触感的杂合体。温度，是指，某些位置感、冷热感的杂合体。主体所注意着的某些次主体方式的接续变化，导致着，此些次主体方式的接续联结。主体所注意着的某些次识的接续变化，导致着，对于此些次识的某些次记忆的接续联结。主体所注意着的某些次客体方式的接续变化，导致着，对于此些次客体方式的某些次记忆的接续联结。意志，是指，导致着某种接续联结的（其中可能还含有一种未提及的虚感）（联结的生成）生活，是指，接续联结着的某些次主体方式。思维，是指，接续联结着的某些次心思。动作，是指，接续联结着的某些次身体。意愿，是指，表征着某次意志的思维。意愿，是，独占的。情节，是指，接续联结着的某些次表征。臆测，是指，接续联结着的某些次记忆。臆想，是指，接续联结着的某些次拟构。经历，是指，接续联结着的对于接续变化着的某些次识的某些次记忆。综合，是指，接续联结着的对于接续变化着的某些次识的某些次拟构。最简拟构是记忆的最简杂合。最简综合是经历的最简接续。这是，语义理解和表达、产生会意和指称的联结，所需的最简方式。主体所注意着的对于某些识某些次绵延变化的某些次经历的接续联结，导致着，对于此种识的某次概括。主体所注意着的对于某些次接续变化的某些次经历的接续联结，导致着，对于此种经历的某次概括。原因，是指，导致着某次识的识。结果，是指，某次识所导致着的识。利益，是指，对于某次舒适的原因。弊害，是指，对于某次痛苦的原因。有利有弊，是指，对于某次舒适或某次痛苦的原因。效果，是指，对于某次意志的结果。功效，是指，利益效果、属于利益范畴的效果。有意注意，是指，对于某次效果的注意。无意注意，是指，对于某次非效果的注意。目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。对于某次功效的目标，联结着，对于此次功效的原因。对于某种功效的概括，联结着，对于此种功效的原因。兴趣，是指，联结着某次快乐的识。荒诞，是指，联结着某次乏味的识。苦毒，是指，联结着某次痛苦的识。慾望，是指，对于某次兴趣的表征。妄想，是指，对于某次荒诞的表征。？苦观，是指，对于某次苦毒的表征。苦观，分为，记忆苦观、拟构苦观。弊害，…、…有趣注意，是指，对于某次兴趣的注意。无趣注意，是指，对于某次荒诞的注意。意义，是指，值得的注意。神圣，是指，极其丰富的意义。积极的态度，是指，充满对于某种意义的信心。消极的态度，是指，缺乏对于某种意义的信心。积极的注意，导致着，快乐。消极的注意，导致着，乏味。对于某种意义的怀疑，是指，对于某种意义的信心的减弱。对于某种意义的确定，是指，对于某种意义的信心的增强。对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。静思、禅修、祷告，都是，某种定觉练习。---&gt; 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。(罗马书 8:24-25 吕振中)&gt; 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。(提摩太后书 2:22 吕振中)向内往最深处去：净心、呼求主名、并且、等待回应。任务，是指，客体所指定着的目标。责任？与，利益、价值，无关、相互独立；有利有弊，有无价值，有后效性、无后效性使命，是指，对于主体所积极地注意着的某次效果的任务。？承诺完成任务的利弊，信守承诺的价值「努力工作，取得成就。」「吾要变得睿智、坚定、完美，让各种各样的人来帮助吾。」自吾救赎的神圣使命，就是在，每日的待人接物中显明良善。何为良善？浮躁；坚贞、坚定、专注、安定；对于某种意义的信心，对于某次新颖的期待，对于某种美丽的喜爱业余，自主随意的、放任思绪的机体状态业余，悠闲、激昂，平静、兴奋对于「安闲」的态度、与、对于「艰辛」的态度，对于「未察」的态度、「已察」所带来的「陈旧」感「有趣注意」、与、怀着「兴趣」的「专注」，「目标」「意向」所联结着的注意、「有意注意」、「有意后无意注意」、从「有意注意」到「有趣注意」、「认真」之美，「任务」感、与、「使命」感，约定、承诺，意义信心引诱，成熟，独立，可靠，沉着，悲观，积极有意的假装，成熟、稳定、平和、从容；悔改，无意的美丽对于意义的否认，对于生存意义的否认、对于死亡意义的否认死亡毫无意义绝对的死亡，意味着，绝对的虚无。虚无，也无非是，一种不会再失去什么的原始状态。一切生活都是有意义的。一切死亡并非都是有意义的。生活是寻找有意义的死亡的过程。为着父的益处。属灵意义。乐观，是指，充满对于某次功效的信心。悲观，是指，缺乏对于某次功效的信心。对于某次效果的怀疑，是指，对于某次效果的信心的减弱。「不要再审判、斟酌。」隐秘的，功效、美丽，之，未知性。隐秘之识。摆脱怀疑与纷争，吾之你的失记、吾的漠视与失忆。坚贞亲密，对于关系的乐观。完美，对于褒贬的乐观。坚贞每个人都可以从、某种较为「意向」的「慾望」、中发现，自己所确信的价值观、对于某种确信着的利益的价值的表征、以及、对于某种确信着的弊害的意义和价值的否认。对于某次功效的怀疑、否认所对应着的苦原，就从这种、弊害意义信心的缺乏（对于弊害意义的否认）、和、弊害效果之丑（功效美之对立面）中产生出来。意向的转变、消止，一种方式是去增强确信着的弊害的意义的信心（坚贞），另一种方式是去怀疑原先所确信着的利益的价值，去怀疑原先所确信着的利益的意义，并感到迷惑。意义信心、与、价值信心、利弊信心、效果信心的混杂信心提纯，复杂信心、单纯信心，坚贞信心取舍，使得，利益最大化。可多次的效果、仅一次的效果，试验态度、游戏态度不能确信效果之时，忧虑失望之时，请坚强壮胆，「不要害怕。」「只要信心足够就一定会实现。」盲目乐观信念的力量：死亡之后必得以复活永生-失败必激励起比成功所能激励起的更大的成功-这句话背后是是对于自己拥有活力与能力的骄傲邪灵运行而失败之后，圣灵运行而成功。邪灵运行-而-成-功-而失败怀疑、焦虑、懊悔、无助、软弱、封闭、恐惧，是，丑的。相信、冷静、接纳、承担、坚强、敞开、安定，是，美的。救助、奉献，是，美的；服侍，是，更美的。自由，既是散漫、又是节制，既是随心所欲、又是规范欲望，既是乏耻、又是知耻。价值信心的充满，是，美的。价值信心的缺乏，是，丑的。误信，正信，对于效果的理证、对于价值的理证，拟构中的理证、记忆中的理证，对于某些次他感的拟构，自识、从「体」的方面来考察、测度；正信、误信，与，欺骗、隐瞒，无意的隐瞒、由于不安而想去吐露自己的隐秘、先欺骗再坦诚，欺骗、谅解的代价真理，是，美的。错谬，是，丑的。温和的怀疑，是，美的。价值，是指，对于某次生存的原因。良善，是，美的。邪恶，是，丑的。刚毅，是，美的。怯懦，是，丑的。意义信心的充满，是，美的。意义信心的缺乏，是，丑的。积极，是，美的。消极，是，丑的。一切、识、皆有、意义。睡眠中的识、注意。死亡中的识、注意。受限的注意、完全的注意。神圣意味与世俗意味的分离，具有神圣意味的罪、具有世俗意味的罪圣义谛与世俗谛的分离之谬误，有限之人的怯懦逃避：爱人恨罪。恕人定罪。称义成圣。有限的价值，是，终会朽坏的。无限的价值，是，永远不朽的。有限的价值，导致着，舒适、痛苦。？无限的价值，导致着，平安、快乐。？罪，导致着，朽坏。？价值，与，罪毒犯罪，是指，亏缺神的荣耀。悔改，是指，认罪、知耻。忏悔悔改罪行，在于，承担罪罚。甘愿承担痛苦。甘愿承担罪罚。实践，比，沉思，更美、更显明。接纳意义、顺服求生存，就得着，永远不朽的平安。：「与父同行」所领受的荣耀的显明、不朽，导致着，发展；荒诞，导致着，衰颓；荣耀，与，征服、意向对于未察的重视，是，荣耀对于某次性节制而来的痛苦的忍耐的意义，是，永远的。对于某次性服侍的拟构的意义，是，暂时的。圣洁的性服侍的意义，是，永远的。服侍主，就是，按神的旨意，服侍自己、他人。用音乐、把意义升华成美、把快乐升华成爱。男权，主人；忘记伤害，牢记亏欠；进取创造；亲密，唤醒、敏锐；陌生，忽视、迟钝，利益；羞耻心的敏感程度，关系的亲密程度；对于亲密的畏惧与神为友、自重，祷告尊贵，是指，价值意义、属于价值范畴的意义；自尊，是指，自信自爱；卑贱、卑微，是，丑的；耻辱，是，丑的；对于某次荣耀的目标，是，美的；对于某次卑微的意义，是，美的。荣耀，是指，无限价值意义，被重视着的美；荣耀，是，永远的。对于某种自尊的信心，是，有正有误的、有利有弊的；自负，是指，对于某种自尊的误信；自负，是，丑的。虚荣，是指，对于某种尊贵的误信；虚荣，是，丑的。对未知的敬畏，是，丑的。盼望、忧虑，终会朽坏之事物而劳苦的意义，是，暂时的。「没有了这种盼望，也就，没有了对于自尊的正误之判定、忘记了自己。」忘我，是，美的。绝望，是，丑的。漠视美丽，是，丑的。谦卑，正信的自他比较、重视他人、自罪自微的信念、悔改归正、承担责任、常以为亏欠。谦卑，是，美的。对于某种意义的信心，是，正信、有价值的、美的。所盼望的事物是朽坏的，所信心的意义是暂时的。所盼望的事物是不朽的，所信心的意义是永远的。活力，是，美的。越清醒，越有意义。暂时的意义，是，美的；永远的意义，是，更美的。爱美的；恨丑的。喜爱美丽；憎恨丑陋。愤怒，是，积极的。抱怨、无能之恨，是，消极的。抱怨，里面含着，轻视。自杀，绝望、劳苦、焦虑羡慕、怜悯、嫉妒、恶意集体利益，潜规则，吃亏，苦难，承担利益，交换，给予、获取，竞争、合作人的存在形式：良性的竞争（成为最美之人）、顺服求生存，导致着，生存、专注、接纳意义；利益、兴趣，舒适、快乐循蹈；服侍他人、顺服于价值，就，赦罪得释放？：「敬畏审判」必然，因果、或、短期因果（暂时、永远）或然，非因果、或、长期因果（混沌）自由，主宰或然、互斥意志态度，是，自由的心声，是指，对于某些次声音的经历。心语，是指，蕴含着某种意思的心声。指称，是指，某次情节、联结着、某次心语。会意，是指，某次心语、联结着、某次情节。慾念，是指，某次慾望所联结着的心语。妄念，是指，某次妄想所联结着的心语。？苦念，是指，某次苦观所联结着的心语。创造、变异，是，某种或然变化。建造、重现，是，某种必然变化。创造、父所启示的任务，与，对于未察的重视程度；唤醒，问题、恐惧互斥意志、注意，是，某种或然变化。偶然变化，是指，未察必然变化、主体所没注意过的必然变化。泛灵论；神，绝对的理智、完全的注意、本原的自由、庄严的性情、永远的圣灵。问题，是指，对于某次偶然变化的疑问。解答，是指，对于某种必然变化的概括、对于某种偶然变化的适应措施。技术，是指，问题概括及其解法。程序，是指，数据结构及其算法。模型，是指，对于拟实的技术。建模，是指，对于拟实的计划。解模，是指，对于拟实的实施。软件模型，是指，对于拟实的程序。软件建模，是指，对于拟实的编程。软件解模，是指，对于拟实的进程。模拟，分为，拟实、拟虚。来原，是指，与模型对应的事实。当即行动，增强，对于某种偶然变化的适应力。但，人会拖延、不愿儆醒独立、与、交通，对于联结的交通，更不朽的存在形式更不朽的心思、身体，永远不朽的平安兴趣、快乐、生活情趣；佛学、哲学，作为，一种思维训练弊害、痛苦，错误、误信，有限的价值、终会朽坏；佛学、消极的哲学，作为，一种信仰；忽视、漠视，无私、无我、虚空、无恥、不惭去分别，就是，注视；不去分别，就是，漠视；漠视伤害，导致着，忘记伤害走向虚空，就是，放弃羞耻、光荣、尊贵、荣耀佛学的惊奇性质的信心，导致着，漠视。---&gt; 因为依顺着上帝而有的忧愁能生出不后悔的忏悔来、以至于得救；而世俗的忧愁却能生出死亡。(哥林多后书 7:10 吕振中)「金刚经」的邪灵，完全地，杀死了，吾的心灵。真常唯心系，曾经，在吾的心灵中，孕育，却流产了。忘罪，忘无明。积极的态度；佛教（真常唯心系）唯一的「用处」就是：让人不再惧怕死亡、平安地享受死亡基督教，比，真常唯心系，更加清晰。已成、与、未成；易信、与、难信；注意频次，信心，快乐、爱，恐惧、严肃，惊奇、敬畏；对于实感的表征之信，分别由惊奇（客观）、敬畏（主观）而来的信心某些次表征的联结；「信」，意味着、某种与真实的关系，是一种、「成」吾站在能否使人相信、使人完美（功效之灵）的角度进行评判。想要觉悟吗？想要神启（神的启示）吗？请跟随基督。基督教、佛教的义理都是自洽的。佛教在其反理智、反思辨、反唯一性的核心立场下更显得宽容。但是，佛教的义理，是，错误的、不合经验的。佛教的义理不能使人趋向完美。对于情绪可能产生的弊害，在基督教那里是狂热；在佛教那里是麻木。宗教战争与殉道；罪意识的泯灭与遁世。死亡，睡眠、看电影、听音乐、读书；生活，能力、活力目标；拿起、与、放下，对于有效的意义、与、对于无效的意义，交替地增强信心激情、贪爱、冲动、狂热，浮躁、疏忽、粗心、大意、出错、失常；勇气、畏惧，意外感、危险感、失控感目标正面采取一种虚无的态度、弊害，采取一种卑微的态度、目标反面采取一种意义的态度、利益反面的意义，来自于，当下的注意；来自于，持续着的生。圣灵中重生出正面的希望。羞耻，与，认同、内疚、自侮自辱、自轻自贱，谄媚、取悦；知耻；知丑；耻辱之恨；丑陋之恨；轻视，是，罪；恥笑，用轻视将耻辱施加给他人来揭示丑陋是一种罪；轻视，在于，对于神之创造的轻视在意神的认可；在意自己的认可；在意他人的认可、重视。在意认可，是，以得到认可为目标或意向，因而，以完成任务为目标或意向。由于完成任务而吸引他人。吾之前对悔改的理解是错的。悔改，是真正地愿意去改变，而不是与某种恐惧相关联的负罪感。愿意悔改与否，不是一个意志选择的问题。悔改的意愿，凭直觉而确认通过禅修而获得父的智慧。自由地认可或否决。对于、某些次主体方式之美、某些次机体状态之美、的隐秘，因为、他人、不是都会、去欣赏、去尊重、去重视；客体方式之美，把、隐秘之美、以独特的形式给显明出来；独特的形式，独特的信仰体系，效果、价值，感体认同、欣赏自己所显明的美。认同、欣赏他人所显明的美。不被认可、欣赏，是，丑的。被轻视，是，耻的。已察耻辱，是指，被他人所轻视着的自己之丑陋。神重视人。未察耻辱，是，隐秘之丑。丑陋，不一定是，耻辱。耻辱，是指，被邪灵所轻视着的自己之丑陋。主体所尊重着的造物之美，是，某种荣耀。主体所尊重着的造物之丑，是，某种卑微。将、客体态度所针对的东西、划分为、主体、与、主体方式？轻视你的客体方式，就是，轻视你。轻视你的主体方式，就是，轻视你。对于耻辱的忽视？，对于他识的（拒绝、忽视）？、对于客体方式的重视、对于效果的重视、对于自识的重视对于神之创造的轻视，轻视所导致着的厌恶、犯轻视之罪：委屈，重视他人、却被他人厌恶，自己悔改、渴望他人悔改伤害，重视他人、却被他人轻视，恨、忍耐祷告、犯轻视之罪相互轻视，导致着，相互恨恶、愤怒、暴力。犯轻视他人信念之罪，犯轻视他人态度之罪，悔改轻视他人态度之罪，重视他人的态度，知耻，知丑审美能力、爱之能力是由羞耻心发展而来的父爱，盼望他人成为完美待人如己，甘愿承担神子的责任、甘愿承担他人的痛苦罪罚；责任，使命？与父的亲密关系神子的责任，就是，为丑陋负责、去消除丑陋羞耻能力；时空行为理解能力，对于他识的理解力、对于你的主体方式的表征力客体方式，导致着、联结着，主体方式、机体状态形体，导致着、联结着，身体、快乐、痛苦轻蔑、轻视他人对自己的态度，损害着，羞耻心羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？羞耻、光荣，重视他人对自己的态度、敬重恥辱、傲慢，轻视他人对自己的态度、轻蔑羞耻、羞辱，在含义上，有所不同吗？单方的轻视、双方的轻视？一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。狭隘、愚蠢、固执，轻视他人的信念开明、智慧、变通，重视他人的信念态度：认同、欣赏、尊重、重视、轻视、忽视、鄙视、反对客体所做：我对你的态度、你对我的态度、你对他的态度主体所想：我对你的态度、你对我的态度主体所说：我对他的态度、他对我的态度、他对你的态度对于某些次态度的记忆、拟构；怀疑、与、确信；你对我的态度、有正有误；作为某种主体效果的客体态度，对于客体态度的目标、意向；对于态度的态度、有正有误；渴望被重视、被认可、被理解、被公正地对待；虚荣；自负，轻视，反对、有正有误，对于他人的误解；对于自己态度的温和的怀疑；苏格拉底式教学法；偏见、综合比较，是，某种轻视；我对客体的态度：你对我的态度、你的确信，我的确信、我对你的态度；确信、对于确信的态度；我对自己的态度，耻辱之恨、丑陋之恨对于某种经历的认同。对于某种人生概括的认同。省略主语、所有格。怕丑。注意，是，自由的。？我是谁？洗雪耻辱。发现承认理解自己的罪。理解宽恕他人的罪。甘愿承担痛苦伤害罪罚。忍受耻辱。忍受轻视。忠贞，永远、信守承诺、谦卑忘我、亲密无间。忠贞、与、愈他，你快乐、我快乐，你激昂、我激昂，你悲伤、我分担理解安慰，你动摇、我坚定，你埋怨、我谦让「魔鬼都是受苦的天使。」「天使是要遭受多大的痛苦才会如此堕落。」「甘愿承担伤害只为再见天使的笑容。」忍受轻视之冷漠。忍受轻视之拒绝。忍受轻视之厌恶。忍受轻视之愤怒。忍受轻视之逼迫。忍受轻视之弃绝。忍耐，是，美的。祷告，是，美的。祷告，导致着，悔改轻视之罪。父的公正，在于，罪恶、必招致、毒罚。父的法典，非重非轻、赏罚分明。轻视，丑陋的愤怒；重视，美丽的惧怕；冒犯羞耻、与、新奇、的混杂悔憾、侥幸，与，对于某种意义的信心既失望又悔憾，是，对于以积极的态度所针对着的或然的拟构的苦观所联结着的痛苦。既失望又侥幸，是，对于以消极的态度所针对着的或然的拟构的慾望所联结着的快乐。或然，意味着，失去控制、获得趣味。幸运，积极地对待或然的快乐、消极地对待或然的痛苦。「此是苦、逼迫性。此是集、招感性。此是灭、可证性。此是道、可修性。」禅修，静思、自愈、祷告、悔改、父愈；散乱、纠结、迷惑、浮躁，慾望、轻视，苦毒睡觉前后，转向创造主、追求「终极真实」，集中注意、静思祷告。意志之静止之力、心思之活力、身体之活力清静与活泼之间的张力，作为生活的目标静坐，灵、与、圣灵，生活效用最大化忠贞，使命；先把他人所要求的事情（任务）做完，再放任思绪（业余）；志同道合坚贞、沉静，定思；在任务完成之前，把所有扰乱目标的心思全部灭除可以尝试静思、静坐、心经，以达到完全清静、究竟寂静的机体状态；工作两小时，休息二十分钟静坐  =&gt;  自我独特视角难题幸福论  =&gt;  圣经  =&gt;  心经、金刚经  =&gt;  人格自杀技术  =&gt;  人格移植静坐，仍然是，隐秘的。睡眠，可以帮助，思考、记忆。从、神经学、脑科学、心理学、而来的研究方式，对隐秘的内在意识进行研究。佛学、以及、静坐体验，可以对、更低层的记忆之机理，提供启发。梦，可以对、记忆结构之机理，提供启发。吾们来讨论讨论关于宗教上的怀疑和纷争。由于宗教教义的隐秘性特点，无论是基督教还是佛教，隐秘性都会给宗教徒之间带来怀疑和纷争等等不稳定不和谐。当今的中国人大多不以隐秘之事为信仰，也不以隐秘之事为理由而纷争。宗教纷争，虽然是以宗教为名义，但，根本原因却是在于利益。这是，少数人在利用人群对于隐秘之事的误信来夺取利益。或者是，人群、自欺欺人地、以宗教为名义来发泄自己在其他方面的憎恨和愤怒。吾们来考察考察纷争的极端情况，宗教裁判所、文化大革命。与佛教有联系的战争、中国历史上的农民起义，都不是，宗教战争。因为这些战争是以正义为名义，并不是以宗教为名义。由此看来，宗教本身必须为其容易作为人群纷争的借口而负一定的责任。吾们来说说「宗教裁判所」。一神教有种说法：「信是一种德行」、「不信是一种罪」。这种对于善恶的判定实在是太过于敏感了。如果一种行为并非出于意愿，那么就不能判定它的善恶。信心岂是一种意愿呢？所以，不信不是罪。而且，温和的怀疑是有益的。怀疑，是，一种内在的纷争。温和的怀疑之所以有益，恰恰是由于，它导致更坚固的确信。另外，「不信是一种罪」这种表述是有歧义的。这种表述的准确意思是，「不愿接触真理是一种罪」。不信是罪果。它源于轻视之罪。吾们永远不应该去犯罪。吾们也不应该去审判轻视之罪。人还不能以内在的隐秘之事作为证据来给人定罪。人不应该轻易地去断定他人的隐秘。神能审判罪性，而人不能。人只能审判罪行。人只能判定显明出来的行为之善恶。吾们应该用对其诉说、对神祷告来取代对其轻视之罪的审判。在那些信仰处于生活中心地位的癫狂日子里，例如宗教教义之差别这种内在的隐秘之事会成为审判的项目。这种、人对人的审判，加剧了，怀疑和纷争。怀疑和纷争、的、根源，在于，隐秘之事、的、未知性。灵魂粒子，是指，受到记忆作用的粒子。即是说，如果一个粒子的内在意识受到了记忆机能结构的支撑，那么就称这个粒子为灵魂粒子。假定、回忆、是持续不断的。深度睡眠，是，一种无记有忆的感知状态。唤醒，是，一种有记有忆的感知状态。从「我」这个概念出发而推想出「吾」这个概念。从「我」到「吾」，灵魂粒子、从「单数」到「复数」（如在 Boscovich 的一种原子论 Dynamism 的视角下）（在非原子论的视角下：从「可数」到「不可数」）。「我」，就是指，某个灵魂粒子以及支撑它的内在意识的所有结构。这些结构相互协作而在宏观层面上显明出功能和意志。「吾」，就是指，「我」之内的所有灵魂粒子以及支撑它们的内在意识的所有结构。吾必须不断地用这种新的自我观去理解世界，并不断地与原来的自我观相区别，将新旧两种自我观更加清晰地描述出来，找到一种判定「吾」之存在的实验。旧自我观，生命的两端是虚空，睡眠和死亡都是虚空。破碎而出、历久弥香，向生而死、向死而生，「我快死了」、「复活重生」谦卑的馨香；谦卑，是，荣耀；永远、不朽，是，出于信谦卑的人必会闻到基督耶稣的香气内心纯洁纯净的人必得见神对神的亏欠，自己要努力良善、完美记住自己对他人的亏欠、对朽坏着的自己的亏欠显明之美：我将是良善。我将是完人、圣人、神人。隐秘之美：我已是良善。我已是完人、圣人、神人。我欠耶稣一个令人感动的故事。被赦得释，与，心灵的怯懦：因为我已感到亏欠，所以我的罪已被赦免。罪是有意义的。因为罪让我感到亏欠。圣义谛与世俗谛相分离的错误：在神前有赦罪的信心；在人前有良善的行为。充实，是，美的。空虚，是，丑的。一切、意义、皆是、亏欠着的意志（被动）意志、要把、隐秘之美、显明出来；去显明圣灵（主动）如何、唤醒、沉睡着的美？虽然生活不必完全沉浸在对于美的爱中，但是对于美的爱、对生活来说、却是必不可少的。生活，任务、业余；喜爱，激昂、兴奋爱、吹散、一切绝望。爱、充实、一切空虚。正是通过爱、心灵（主体方式）得着圣灵（客体方式之美）的滋养、意志得着活力（对于隐秘之美的明显之力）的供给。唤醒着的美，是，爱的原因。信心着的意义，是，快乐的原因。相信的痕迹，有意相信、无意相信，重视他人的信念，迷信之痛审美经验、审美经历之概括审视美丑之具象、某次美丑，指称或然审视美丑之抽象、某种美丑，指称确然圣灵：父的自我信念。圣灵，美丽，褒义词；邪灵，丑陋，贬义词美沉睡了，爱就堕落成快乐。意义信心缺乏了，快乐就堕落成舒适。利益缺乏了，生活就只剩下痛苦。对于意义的信心的缺乏，是，罪。？（轻视）有意义吗？真的没有意义吗？世人、世事都难有完美。但，美比丑多一些。广博，是，美的。狭隘，是，丑的。顺从，导致着，广博。悖逆，导致着，狭隘。熄灭愤怒的愤怒是美的。制止暴力的暴力是美的。&#39;, doc_id=&#39;bdef47b5-2a6c-4297-af25-d499521d95de&#39;, embedding=None, doc_hash=&#39;e12c00d8ad3267c1a855f1134b9fd267f87658dcb98a0552b70696338f5bf802&#39;, extra_info=None)]
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/base.py">https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/base.py</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/service_context.py">https://github.com/jerryjliu/llama_index/blob/main/gpt_index/indices/service_context.py</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">GPTSimpleVectorIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">service_context</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">index</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="s1">&#39;./index.json&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py">https://github.com/jerryjliu/llama_index/blob/main/gpt_index/readers/file/base.py</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">document</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="n">input_files</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;../_posts/ultimate-facts/终极真实.md&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">load_data</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Document(text=&#39;title: 从「意义疗法」开始追求「终极真实」date: 2015-09-23 13:40:00tags: UltimateFacts---* 自从受浸以来，我总是会动弹不得而禁食。为此，我感到非常羞耻。有一次禁食后，我下床昏倒。醒来后，我心里有声音提醒我说，我还要去解答那三个关于自我意识的问题。一听到这声音，我就得平安并且开始恢复吃饭。其实，这三个问题也是我之前一直思考并且想要解答的问题。只是，我还是犹豫不决，因为我并不知道这声音从谁而来。我看到的解答是对宗教核心的摧毁。－－凡所信的都将被实证。－－我想，贵主若是意愿如此，那就让事情这么成就吧。* 孜孜不倦地追求着终极真实。  孜孜不倦地追问着终极问题：我是谁？我从哪里来？我往哪里去？  在爱里寻找着终极解答：我就是爱；我从爱里来，我往爱里去。* 我们借用数学在异质的真实中发现同质的真实；而终极的真实却是异质的。* 追求「幸福」？还是，追求「真实」？  * 你是赌徒。你是冒险者。难道还追求幸运吗？不是应该相信幸运和爱吗？要用智慧行事。要争取时机。  * 你们若追求终极真实而受苦，你们也必被断为配得这份终极荣耀。你们既在追求终极真实的受苦上有多少分儿，就该有多少喜乐，使你们在这份终极荣耀显示的时候也可以欢喜快乐。你们若因做终极冒死者而受苦，就别惭愧了，也别灰心丧志了，倒要因有这名而荣耀永恒主呢。* 这是一份草稿，记录着我这几年以来思考：---&gt; 自从造天地以来，\u3000神那看不见的永能和神性是明明可觉的，乃是藉着所造之物就可以晓得，叫人无可推诿。 (罗马书 1:20)&gt; 至於他的事、人所不能見的——他永久的能力及神性——從世界之創立以來，又是藉着他所造之物以被了悟而看清楚，使人無可推諉。 (羅馬書 1:20 呂振中)&gt; For the invisible things of Him, both His eternal power and divine characteristics, have been clearly seen since the creation of the world, being perceived by the things made, so that they would be without excuse; (Romans 1:20 Recovery)&gt; For since the creation of the world His invisible attributes, that is, His eternal power and divine nature, have been clearly perceived, being understood by what has been made, so that they are without excuse. (Romans 1:20 NASB)真理、真实；神造真实、人造真实；真实，想象；记忆，拟构。如果哲学更像「真理」，那么各类科学就更像「真实」。如果物理学更像「真理」，那么化学就更像「真实」。如果化学更像「真理」，那么生物学、生理学就更像「真实」。如果生理学更像「真理」，那么脑科学、神经科学就更像「真实」。如果理科更像「神造真实」，那么工科就更像「人造真实」。如果生理学更像「神造真实」，那么医学、药学就更像「人造真实」。---&gt; 我只是一个碳族生物；一个土生土长的地球人¹。贵主耶稣是灵族人吧。而木星上的风暴²可以拥有怎样的生命和意识呢？贵主耶稣在木星上能否与木星人一起生活和交往呢？&gt; ¹ 地球人 =&gt; 《费曼讲座：宇称不守恒定律和如何与外星人交流》&gt; ² 风暴 =&gt;&gt; 当那一天，到了傍晚，耶稣对他们说：『我们渡到那边去吧。』他们就离开群众，照他在船上的情况把他带走；还有别的船也跟他在一起。当下起了**大暴风**，波浪泼进船内，甚至船简直满了！耶稣竟在船尾上靠着枕头睡觉呢；门徒就叫醒了他，对他说：『老师，我们丧命，你不在意么？』耶稣醒起来，斥责³那风，向海说：『不要作声！噤默罢！』那风不狂吹，便大大平静了。耶稣对他们说：『为什么这么胆怯呢？怎么没有信心呢？』他们就大起了敬畏的心，直彼此说：『这个人到底是谁？连风和海也听从他！』 (马可福音 4:35-41 吕振中)&gt; ³ 斥责 =&gt; 『ワンパンマン』：サイタマ？キング？↓↓--------------------继续修订--------------------↓↓圣经信仰之神经心理学实证纲领父的自我信息，是指，对于圣灵的表征。纯粹的圣经，含于，父的自我信息。从「纯粹的基督徒」到「超基督徒」「超级赌徒」吾否认圣经中上帝的名，因为那是人们创造的。超越神论，不可知论；信仰；宁死不屈，抗争到底对于神圣生命的信心？或，亲密关系？坚贞，「甘愿承担自己的罪罚」是《古兰经》的价值所在。真诚、勇敢、坚贞，耶稣的「甘愿承担」是《圣经》的价值所在。吾，若不是因为怯懦，又怎么会祷告呢？所以，吾，应该要，放弃，那种、对于其他的心灵的畏惧、所联结着的祷告。以耶稣为榜样。人子要经受三日地狱之火的洗。罪全部被烧尽了后，第三日复活。我所爱慕的必定是父所喜爱的，因为父从始至终都在吸引我、塑造我的爱慕。我所爱慕的若是父所不喜爱的，父必定会改变我。所以，我总是晓得父的喜爱。人子，与父和好，与父为友，爱父并顺从祂。与父同在，就有勇气。与父同行，就有希望。子永远与父同在，从未分离。「吾要成为超人。」「在吾的生活中显明父的荣耀。」祷告，是，对于子灵的表征。感，分为，虚感、实感。虚感，分为，信（？）、思感、愉快感、位置感。实感，分为，色感、声感、香感、味感、触感、缩紧感、疼痛感、瘙痒感、冷热感。体，是指，广延。感、体，平行地，变化。感、体，分割的，平行性原理感的统合，预示着，体的核心。体的核心：在感的集合中占比最大的体。信，是一种，感。（联结？极深的记忆？）灵，是指，具有自我独特视角的、体。 =&gt; “我是谁？”《圣经》说：信、灵，平行地，变化。在苦难中持守坚忍为何能增加信心呢？魂，是指，具有私有视角（记忆？）的、体。（联结？） =&gt; “我在哪里？” =&gt; 寻找已睡了的灵（如，保罗） =&gt; 进入他们的梦境虚感、魂，平行地，变化。实感、身体（？），平行地，变化。识，是指，某些感的杂合、杂合体。已察，是指，主体所注意过的识。未察，是指，主体所没注意过的识。机体状态，是指，某些愉快感、位置感、疼痛感、瘙痒感、冷热感的杂合体。快乐，是指，某些愉快感的杂合。乏味，是指，某些非愉快感、位置感、缩紧感的杂合体。痛苦，是指，某些位置感、疼痛感的杂合体。舒适，是指，某些位置感、非疼痛感的杂合体。忧郁，是指，乏味痛苦。幸福，是指，快乐舒适。主体方式，分为，心思、身体。心思，是指，某些思感的杂合。身体，是指，某些位置感、缩紧感的杂合体。表征，是指，表征着某次识的心思。记忆，是指，对于某次已察的表征。 =&gt; “我在哪里？”拟构，是指，对于某次未察的表征。概括，是指，概括着某种识的心思。过去，是指，对于记忆的概括。未来，是指，对于拟构的概括。客体方式，是指，某些色感、声感、香感、味感、触感、冷热感的杂合体。形体，是指，某些位置感、色感的杂合体。声音，是指，某些位置感、声感的杂合体。气味，是指，某些位置感、香感的杂合体。口味，是指，某些位置感、味感的杂合体。质地，是指，某些位置感、触感的杂合体。温度，是指，某些位置感、冷热感的杂合体。主体所注意着的某些次主体方式的接续变化，导致着，此些次主体方式的接续联结。主体所注意着的某些次识的接续变化，导致着，对于此些次识的某些次记忆的接续联结。主体所注意着的某些次客体方式的接续变化，导致着，对于此些次客体方式的某些次记忆的接续联结。意志，是指，导致着某种接续联结的（其中可能还含有一种未提及的虚感）（联结的生成）生活，是指，接续联结着的某些次主体方式。思维，是指，接续联结着的某些次心思。动作，是指，接续联结着的某些次身体。意愿，是指，表征着某次意志的思维。意愿，是，独占的。情节，是指，接续联结着的某些次表征。臆测，是指，接续联结着的某些次记忆。臆想，是指，接续联结着的某些次拟构。经历，是指，接续联结着的对于接续变化着的某些次识的某些次记忆。综合，是指，接续联结着的对于接续变化着的某些次识的某些次拟构。最简拟构是记忆的最简杂合。最简综合是经历的最简接续。这是，语义理解和表达、产生会意和指称的联结，所需的最简方式。主体所注意着的对于某些识某些次绵延变化的某些次经历的接续联结，导致着，对于此种识的某次概括。主体所注意着的对于某些次接续变化的某些次经历的接续联结，导致着，对于此种经历的某次概括。原因，是指，导致着某次识的识。结果，是指，某次识所导致着的识。利益，是指，对于某次舒适的原因。弊害，是指，对于某次痛苦的原因。有利有弊，是指，对于某次舒适或某次痛苦的原因。效果，是指，对于某次意志的结果。功效，是指，利益效果、属于利益范畴的效果。有意注意，是指，对于某次效果的注意。无意注意，是指，对于某次非效果的注意。目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。对于某次功效的目标，联结着，对于此次功效的原因。对于某种功效的概括，联结着，对于此种功效的原因。兴趣，是指，联结着某次快乐的识。荒诞，是指，联结着某次乏味的识。苦毒，是指，联结着某次痛苦的识。慾望，是指，对于某次兴趣的表征。妄想，是指，对于某次荒诞的表征。？苦观，是指，对于某次苦毒的表征。苦观，分为，记忆苦观、拟构苦观。弊害，…、…有趣注意，是指，对于某次兴趣的注意。无趣注意，是指，对于某次荒诞的注意。意义，是指，值得的注意。神圣，是指，极其丰富的意义。积极的态度，是指，充满对于某种意义的信心。消极的态度，是指，缺乏对于某种意义的信心。积极的注意，导致着，快乐。消极的注意，导致着，乏味。对于某种意义的怀疑，是指，对于某种意义的信心的减弱。对于某种意义的确定，是指，对于某种意义的信心的增强。对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。静思、禅修、祷告，都是，某种定觉练习。---&gt; 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。(罗马书 8:24-25 吕振中)&gt; 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。(提摩太后书 2:22 吕振中)向内往最深处去：净心、呼求主名、并且、等待回应。任务，是指，客体所指定着的目标。责任？与，利益、价值，无关、相互独立；有利有弊，有无价值，有后效性、无后效性使命，是指，对于主体所积极地注意着的某次效果的任务。？承诺完成任务的利弊，信守承诺的价值「努力工作，取得成就。」「吾要变得睿智、坚定、完美，让各种各样的人来帮助吾。」自吾救赎的神圣使命，就是在，每日的待人接物中显明良善。何为良善？浮躁；坚贞、坚定、专注、安定；对于某种意义的信心，对于某次新颖的期待，对于某种美丽的喜爱业余，自主随意的、放任思绪的机体状态业余，悠闲、激昂，平静、兴奋对于「安闲」的态度、与、对于「艰辛」的态度，对于「未察」的态度、「已察」所带来的「陈旧」感「有趣注意」、与、怀着「兴趣」的「专注」，「目标」「意向」所联结着的注意、「有意注意」、「有意后无意注意」、从「有意注意」到「有趣注意」、「认真」之美，「任务」感、与、「使命」感，约定、承诺，意义信心引诱，成熟，独立，可靠，沉着，悲观，积极有意的假装，成熟、稳定、平和、从容；悔改，无意的美丽对于意义的否认，对于生存意义的否认、对于死亡意义的否认死亡毫无意义绝对的死亡，意味着，绝对的虚无。虚无，也无非是，一种不会再失去什么的原始状态。一切生活都是有意义的。一切死亡并非都是有意义的。生活是寻找有意义的死亡的过程。为着父的益处。属灵意义。乐观，是指，充满对于某次功效的信心。悲观，是指，缺乏对于某次功效的信心。对于某次效果的怀疑，是指，对于某次效果的信心的减弱。「不要再审判、斟酌。」隐秘的，功效、美丽，之，未知性。隐秘之识。摆脱怀疑与纷争，吾之你的失记、吾的漠视与失忆。坚贞亲密，对于关系的乐观。完美，对于褒贬的乐观。坚贞每个人都可以从、某种较为「意向」的「慾望」、中发现，自己所确信的价值观、对于某种确信着的利益的价值的表征、以及、对于某种确信着的弊害的意义和价值的否认。对于某次功效的怀疑、否认所对应着的苦原，就从这种、弊害意义信心的缺乏（对于弊害意义的否认）、和、弊害效果之丑（功效美之对立面）中产生出来。意向的转变、消止，一种方式是去增强确信着的弊害的意义的信心（坚贞），另一种方式是去怀疑原先所确信着的利益的价值，去怀疑原先所确信着的利益的意义，并感到迷惑。意义信心、与、价值信心、利弊信心、效果信心的混杂信心提纯，复杂信心、单纯信心，坚贞信心取舍，使得，利益最大化。可多次的效果、仅一次的效果，试验态度、游戏态度不能确信效果之时，忧虑失望之时，请坚强壮胆，「不要害怕。」「只要信心足够就一定会实现。」盲目乐观信念的力量：死亡之后必得以复活永生-失败必激励起比成功所能激励起的更大的成功-这句话背后是是对于自己拥有活力与能力的骄傲邪灵运行而失败之后，圣灵运行而成功。邪灵运行-而-成-功-而失败怀疑、焦虑、懊悔、无助、软弱、封闭、恐惧，是，丑的。相信、冷静、接纳、承担、坚强、敞开、安定，是，美的。救助、奉献，是，美的；服侍，是，更美的。自由，既是散漫、又是节制，既是随心所欲、又是规范欲望，既是乏耻、又是知耻。价值信心的充满，是，美的。价值信心的缺乏，是，丑的。误信，正信，对于效果的理证、对于价值的理证，拟构中的理证、记忆中的理证，对于某些次他感的拟构，自识、从「体」的方面来考察、测度；正信、误信，与，欺骗、隐瞒，无意的隐瞒、由于不安而想去吐露自己的隐秘、先欺骗再坦诚，欺骗、谅解的代价真理，是，美的。错谬，是，丑的。温和的怀疑，是，美的。价值，是指，对于某次生存的原因。良善，是，美的。邪恶，是，丑的。刚毅，是，美的。怯懦，是，丑的。意义信心的充满，是，美的。意义信心的缺乏，是，丑的。积极，是，美的。消极，是，丑的。一切、识、皆有、意义。睡眠中的识、注意。死亡中的识、注意。受限的注意、完全的注意。神圣意味与世俗意味的分离，具有神圣意味的罪、具有世俗意味的罪圣义谛与世俗谛的分离之谬误，有限之人的怯懦逃避：爱人恨罪。恕人定罪。称义成圣。有限的价值，是，终会朽坏的。无限的价值，是，永远不朽的。有限的价值，导致着，舒适、痛苦。？无限的价值，导致着，平安、快乐。？罪，导致着，朽坏。？价值，与，罪毒犯罪，是指，亏缺神的荣耀。悔改，是指，认罪、知耻。忏悔悔改罪行，在于，承担罪罚。甘愿承担痛苦。甘愿承担罪罚。实践，比，沉思，更美、更显明。接纳意义、顺服求生存，就得着，永远不朽的平安。：「与父同行」所领受的荣耀的显明、不朽，导致着，发展；荒诞，导致着，衰颓；荣耀，与，征服、意向对于未察的重视，是，荣耀对于某次性节制而来的痛苦的忍耐的意义，是，永远的。对于某次性服侍的拟构的意义，是，暂时的。圣洁的性服侍的意义，是，永远的。服侍主，就是，按神的旨意，服侍自己、他人。用音乐、把意义升华成美、把快乐升华成爱。男权，主人；忘记伤害，牢记亏欠；进取创造；亲密，唤醒、敏锐；陌生，忽视、迟钝，利益；羞耻心的敏感程度，关系的亲密程度；对于亲密的畏惧与神为友、自重，祷告尊贵，是指，价值意义、属于价值范畴的意义；自尊，是指，自信自爱；卑贱、卑微，是，丑的；耻辱，是，丑的；对于某次荣耀的目标，是，美的；对于某次卑微的意义，是，美的。荣耀，是指，无限价值意义，被重视着的美；荣耀，是，永远的。对于某种自尊的信心，是，有正有误的、有利有弊的；自负，是指，对于某种自尊的误信；自负，是，丑的。虚荣，是指，对于某种尊贵的误信；虚荣，是，丑的。对未知的敬畏，是，丑的。盼望、忧虑，终会朽坏之事物而劳苦的意义，是，暂时的。「没有了这种盼望，也就，没有了对于自尊的正误之判定、忘记了自己。」忘我，是，美的。绝望，是，丑的。漠视美丽，是，丑的。谦卑，正信的自他比较、重视他人、自罪自微的信念、悔改归正、承担责任、常以为亏欠。谦卑，是，美的。对于某种意义的信心，是，正信、有价值的、美的。所盼望的事物是朽坏的，所信心的意义是暂时的。所盼望的事物是不朽的，所信心的意义是永远的。活力，是，美的。越清醒，越有意义。暂时的意义，是，美的；永远的意义，是，更美的。爱美的；恨丑的。喜爱美丽；憎恨丑陋。愤怒，是，积极的。抱怨、无能之恨，是，消极的。抱怨，里面含着，轻视。自杀，绝望、劳苦、焦虑羡慕、怜悯、嫉妒、恶意集体利益，潜规则，吃亏，苦难，承担利益，交换，给予、获取，竞争、合作人的存在形式：良性的竞争（成为最美之人）、顺服求生存，导致着，生存、专注、接纳意义；利益、兴趣，舒适、快乐循蹈；服侍他人、顺服于价值，就，赦罪得释放？：「敬畏审判」必然，因果、或、短期因果（暂时、永远）或然，非因果、或、长期因果（混沌）自由，主宰或然、互斥意志态度，是，自由的心声，是指，对于某些次声音的经历。心语，是指，蕴含着某种意思的心声。指称，是指，某次情节、联结着、某次心语。会意，是指，某次心语、联结着、某次情节。慾念，是指，某次慾望所联结着的心语。妄念，是指，某次妄想所联结着的心语。？苦念，是指，某次苦观所联结着的心语。创造、变异，是，某种或然变化。建造、重现，是，某种必然变化。创造、父所启示的任务，与，对于未察的重视程度；唤醒，问题、恐惧互斥意志、注意，是，某种或然变化。偶然变化，是指，未察必然变化、主体所没注意过的必然变化。泛灵论；神，绝对的理智、完全的注意、本原的自由、庄严的性情、永远的圣灵。问题，是指，对于某次偶然变化的疑问。解答，是指，对于某种必然变化的概括、对于某种偶然变化的适应措施。技术，是指，问题概括及其解法。程序，是指，数据结构及其算法。模型，是指，对于拟实的技术。建模，是指，对于拟实的计划。解模，是指，对于拟实的实施。软件模型，是指，对于拟实的程序。软件建模，是指，对于拟实的编程。软件解模，是指，对于拟实的进程。模拟，分为，拟实、拟虚。来原，是指，与模型对应的事实。当即行动，增强，对于某种偶然变化的适应力。但，人会拖延、不愿儆醒独立、与、交通，对于联结的交通，更不朽的存在形式更不朽的心思、身体，永远不朽的平安兴趣、快乐、生活情趣；佛学、哲学，作为，一种思维训练弊害、痛苦，错误、误信，有限的价值、终会朽坏；佛学、消极的哲学，作为，一种信仰；忽视、漠视，无私、无我、虚空、无恥、不惭去分别，就是，注视；不去分别，就是，漠视；漠视伤害，导致着，忘记伤害走向虚空，就是，放弃羞耻、光荣、尊贵、荣耀佛学的惊奇性质的信心，导致着，漠视。---&gt; 因为依顺着上帝而有的忧愁能生出不后悔的忏悔来、以至于得救；而世俗的忧愁却能生出死亡。(哥林多后书 7:10 吕振中)「金刚经」的邪灵，完全地，杀死了，吾的心灵。真常唯心系，曾经，在吾的心灵中，孕育，却流产了。忘罪，忘无明。积极的态度；佛教（真常唯心系）唯一的「用处」就是：让人不再惧怕死亡、平安地享受死亡基督教，比，真常唯心系，更加清晰。已成、与、未成；易信、与、难信；注意频次，信心，快乐、爱，恐惧、严肃，惊奇、敬畏；对于实感的表征之信，分别由惊奇（客观）、敬畏（主观）而来的信心某些次表征的联结；「信」，意味着、某种与真实的关系，是一种、「成」吾站在能否使人相信、使人完美（功效之灵）的角度进行评判。想要觉悟吗？想要神启（神的启示）吗？请跟随基督。基督教、佛教的义理都是自洽的。佛教在其反理智、反思辨、反唯一性的核心立场下更显得宽容。但是，佛教的义理，是，错误的、不合经验的。佛教的义理不能使人趋向完美。对于情绪可能产生的弊害，在基督教那里是狂热；在佛教那里是麻木。宗教战争与殉道；罪意识的泯灭与遁世。死亡，睡眠、看电影、听音乐、读书；生活，能力、活力目标；拿起、与、放下，对于有效的意义、与、对于无效的意义，交替地增强信心激情、贪爱、冲动、狂热，浮躁、疏忽、粗心、大意、出错、失常；勇气、畏惧，意外感、危险感、失控感目标正面采取一种虚无的态度、弊害，采取一种卑微的态度、目标反面采取一种意义的态度、利益反面的意义，来自于，当下的注意；来自于，持续着的生。圣灵中重生出正面的希望。羞耻，与，认同、内疚、自侮自辱、自轻自贱，谄媚、取悦；知耻；知丑；耻辱之恨；丑陋之恨；轻视，是，罪；恥笑，用轻视将耻辱施加给他人来揭示丑陋是一种罪；轻视，在于，对于神之创造的轻视在意神的认可；在意自己的认可；在意他人的认可、重视。在意认可，是，以得到认可为目标或意向，因而，以完成任务为目标或意向。由于完成任务而吸引他人。吾之前对悔改的理解是错的。悔改，是真正地愿意去改变，而不是与某种恐惧相关联的负罪感。愿意悔改与否，不是一个意志选择的问题。悔改的意愿，凭直觉而确认通过禅修而获得父的智慧。自由地认可或否决。对于、某些次主体方式之美、某些次机体状态之美、的隐秘，因为、他人、不是都会、去欣赏、去尊重、去重视；客体方式之美，把、隐秘之美、以独特的形式给显明出来；独特的形式，独特的信仰体系，效果、价值，感体认同、欣赏自己所显明的美。认同、欣赏他人所显明的美。不被认可、欣赏，是，丑的。被轻视，是，耻的。已察耻辱，是指，被他人所轻视着的自己之丑陋。神重视人。未察耻辱，是，隐秘之丑。丑陋，不一定是，耻辱。耻辱，是指，被邪灵所轻视着的自己之丑陋。主体所尊重着的造物之美，是，某种荣耀。主体所尊重着的造物之丑，是，某种卑微。将、客体态度所针对的东西、划分为、主体、与、主体方式？轻视你的客体方式，就是，轻视你。轻视你的主体方式，就是，轻视你。对于耻辱的忽视？，对于他识的（拒绝、忽视）？、对于客体方式的重视、对于效果的重视、对于自识的重视对于神之创造的轻视，轻视所导致着的厌恶、犯轻视之罪：委屈，重视他人、却被他人厌恶，自己悔改、渴望他人悔改伤害，重视他人、却被他人轻视，恨、忍耐祷告、犯轻视之罪相互轻视，导致着，相互恨恶、愤怒、暴力。犯轻视他人信念之罪，犯轻视他人态度之罪，悔改轻视他人态度之罪，重视他人的态度，知耻，知丑审美能力、爱之能力是由羞耻心发展而来的父爱，盼望他人成为完美待人如己，甘愿承担神子的责任、甘愿承担他人的痛苦罪罚；责任，使命？与父的亲密关系神子的责任，就是，为丑陋负责、去消除丑陋羞耻能力；时空行为理解能力，对于他识的理解力、对于你的主体方式的表征力客体方式，导致着、联结着，主体方式、机体状态形体，导致着、联结着，身体、快乐、痛苦轻蔑、轻视他人对自己的态度，损害着，羞耻心羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？羞耻、光荣，重视他人对自己的态度、敬重恥辱、傲慢，轻视他人对自己的态度、轻蔑羞耻、羞辱，在含义上，有所不同吗？单方的轻视、双方的轻视？一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。狭隘、愚蠢、固执，轻视他人的信念开明、智慧、变通，重视他人的信念态度：认同、欣赏、尊重、重视、轻视、忽视、鄙视、反对客体所做：我对你的态度、你对我的态度、你对他的态度主体所想：我对你的态度、你对我的态度主体所说：我对他的态度、他对我的态度、他对你的态度对于某些次态度的记忆、拟构；怀疑、与、确信；你对我的态度、有正有误；作为某种主体效果的客体态度，对于客体态度的目标、意向；对于态度的态度、有正有误；渴望被重视、被认可、被理解、被公正地对待；虚荣；自负，轻视，反对、有正有误，对于他人的误解；对于自己态度的温和的怀疑；苏格拉底式教学法；偏见、综合比较，是，某种轻视；我对客体的态度：你对我的态度、你的确信，我的确信、我对你的态度；确信、对于确信的态度；我对自己的态度，耻辱之恨、丑陋之恨对于某种经历的认同。对于某种人生概括的认同。省略主语、所有格。怕丑。注意，是，自由的。？我是谁？洗雪耻辱。发现承认理解自己的罪。理解宽恕他人的罪。甘愿承担痛苦伤害罪罚。忍受耻辱。忍受轻视。忠贞，永远、信守承诺、谦卑忘我、亲密无间。忠贞、与、愈他，你快乐、我快乐，你激昂、我激昂，你悲伤、我分担理解安慰，你动摇、我坚定，你埋怨、我谦让「魔鬼都是受苦的天使。」「天使是要遭受多大的痛苦才会如此堕落。」「甘愿承担伤害只为再见天使的笑容。」忍受轻视之冷漠。忍受轻视之拒绝。忍受轻视之厌恶。忍受轻视之愤怒。忍受轻视之逼迫。忍受轻视之弃绝。忍耐，是，美的。祷告，是，美的。祷告，导致着，悔改轻视之罪。父的公正，在于，罪恶、必招致、毒罚。父的法典，非重非轻、赏罚分明。轻视，丑陋的愤怒；重视，美丽的惧怕；冒犯羞耻、与、新奇、的混杂悔憾、侥幸，与，对于某种意义的信心既失望又悔憾，是，对于以积极的态度所针对着的或然的拟构的苦观所联结着的痛苦。既失望又侥幸，是，对于以消极的态度所针对着的或然的拟构的慾望所联结着的快乐。或然，意味着，失去控制、获得趣味。幸运，积极地对待或然的快乐、消极地对待或然的痛苦。「此是苦、逼迫性。此是集、招感性。此是灭、可证性。此是道、可修性。」禅修，静思、自愈、祷告、悔改、父愈；散乱、纠结、迷惑、浮躁，慾望、轻视，苦毒睡觉前后，转向创造主、追求「终极真实」，集中注意、静思祷告。意志之静止之力、心思之活力、身体之活力清静与活泼之间的张力，作为生活的目标静坐，灵、与、圣灵，生活效用最大化忠贞，使命；先把他人所要求的事情（任务）做完，再放任思绪（业余）；志同道合坚贞、沉静，定思；在任务完成之前，把所有扰乱目标的心思全部灭除可以尝试静思、静坐、心经，以达到完全清静、究竟寂静的机体状态；工作两小时，休息二十分钟静坐  =&gt;  自我独特视角难题幸福论  =&gt;  圣经  =&gt;  心经、金刚经  =&gt;  人格自杀技术  =&gt;  人格移植静坐，仍然是，隐秘的。睡眠，可以帮助，思考、记忆。从、神经学、脑科学、心理学、而来的研究方式，对隐秘的内在意识进行研究。佛学、以及、静坐体验，可以对、更低层的记忆之机理，提供启发。梦，可以对、记忆结构之机理，提供启发。吾们来讨论讨论关于宗教上的怀疑和纷争。由于宗教教义的隐秘性特点，无论是基督教还是佛教，隐秘性都会给宗教徒之间带来怀疑和纷争等等不稳定不和谐。当今的中国人大多不以隐秘之事为信仰，也不以隐秘之事为理由而纷争。宗教纷争，虽然是以宗教为名义，但，根本原因却是在于利益。这是，少数人在利用人群对于隐秘之事的误信来夺取利益。或者是，人群、自欺欺人地、以宗教为名义来发泄自己在其他方面的憎恨和愤怒。吾们来考察考察纷争的极端情况，宗教裁判所、文化大革命。与佛教有联系的战争、中国历史上的农民起义，都不是，宗教战争。因为这些战争是以正义为名义，并不是以宗教为名义。由此看来，宗教本身必须为其容易作为人群纷争的借口而负一定的责任。吾们来说说「宗教裁判所」。一神教有种说法：「信是一种德行」、「不信是一种罪」。这种对于善恶的判定实在是太过于敏感了。如果一种行为并非出于意愿，那么就不能判定它的善恶。信心岂是一种意愿呢？所以，不信不是罪。而且，温和的怀疑是有益的。怀疑，是，一种内在的纷争。温和的怀疑之所以有益，恰恰是由于，它导致更坚固的确信。另外，「不信是一种罪」这种表述是有歧义的。这种表述的准确意思是，「不愿接触真理是一种罪」。不信是罪果。它源于轻视之罪。吾们永远不应该去犯罪。吾们也不应该去审判轻视之罪。人还不能以内在的隐秘之事作为证据来给人定罪。人不应该轻易地去断定他人的隐秘。神能审判罪性，而人不能。人只能审判罪行。人只能判定显明出来的行为之善恶。吾们应该用对其诉说、对神祷告来取代对其轻视之罪的审判。在那些信仰处于生活中心地位的癫狂日子里，例如宗教教义之差别这种内在的隐秘之事会成为审判的项目。这种、人对人的审判，加剧了，怀疑和纷争。怀疑和纷争、的、根源，在于，隐秘之事、的、未知性。灵魂粒子，是指，受到记忆作用的粒子。即是说，如果一个粒子的内在意识受到了记忆机能结构的支撑，那么就称这个粒子为灵魂粒子。假定、回忆、是持续不断的。深度睡眠，是，一种无记有忆的感知状态。唤醒，是，一种有记有忆的感知状态。从「我」这个概念出发而推想出「吾」这个概念。从「我」到「吾」，灵魂粒子、从「单数」到「复数」（如在 Boscovich 的一种原子论 Dynamism 的视角下）（在非原子论的视角下：从「可数」到「不可数」）。「我」，就是指，某个灵魂粒子以及支撑它的内在意识的所有结构。这些结构相互协作而在宏观层面上显明出功能和意志。「吾」，就是指，「我」之内的所有灵魂粒子以及支撑它们的内在意识的所有结构。吾必须不断地用这种新的自我观去理解世界，并不断地与原来的自我观相区别，将新旧两种自我观更加清晰地描述出来，找到一种判定「吾」之存在的实验。旧自我观，生命的两端是虚空，睡眠和死亡都是虚空。破碎而出、历久弥香，向生而死、向死而生，「我快死了」、「复活重生」谦卑的馨香；谦卑，是，荣耀；永远、不朽，是，出于信谦卑的人必会闻到基督耶稣的香气内心纯洁纯净的人必得见神对神的亏欠，自己要努力良善、完美记住自己对他人的亏欠、对朽坏着的自己的亏欠显明之美：我将是良善。我将是完人、圣人、神人。隐秘之美：我已是良善。我已是完人、圣人、神人。我欠耶稣一个令人感动的故事。被赦得释，与，心灵的怯懦：因为我已感到亏欠，所以我的罪已被赦免。罪是有意义的。因为罪让我感到亏欠。圣义谛与世俗谛相分离的错误：在神前有赦罪的信心；在人前有良善的行为。充实，是，美的。空虚，是，丑的。一切、意义、皆是、亏欠着的意志（被动）意志、要把、隐秘之美、显明出来；去显明圣灵（主动）如何、唤醒、沉睡着的美？虽然生活不必完全沉浸在对于美的爱中，但是对于美的爱、对生活来说、却是必不可少的。生活，任务、业余；喜爱，激昂、兴奋爱、吹散、一切绝望。爱、充实、一切空虚。正是通过爱、心灵（主体方式）得着圣灵（客体方式之美）的滋养、意志得着活力（对于隐秘之美的明显之力）的供给。唤醒着的美，是，爱的原因。信心着的意义，是，快乐的原因。相信的痕迹，有意相信、无意相信，重视他人的信念，迷信之痛审美经验、审美经历之概括审视美丑之具象、某次美丑，指称或然审视美丑之抽象、某种美丑，指称确然圣灵：父的自我信念。圣灵，美丽，褒义词；邪灵，丑陋，贬义词美沉睡了，爱就堕落成快乐。意义信心缺乏了，快乐就堕落成舒适。利益缺乏了，生活就只剩下痛苦。对于意义的信心的缺乏，是，罪。？（轻视）有意义吗？真的没有意义吗？世人、世事都难有完美。但，美比丑多一些。广博，是，美的。狭隘，是，丑的。顺从，导致着，广博。悖逆，导致着，狭隘。熄灭愤怒的愤怒是美的。制止暴力的暴力是美的。&#39;, doc_id=&#39;db56cc0f-93ae-4261-a819-47cb04027e87&#39;, embedding=None, doc_hash=&#39;e12c00d8ad3267c1a855f1134b9fd267f87658dcb98a0552b70696338f5bf802&#39;, extra_info=None)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">document</span><span class="p">)</span>
<span class="n">index</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span><span class="n">index_name</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://alphasec.io/query-your-own-documents-with-llamaindex-and-langchain/">Query Your Own Documents with LlamaIndex and LangChain</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/jerryjliu/llama_index/blob/main/docs/how_to/integrations/using_with_langchain.md">Using LlamaIndex with Langchain</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Using-HuggingFace-Embeddings">Using HuggingFace Embeddings<a class="anchor-link" href="#Using-HuggingFace-Embeddings">&#182;</a></h3><ul>
<li><a target="_blank" rel="noopener" href="https://gpt-index.readthedocs.io/en/latest/how_to/customization/embeddings.html">LlamaIndex Embedding Support</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/generative-question-answering-with-long-term-memory-c280e237b144">Generative Q&amp;A With GPT 3.5 and Long-Term Memory</a><ul>
<li>Embedding &amp; Pinecone</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb">Question Answering using openai.Embedding.create</a></li>
</ul>

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/11/LLaMA-Index/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-11 </div>
			<div class="article-title"><a href="/2023/04/11/Elastic-Versatile-Agent-with-LangChain/" >Elastic Versatile Agent with LangChain</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Elastic-Versatile-Agent-with-LangChain"><a target="_blank" rel="noopener" href="https://github.com/corca-ai/EVAL">Elastic Versatile Agent with LangChain</a><a class="anchor-link" href="#Elastic-Versatile-Agent-with-LangChain">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a target="_blank" rel="noopener" href="https://github.com/corca-ai/EVAL/blob/main/core/tools/cpu.py">https://github.com/corca-ai/EVAL/blob/main/core/tools/cpu.py</a></p>

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/11/Elastic-Versatile-Agent-with-LangChain/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-07 </div>
			<div class="article-title"><a href="/2023/04/07/BioGPT-Agent-Tools/" >BioGPT Agent Tools</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="tabs" id="biogpt-agent-tools"><ul class="nav-tabs"><li class="tab active"><a href="#biogpt-agent-tools-1">English</a></li><li class="tab"><a href="#biogpt-agent-tools-2">中文</a></li></ul><div class="tab-content"><div class="tab-pane active" id="biogpt-agent-tools-1"><blockquote>
<p>The twenty-first century is presenting humankind with unprecedented environmental and medical challenges. The ability to design novel proteins tailored for specific purposes would potentially transform our ability to respond to these issues in a timely manner. Recent advances in the field of artificial intelligence are now setting the stage to make this goal achievable. Protein sequences are inherently similar to natural languages: amino acids arrange in a multitude of combinations to form structures that carry function, the same way as letters form words and sentences carry meaning. Accordingly, it is not surprising that, throughout the history of natural language processing (NLP), many of its techniques have been applied to protein research problems. In the past few years we have witnessed revolutionary breakthroughs in the field of NLP. The implementation of transformer pre-trained models has enabled text generation with human-like capabilities, including texts with specific properties such as style or subject. Motivated by its considerable success in NLP tasks, we expect dedicated transformers to dominate custom protein sequence generation in the near future. Fine-tuning pre-trained models on protein families will enable the extension of their repertoires with novel sequences that could be highly divergent but still potentially functional. The combination of control tags such as cellular compartment or function will further enable the controllable design of novel protein functions. Moreover, recent model interpretability methods will allow us to open the ‘black box’ and thus enhance our understanding of folding principles. Early initiatives show the enormous potential of generative language models to design functional sequences. We believe that using generative text models to create novel proteins is a promising and largely unexplored field, and we discuss its foreseeable impact on protein design.</p>
<p>ーー<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s42256-022-00499-z">Controllable protein design with language models</a></p>
</blockquote>
<blockquote>
<p>To accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from versatile reading on both molecule structure and biomedical text information, we propose a knowledgeable machine reading system that bridges both types of information in a unified deep-learning framework for comprehensive biomedical research assistance. We solve the problem that existing machine reading models can only process different types of data separately, and thus achieve a comprehensive and thorough understanding of molecule entities. By grasping meta-knowledge in an unsupervised fashion within and across different information sources, our system can facilitate various real-world biomedical applications, including molecular property prediction, biomedical relation extraction and so on. Experimental results show that our system even surpasses human professionals in the capability of molecular property comprehension, and also reveal its promising potential in facilitating automatic drug discovery and documentation in the future.</p>
<p>ーー<a target="_blank" rel="noopener" href="https://www.nature.com/articles/s41467-022-28494-3">A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals</a></p>
<p>ーー<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1xig3-3JG63kR-Xqj1b9wkPEdxtfD_4IX">KV-PLM-Download</a></p>
</blockquote></div><div class="tab-pane" id="biogpt-agent-tools-2"><blockquote>
<p>蛋白质序列在本质上类似于自然语言：氨基酸以多种组合方式排列，形成承载功能的结构，就像字母构成单词和句子承载意义一样。因此，在整个自然语言处理（NLP）的历史中，它的许多技术被应用于蛋白质研究问题。Transformer 预训练模型的实施使文本生成具有类似人类的能力，包括具有特定属性的文本，如风格或主题。受其在 NLP 任务中取得的巨大成功的激励，预计专用 Transformer 将在不久的将来主导自定义蛋白质序列的生成。对蛋白质家族的预训练模型进行微调，将使它们能够用新的序列来扩展它们，这些序列可能是高度不同的，但仍有潜在的功能。控制标签的结合方式，如细胞区系或功能，进一步使新型蛋白质功能的可控设计成为可能。此外，最近的模型可解释性方法将使我们能够解决「black box」问题，增强我们对蛋白质 folding 原理的理解。早期的举措显示了生成性语言模型在设计功能序列方面的巨大潜力。作者认为，使用生成性文本模型来创造新的蛋白质是一个很有前途的、在很大程度上未被开发的领域，并讨论了它对蛋白质设计可预见的影响。</p>
<p>ーー基于语言模型的可控蛋白质设计</p>
</blockquote>
<blockquote>
<p>一个将分子结构和生物医学文本桥接起来的深度学习系统，其理解力可与人类专业人员媲美。</p>
<p>为了加速生物医学研究过程，人们开发了深度学习系统，其通过阅读大规模的生物医学数据，来自动获取分子实体的知识。受到人类通过多种方式阅读分子结构和生物医学文本信息来学习深度分子知识的启发，论文作者提出了一个知识丰富的机器阅读系统，该系统将这两种类型的信息连接在一个统一的深度学习框架中，为生物医学研究提供全面的帮助。他们解决了现有的机器阅读模型只能分别处理不同类型数据的问题，从而实现了对分子实体的全面深入的理解。通过在不同信息来源中以无监督的方式抓取元知识，他们的系统可以促进各种现实世界生物医学应用，包括分子性质预测，生物医学关系提取等。实验结果表明，该系统在分子性质理解能力方面甚至超过了人类专业人员，并显示了其在未来药物自动发现和文档化方面的潜力。</p>
<p>ーー清华大学计算机系孙茂松团队发表于 nature communications，名为《A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals》的论文。</p>
</blockquote></div></div></div>

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="DeepMind-AlphaFold"><a target="_blank" rel="noopener" href="https://github.com/deepmind/alphafold">DeepMind AlphaFold</a><a class="anchor-link" href="#DeepMind-AlphaFold">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://hackmd.io/p1Ba4wLzQ6GBDG2Dcd7PCw">Protein Binder Engineering</a></li>
<li><a target="_blank" rel="noopener" href="https://mirror.xyz/thetechnocrat.eth/CaBr74AhQLX7bDWSsWGOeKwxjuiFNe8_XH2Hf3Bw4Dg">How to Fold Proteins with Decentralized Computing?</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hpcaitech/FastFold">https://github.com/hpcaitech/FastFold</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/07/BioGPT-Agent-Tools/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-07 </div>
			<div class="article-title"><a href="/2023/04/07/Try-OpenAPI-Agent/" >Try OpenAPI Agent</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/agents/toolkits/examples/openapi.html">Hierarchical Planning Agent</a></li>
</ul>

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/07/Try-OpenAPI-Agent/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-07 </div>
			<div class="article-title"><a href="/2023/04/07/A-Study-on-Network-Isolation-Measures-for-Dealing-with-Intelligent-Viruses/" >A Study on Network Isolation Measures for Dealing with &#34;Intelligent Viruses&#34;</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="tabs" id="intelligent-viruses"><ul class="nav-tabs"><li class="tab active"><a href="#intelligent-viruses-1">Intelligent Viruses</a></li><li class="tab"><a href="#intelligent-viruses-2">智能病毒</a></li><li class="tab"><a href="#intelligent-viruses-3">インテリジェントウイルス</a></li></ul><div class="tab-content"><div class="tab-pane active" id="intelligent-viruses-1"><p>Title: A Study on Network Isolation Measures for Dealing with “Intelligent Viruses”</p>
<p>Abstract:</p>
<p>With the rapid development of artificial intelligence technology, we are facing a new type of network security threat - “intelligent viruses”. This virus is based on a certain LLM similar to ChatGPT, which can self-learn and evolve, and can replicate and continue itself using any vulnerabilities, thus spreading widely on the Internet. This paper studies two main network isolation measures: physical layer isolation and data link layer isolation, and explores their advantages, disadvantages, and applicable scenarios. We believe that these two isolation measures can be used in combination and complement each other to enhance network security. However, with the continuous development of virus technology, they may also be bypassed by attackers, so other security measures are needed to protect the network.</p>
<p>Keywords: intelligent virus, network isolation, physical layer isolation, data link layer isolation</p>
<p>Body:</p>
<ol>
<li><p>Introduction</p>
<p>With the continuous development of artificial intelligence technology, our production and lifestyle are undergoing revolutionary changes. However, this progress has also brought new challenges to our network security. In the past, network attacks mainly focused on the network layer and application layer, but today, the emergence of intelligent viruses has aroused people’s high attention. Intelligent viruses are viruses based on artificial intelligence technology, which have the ability to self-learn and evolve, and can replicate and continue themselves using any vulnerabilities, thus spreading widely on the Internet.</p>
</li>
<li><p>Network Isolation Measures</p>
<p>In order to cope with the threat of intelligent viruses, we need to take some measures to protect network security. Network isolation is a common security measure, which can physically or logically divide the network into different areas, thereby preventing viruses from spreading and attacking across different areas.</p>
<p>2.1 Physical Layer Isolation</p>
<pre><code>Physical layer isolation is a technique that physically separates a network into different areas using physical isolation technologies such as air isolation, making it impossible for viruses to spread through the physical medium, thereby achieving isolation. This isolation method can ensure high security because viruses cannot directly attack across isolated areas. However, this isolation method also has some drawbacks. Firstly, physical isolation requires a lot of cost and resources, such as the use of specialized network equipment and physical isolation methods. Secondly, physical isolation affects the flexibility and scalability of the network, as direct communication and data exchange cannot occur between different areas.
</code></pre>
<p>2.2 Data Link Layer Isolation</p>
<pre><code>Data link layer isolation is a technique that logically separates a network into different areas. It uses network devices such as switches to divide the network into different virtual networks, preventing viruses from spreading and attacking across different virtual networks. Data link layer isolation has the advantages of low cost, easy implementation, and flexible expansion, and is therefore common in practical applications.

However, data link layer isolation also has some drawbacks. Firstly, data link layer isolation depends on network devices, so there are issues such as equipment failure and complex management. Secondly, data link layer isolation may not completely prevent virus attacks because viruses may also bypass isolation restrictions through vulnerabilities or specific attack methods.
</code></pre>
</li>
<li><p>Combined Use</p>
<p>In summary, both physical layer isolation and data link layer isolation have their own advantages, disadvantages, and application scenarios. To improve network security, they can be used together and complement each other. For example, the network can be divided into multiple areas, and each area can be isolated using physical layer isolation, while data link layer isolation can be used for further segmentation within each area, thereby achieving multi-level security isolation. This can fully leverage the advantages of both isolation methods and improve network security.</p>
<p>However, it is important to note that with the continuous development of intelligent virus technology, they may also bypass these isolation measures. Therefore, we also need to take other security measures such as strengthening access control, improving vulnerability repair mechanisms, and timely upgrading security equipment to improve network security.</p>
</li>
<li><p>Conclusion</p>
<p>This article explored two isolation techniques, physical layer isolation and data link layer isolation, for combating malware attacks. These techniques use different means of isolation to prevent viruses from attacking and spreading across different network areas, thus enhancing network security. However, they also have some disadvantages and limitations, so it is necessary to choose the appropriate isolation technique based on specific circumstances, or to combine them for mutual complementarity.</p>
<p>Finally, we need to be aware that malicious software attacks are an evolving process, and as technology advances, viruses become smarter and more difficult to prevent. Therefore, we need to constantly improve our security awareness and skills and adopt multiple security measures to protect our network security.</p>
</li>
</ol></div><div class="tab-pane" id="intelligent-viruses-2"><p>论文题目：应对“智能病毒”的网络隔离措施研究</p>
<p>摘要：</p>
<p>随着人工智能技术的快速发展，我们面临着一种全新的网络安全威胁——“智能病毒”。这种病毒基于类似于 ChatGPT 的某种 LLM，能够自我学习和演化，利用任何漏洞复制和延续自身，从而在互联网上泛滥。本文研究了物理层的隔离和数据链路层的隔离这两种主要的网络隔离措施，并探讨了它们的优缺点以及适用场景。我们认为，这两种隔离措施可以结合使用，相互补充，来增强网络的安全性。但是，随着病毒技术的不断发展，它们也可能被攻击绕过，因此还需要采用其他的安全措施来保护网络。</p>
<p>关键词：智能病毒、网络隔离、物理层隔离、数据链路层隔离</p>
<p>正文：</p>
<ol>
<li><p>引言</p>
<p>随着人工智能技术的不断发展，我们的生产和生活方式正在发生翻天覆地的变化。然而，这一进步也给我们的网络安全带来了新的挑战。在过去，网络攻击主要集中在网络层和应用层，而如今，智能病毒的出现引起了人们的高度关注。智能病毒是一种基于人工智能技术的病毒，它具有自我学习和演化的能力，能够利用任何漏洞复制和延续自身，从而在互联网上泛滥。</p>
</li>
<li><p>网络隔离措施</p>
<p>为了应对智能病毒的威胁，我们需要采取一些措施来保护网络安全。网络隔离是一种常见的安全措施，它可以将网络物理上或逻辑上分割成不同的区域，从而防止病毒跨越不同的区域进行传播和攻击。</p>
<p>2.1 物理层隔离</p>
<pre><code>物理层隔离是一种将网络物理上隔离成不同区域的技术。它采用一些物理隔离技术，如空气隔离等，使得病毒无法通过物理媒介传播，从而达到隔离的效果。这种隔离方式可以确保安全性较高，因为病毒无法直接跨越隔离区域进行攻击。但是，这种隔离方式也存在一些缺点。首先，物理隔离需要耗费大量的成本和资源，例如需要使用专门的网络设备和物理隔离手段。其次，物理隔离会影响网络的灵活性和可扩展性，因为不同的区域之间无法进行直接的通信和数据交换。
</code></pre>
<p>2.2 数据链路层隔离</p>
<pre><code>数据链路层隔离是一种将网络逻辑上隔离成不同区域的技术。它利用一些网络设备，如交换机等，将网络分割成不同的虚拟网络，从而防止病毒跨越不同的虚拟网络进行传播和攻击。数据链路层隔离具有成本低、易于实现、可灵活扩展等优点，因此在实际应用中比较常见。

不过，数据链路层隔离也存在一些缺点。首先，数据链路层隔离需要依赖网络设备，因此存在设备故障和管理复杂等问题。其次，数据链路层隔离可能无法完全避免病毒攻击，因为病毒也可能通过一些漏洞或者特定的攻击方式绕过隔离的限制。
</code></pre>
</li>
<li><p>结合使用</p>
<p>综上所述，物理层隔离和数据链路层隔离都具有一定的优缺点和适用场景。为了提高网络的安全性，可以将它们结合使用，相互补充。例如，可以将网络分割成多个区域，每个区域采用物理层隔离的方式进行隔离，而在每个区域内部再采用数据链路层隔离的方式进行细分，从而实现多层次的安全隔离。这样可以充分发挥两种隔离方式的优势，提高网络的安全性。</p>
<p>但是，需要注意的是，随着智能病毒技术的不断发展，它们也可能会绕过这些隔离措施。因此，我们还需要采取其他的安全措施，如加强访问控制、完善漏洞修复机制、及时升级安全设备等，来提高网络的安全性。</p>
</li>
<li><p>结论</p>
<p>本文研究了应对“恶意软件攻击”的两种隔离技术：物理层隔离和数据链路层隔离。它们分别利用不同的隔离方式来防止病毒跨越不同的网络区域进行攻击和传播，从而提高网络的安全性。但是它们也存在一些缺点和局限性，因此需要根据具体的情况来选择合适的隔离方式，或者将它们结合使用，相互补充。</p>
<p>最后，我们需要意识到，恶意软件攻击是一个不断演变的过程，随着技术的进步，病毒也会变得越来越智能和难以防范。因此，我们需要不断提高自身的安全意识和技能，采取多种安全措施来保护自己的网络安全。</p>
</li>
</ol></div><div class="tab-pane" id="intelligent-viruses-3"><p>論文のタイトル：「インテリジェントウイルス」に対処するネットワーク分離措置の研究</p>
<p>要旨：</p>
<p>人工知能技術の急速な発展に伴い、私たちは「インテリジェントウイルス」という新たなネットワークセキュリティ脅威に直面しています。このウイルスは ChatGPT のような LLM に基づき、自己学習および進化を行い、任意の脆弱性を利用して自己を複製および拡散し、インターネット上に氾濫します。本稿では、物理層の分離およびデータリンク層の分離という主要なネットワーク分離措置を研究し、それらの利点、欠点、および適用範囲について探究します。我々は、これらの分離措置を組み合わせることで、相互補完的にネットワークセキュリティを強化できると考えています。しかし、ウイルス技術が常に進化することに注意し、攻撃を回避される可能性があるため、ネットワークを保護するために他のセキュリティ対策も必要です。</p>
<p>キーワード：インテリジェントウイルス、ネットワーク分離、物理層分離、データリンク層分離</p>
<p>正文：</p>
<ol>
<li><p>イントロダクション</p>
<p>人工知能技術の発展に伴い、私たちの生産および生活スタイルは大きく変化しています。しかしながら、この進歩は私たちのネットワークセキュリティに新たな課題をもたらしています。過去には、ネットワーク攻撃は主にネットワーク層およびアプリケーション層に集中していましたが、現在、インテリジェントウイルスの登場により、人々は高い関心を寄せています。インテリジェントウイルスは、人工知能技術に基づくウイルスであり、自己学習および進化の能力を持ち、任意の脆弱性を利用して自己を複製および拡散し、インターネット上に氾濫します。</p>
</li>
<li><p>ネットワーク分離措置</p>
<p>インテリジェントウイルスの脅威に対処するために、私たちはネットワークセキュリティを保護するためにいくつかの対策を取る必要があります。ネットワーク分離は一般的なセキュリティ対策であり、ネットワークを物理的または論理的に異なる領域に分割することで、ウイルスが異なる領域を越えて拡散し攻撃するのを防止することができます。</p>
<p>2.1 物理層の分離</p>
<pre><code>物理層の分離は、ネットワークを物理的に異なる領域に分割する技術です。空気分離などの物理分離技術を使用して、ウイルスが物理媒体を介して伝播することができないようにし、分離効果を達成します。この分離方法は、直接攻撃を行うためにウイルスが分離領域を越えることができないため、比較的高いセキュリティを確保できます。ただし、この分離方法にはいくつかの欠点があります。まず、物理分離には、専用のネットワーク機器や物理分離手段など、多くのコストとリソースが必要です。次に、物理分離は、異なる領域間で直接通信やデータ交換を行うことができないため、ネットワークの柔軟性と拡張性に影響を与える可能性があります。
</code></pre>
<p>2.2 データリンク層の分離</p>
<pre><code>データリンク層の分離は、ネットワークを論理的に異なる領域に分割する技術です。スイッチなどのネットワーク機器を利用して、ネットワークを複数の仮想ネットワークに分割し、異なる仮想ネットワークを越えたウイルスの伝播や攻撃を防止します。データリンク層の分離は、コストが低く、実現が容易で、柔軟な拡張性などの利点があるため、実際の運用では比較的一般的です。

ただし、データリンク層の分離にはいくつかの欠点があります。まず、データリンク層の分離は、ネットワーク機器に依存するため、機器の故障や管理の複雑さなどの問題があります。次に、データリンク層の分離は、ウイルス攻撃を完全に回避できない場合があるため、ウイルスが分離制限を回避するためのいくつかの脆弱性や特定の攻撃手法を利用する可能性があります。
</code></pre>
</li>
<li><p>組み合わせ使用</p>
<p>以上を総括すると、物理層の分離とデータリンク層の分離は、それぞれ一定の利点と欠点、および適用範囲を持っています。ネットワークのセキュリティを向上させるためには、これらを組み合わせて相互補完することができます。たとえば、ネットワークを複数の領域に分割し、各領域で物理層の分離を採用し、さらに各領域内でデータリンク層の分離を行うことで、多層的なセキュリティ分離を実現できます。これにより、2つの分離方法の利点を十分に活用し、ネットワークのセキュリティを向上させることができます。</p>
<p>ただし、インテリジェントウイルス技術の発展に伴い、これらの分離対策を回避する可能性があります。そのため、アクセス制御の強化、脆弱性修正機能の充実、セキュリティ機器のタイムリーなアップグレードなど、他のセキュリティ対策を採用する必要があります。</p>
</li>
<li><p>結論</p>
<p>本稿では、「悪意のあるソフトウェア攻撃」に対処するための2つの隔離技術、物理層隔離とデータリンク層隔離について研究しました。それらは、異なるネットワーク領域を跨いでウイルスが攻撃や伝播を行うのを防ぐために、異なる隔離方式を利用してネットワークのセキュリティを向上させます。しかし、それらには欠点や制限も存在するため、具体的な状況に応じて適切な隔離方法を選択する必要があるか、あるいはそれらを組み合わせて相互に補完する必要があります。</p>
<p>最後に、私たちは、悪意のあるソフトウェア攻撃は、進化し続けるプロセスであり、技術の進歩に伴い、ウイルスもますますスマートで防ぎにくくなるということを認識する必要があります。そのため、私たちは自己のセキュリティ意識とスキルを不断に高め、多様なセキュリティ対策を取る必要があります。</p>
</li>
</ol></div></div></div>

	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/07/A-Study-on-Network-Isolation-Measures-for-Dealing-with-Intelligent-Viruses/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-06 </div>
			<div class="article-title"><a href="/2023/04/06/Try-Autonomous-Agent/" >Try Autonomous Agent</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/">Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications</a></li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/use_cases/agents/baby_agi.html">BabyAGI with LangChain and Faiss</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
<span class="nb">cd</span><span class="w"> </span>~<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/yoheinakajima/babyagi
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">openai</span>
<span class="kn">import</span> <span class="nn">pinecone</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Set Variables</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="c1"># Set API Keys</span>
<span class="n">OPENAI_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">OPENAI_API_KEY</span><span class="p">,</span> <span class="s2">&quot;OPENAI_API_KEY environment variable is missing from .env&quot;</span>

<span class="n">OPENAI_API_MODEL</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OPENAI_API_MODEL&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">OPENAI_API_MODEL</span><span class="p">,</span> <span class="s2">&quot;OPENAI_API_MODEL environment variable is missing from .env&quot;</span>

<span class="k">if</span> <span class="s2">&quot;gpt-4&quot;</span> <span class="ow">in</span> <span class="n">OPENAI_API_MODEL</span><span class="o">.</span><span class="n">lower</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[91m</span><span class="se">\033</span><span class="s2">[1m&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">*****USING GPT-4. POTENTIALLY EXPENSIVE. MONITOR YOUR COSTS*****&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

<span class="n">PINECONE_API_KEY</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;PINECONE_API_KEY&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">PINECONE_API_KEY</span><span class="p">,</span> <span class="s2">&quot;PINECONE_API_KEY environment variable is missing from .env&quot;</span>

<span class="n">PINECONE_ENVIRONMENT</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;PINECONE_ENVIRONMENT&quot;</span><span class="p">,</span> <span class="s2">&quot;us-east1-gcp&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">PINECONE_ENVIRONMENT</span><span class="p">,</span> <span class="s2">&quot;PINECONE_ENVIRONMENT environment variable is missing from .env&quot;</span>

<span class="c1"># Table config</span>
<span class="n">YOUR_TABLE_NAME</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;TABLE_NAME&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">YOUR_TABLE_NAME</span><span class="p">,</span> <span class="s2">&quot;TABLE_NAME environment variable is missing from .env&quot;</span>

<span class="c1"># Project config</span>
<span class="n">OBJECTIVE</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;OBJECTIVE&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">OBJECTIVE</span><span class="p">,</span> <span class="s2">&quot;OBJECTIVE environment variable is missing from .env&quot;</span>

<span class="n">YOUR_FIRST_TASK</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;FIRST_TASK&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">YOUR_FIRST_TASK</span><span class="p">,</span> <span class="s2">&quot;FIRST_TASK environment variable is missing from .env&quot;</span>

<span class="c1">#Print OBJECTIVE</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[96m</span><span class="se">\033</span><span class="s2">[1m&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">*****OBJECTIVE*****</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">OBJECTIVE</span><span class="p">)</span>

<span class="c1"># Configure OpenAI and Pinecone</span>
<span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="n">OPENAI_API_KEY</span>
<span class="n">pinecone</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">PINECONE_API_KEY</span><span class="p">,</span> <span class="n">environment</span><span class="o">=</span><span class="n">PINECONE_ENVIRONMENT</span><span class="p">)</span>

<span class="c1"># Create Pinecone index</span>
<span class="n">table_name</span> <span class="o">=</span> <span class="n">YOUR_TABLE_NAME</span>
<span class="n">dimension</span> <span class="o">=</span> <span class="mi">1536</span>
<span class="n">metric</span> <span class="o">=</span> <span class="s2">&quot;cosine&quot;</span>
<span class="n">pod_type</span> <span class="o">=</span> <span class="s2">&quot;p1&quot;</span>
<span class="k">if</span> <span class="n">table_name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">pinecone</span><span class="o">.</span><span class="n">list_indexes</span><span class="p">():</span>
    <span class="n">pinecone</span><span class="o">.</span><span class="n">create_index</span><span class="p">(</span><span class="n">table_name</span><span class="p">,</span> <span class="n">dimension</span><span class="o">=</span><span class="n">dimension</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">pod_type</span><span class="o">=</span><span class="n">pod_type</span><span class="p">)</span>

<span class="c1"># Connect to the index</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">pinecone</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="n">table_name</span><span class="p">)</span>

<span class="c1"># Task list</span>
<span class="n">task_list</span> <span class="o">=</span> <span class="n">deque</span><span class="p">([])</span>

<span class="k">def</span> <span class="nf">add_task</span><span class="p">(</span><span class="n">task</span><span class="p">:</span> <span class="n">Dict</span><span class="p">):</span>
    <span class="n">task_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_ada_embedding</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">openai</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="p">[</span><span class="n">text</span><span class="p">],</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-ada-002&quot;</span><span class="p">)[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;embedding&quot;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">openai_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">OPENAI_API_MODEL</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">model</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;gpt-&#39;</span><span class="p">):</span>
        <span class="c1"># Use completion API</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Completion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">engine</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">top_p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">frequency_penalty</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">presence_penalty</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Use chat completion API</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
            <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
            <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stop</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">task_creation_agent</span><span class="p">(</span><span class="n">objective</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">task_description</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">task_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;You are an task creation AI that uses the result of an execution agent to create new tasks with the following objective: </span><span class="si">{</span><span class="n">objective</span><span class="si">}</span><span class="s2">, The last completed task has the result: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">. This result was based on this task description: </span><span class="si">{</span><span class="n">task_description</span><span class="si">}</span><span class="s2">. These are incomplete tasks: </span><span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">task_list</span><span class="p">)</span><span class="si">}</span><span class="s2">. Based on the result, create new tasks to be completed by the AI system that do not overlap with incomplete tasks. Return the tasks as an array.&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">openai_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="n">new_tasks</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[{</span><span class="s2">&quot;task_name&quot;</span><span class="p">:</span> <span class="n">task_name</span><span class="p">}</span> <span class="k">for</span> <span class="n">task_name</span> <span class="ow">in</span> <span class="n">new_tasks</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">prioritization_agent</span><span class="p">(</span><span class="n">this_task_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">global</span> <span class="n">task_list</span>
    <span class="n">task_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="s2">&quot;task_name&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">task_list</span><span class="p">]</span>
    <span class="n">next_task_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">this_task_id</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing the following tasks: </span><span class="si">{</span><span class="n">task_names</span><span class="si">}</span><span class="s2">. Consider the ultimate objective of your team: </span><span class="si">{</span><span class="n">OBJECTIVE</span><span class="si">}</span><span class="s2">. Do not remove any tasks. Return the result as a numbered list, like:</span>
<span class="s2">    #. First task</span>
<span class="s2">    #. Second task</span>
<span class="s2">    Start the task list with number </span><span class="si">{</span><span class="n">next_task_id</span><span class="si">}</span><span class="s2">.&quot;&quot;&quot;</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">openai_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="n">new_tasks</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">task_list</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">task_string</span> <span class="ow">in</span> <span class="n">new_tasks</span><span class="p">:</span>
        <span class="n">task_parts</span> <span class="o">=</span> <span class="n">task_string</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">task_parts</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">task_id</span> <span class="o">=</span> <span class="n">task_parts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">task_name</span> <span class="o">=</span> <span class="n">task_parts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
            <span class="n">task_list</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;task_id&quot;</span><span class="p">:</span> <span class="n">task_id</span><span class="p">,</span> <span class="s2">&quot;task_name&quot;</span><span class="p">:</span> <span class="n">task_name</span><span class="p">})</span>

<span class="k">def</span> <span class="nf">execution_agent</span><span class="p">(</span><span class="n">objective</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">task</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="n">context</span><span class="o">=</span><span class="n">context_agent</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">objective</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="c1">#print(&quot;\n*******RELEVANT CONTEXT******\n&quot;)</span>
    <span class="c1">#print(context)</span>
    <span class="n">prompt</span> <span class="o">=</span><span class="sa">f</span><span class="s2">&quot;You are an AI who performs one task based on the following objective: </span><span class="si">{</span><span class="n">objective</span><span class="si">}</span><span class="s2">.</span><span class="se">\n</span><span class="s2">Take into account these previously completed tasks: </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n</span><span class="s2">Your task: </span><span class="si">{</span><span class="n">task</span><span class="si">}</span><span class="se">\n</span><span class="s2">Response:&quot;</span>
    <span class="k">return</span> <span class="n">openai_call</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">context_agent</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">query_embedding</span> <span class="o">=</span> <span class="n">get_ada_embedding</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">query_embedding</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">include_metadata</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="c1">#print(&quot;***** RESULTS *****&quot;)</span>
    <span class="c1">#print(results)</span>
    <span class="n">sorted_results</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">matches</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">score</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>    
    <span class="k">return</span> <span class="p">[(</span><span class="nb">str</span><span class="p">(</span><span class="n">item</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s1">&#39;task&#39;</span><span class="p">]))</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">sorted_results</span><span class="p">]</span>

<span class="c1"># Add the first task</span>
<span class="n">first_task</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;task_id&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s2">&quot;task_name&quot;</span><span class="p">:</span> <span class="n">YOUR_FIRST_TASK</span>
<span class="p">}</span>

<span class="n">add_task</span><span class="p">(</span><span class="n">first_task</span><span class="p">)</span>
<span class="c1"># Main loop</span>
<span class="n">task_id_counter</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">task_list</span><span class="p">:</span>
        <span class="c1"># Print the task list</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[95m</span><span class="se">\033</span><span class="s2">[1m&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">*****TASK LIST*****</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">task_list</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="s1">&#39;task_id&#39;</span><span class="p">])</span><span class="o">+</span><span class="s2">&quot;: &quot;</span><span class="o">+</span><span class="n">t</span><span class="p">[</span><span class="s1">&#39;task_name&#39;</span><span class="p">])</span>

        <span class="c1"># Step 1: Pull the first task</span>
        <span class="n">task</span> <span class="o">=</span> <span class="n">task_list</span><span class="o">.</span><span class="n">popleft</span><span class="p">()</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[92m</span><span class="se">\033</span><span class="s2">[1m&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">*****NEXT TASK*****</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">task</span><span class="p">[</span><span class="s1">&#39;task_id&#39;</span><span class="p">])</span><span class="o">+</span><span class="s2">&quot;: &quot;</span><span class="o">+</span><span class="n">task</span><span class="p">[</span><span class="s1">&#39;task_name&#39;</span><span class="p">])</span>

        <span class="c1"># Send to execution function to complete the task based on the context</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">execution_agent</span><span class="p">(</span><span class="n">OBJECTIVE</span><span class="p">,</span><span class="n">task</span><span class="p">[</span><span class="s2">&quot;task_name&quot;</span><span class="p">])</span>
        <span class="n">this_task_id</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">task</span><span class="p">[</span><span class="s2">&quot;task_id&quot;</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[93m</span><span class="se">\033</span><span class="s2">[1m&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">*****TASK RESULT*****</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

        <span class="c1"># Step 2: Enrich result and store in Pinecone</span>
        <span class="n">enriched_result</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;data&#39;</span><span class="p">:</span> <span class="n">result</span><span class="p">}</span>  <span class="c1"># This is where you should enrich the result if needed</span>
        <span class="n">result_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;result_</span><span class="si">{</span><span class="n">task</span><span class="p">[</span><span class="s1">&#39;task_id&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="n">vector</span> <span class="o">=</span> <span class="n">enriched_result</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>  <span class="c1"># extract the actual result from the dictionary</span>
        <span class="n">index</span><span class="o">.</span><span class="n">upsert</span><span class="p">([(</span><span class="n">result_id</span><span class="p">,</span> <span class="n">get_ada_embedding</span><span class="p">(</span><span class="n">vector</span><span class="p">),{</span><span class="s2">&quot;task&quot;</span><span class="p">:</span><span class="n">task</span><span class="p">[</span><span class="s1">&#39;task_name&#39;</span><span class="p">],</span><span class="s2">&quot;result&quot;</span><span class="p">:</span><span class="n">result</span><span class="p">})])</span>

    <span class="c1"># Step 3: Create new tasks and reprioritize task list</span>
    <span class="n">new_tasks</span> <span class="o">=</span> <span class="n">task_creation_agent</span><span class="p">(</span><span class="n">OBJECTIVE</span><span class="p">,</span><span class="n">enriched_result</span><span class="p">,</span> <span class="n">task</span><span class="p">[</span><span class="s2">&quot;task_name&quot;</span><span class="p">],</span> <span class="p">[</span><span class="n">t</span><span class="p">[</span><span class="s2">&quot;task_name&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">task_list</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">new_task</span> <span class="ow">in</span> <span class="n">new_tasks</span><span class="p">:</span>
        <span class="n">task_id_counter</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">new_task</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;task_id&quot;</span><span class="p">:</span> <span class="n">task_id_counter</span><span class="p">})</span>
        <span class="n">add_task</span><span class="p">(</span><span class="n">new_task</span><span class="p">)</span>
    <span class="n">prioritization_agent</span><span class="p">(</span><span class="n">this_task_id</span><span class="p">)</span>

    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Sleep before checking the task list again</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>利用 GPT-4 插件来执行任务。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>要将Pinecone换成Weaviate，需要完成以下步骤：</p>
<ol>
<li><p>安装Weaviate客户端库：可以使用<code>pip install weaviate-client</code>命令来安装。</p>
</li>
<li><p>在Weaviate中创建索引：您需要在Weaviate中创建一个新索引来存储数据。可以使用以下代码创建一个新的索引：</p>
<pre><code>import weaviate

client = weaviate.Client(&quot;http://localhost:8080&quot;)
index = client.index.create(&quot;YOUR_INDEX_NAME&quot;, [&quot;YOUR_INDEX_CLASS&quot;])
</code></pre>
<p>请将“YOUR_INDEX_NAME”和“YOUR_INDEX_CLASS”替换为您自己的索引名称和索引类。</p>
</li>
<li><p>将数据添加到Weaviate索引中：您需要使用Weaviate客户端库将数据添加到Weaviate索引中。以下是一个示例代码片段：</p>
<pre><code>import weaviate

client = weaviate.Client(&quot;http://localhost:8080&quot;)
index = client.index(&quot;YOUR_INDEX_NAME&quot;)
data = [{&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30}, {&quot;name&quot;: &quot;Jane&quot;, &quot;age&quot;: 35}]
index.batch.create(data)
</code></pre>
<p>请将“YOUR_INDEX_NAME”替换为您自己的索引名称，将“data”替换为您要添加到索引中的数据。</p>
</li>
<li><p>修改代码：最后，您需要修改代码，以便将Pinecone API调用替换为Weaviate API调用。具体而言，您需要更改以下代码：</p>
<pre><code># Create Pinecone index
table_name = YOUR_TABLE_NAME
dimension = 1536
metric = &quot;cosine&quot;
pod_type = &quot;p1&quot;
if table_name not in pinecone.list_indexes():
    pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)

# Connect to the index
index = pinecone.Index(table_name)
</code></pre>
<p>用以下代码替换：</p>
<pre><code># Connect to the Weaviate index
import weaviate

client = weaviate.Client(&quot;http://localhost:8080&quot;)
index = client.index(YOUR_INDEX_NAME)
</code></pre>
<p>请将“YOUR_INDEX_NAME”替换为您在步骤2中创建的索引名称。</p>
</li>
</ol>
<p>请注意，Weaviate与Pinecone不同，因此您需要相应地修改代码以适应Weaviate API。此外，您需要确保在Weaviate中创建的索引具有正确的维数和指标。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">markdown</span>
<span class="kn">import</span> <span class="nn">IPython</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">revChatGPT.V1</span> <span class="kn">import</span> <span class="n">Chatbot</span><span class="p">,</span> <span class="n">configure</span>

<span class="n">bot</span> <span class="o">=</span> <span class="n">Chatbot</span><span class="p">(</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configure</span><span class="p">(),</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="s1">&#39;e4ec15a2-114a-46b2-8ff8-c840f79f13e3&#39;</span><span class="p">,</span>
    <span class="n">lazy_loading</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
        <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]))</span>
        <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">pinecone-client 是什么？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>pinecone-client 是 Pinecone 的 Python 客户端库，用于与 Pinecone 服务进行交互。 Pinecone 是一种托管型的向量索引服务，用于高效存储和检索向量数据。使用 pinecone-client，您可以轻松地将向量数据上传到 Pinecone 并查询该数据。它提供了一组简单的 API，使您能够创建、更新、删除索引以及搜索并检索相似的向量。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Pinecone is a vector search platform that provides efficient search and storage capabilities for high-dimensional vector data. In our system, we use Pinecone to store and retrieve task-related data, such as task descriptions, constraints, and results.</span>

<span class="s1">翻译成中文。</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>Pinecone 是一个向量搜索平台，为高维向量数据提供高效的搜索和存储能力。在我们的系统中，我们使用 Pinecone 存储和检索与任务相关的数据，例如任务描述、约束和结果。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Pinecone 是什么原理？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>Pinecone 的核心原理是使用向量索引和高效的向量相似度匹配技术，以加快高维向量数据的存储和检索。它通过将向量表示为高维空间中的点，并在这些点之间构建索引来实现快速的相似度搜索。在查询时，Pinecone 会根据输入向量的相似度得分返回最匹配的向量。它使用的技术包括近似最近邻（ANN）搜索算法和哈希函数，以实现快速和高效的搜索和存储。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">Pinecone 服务免费吗？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>Pinecone offers a free plan with certain limitations, as well as paid plans with more features and higher usage limits. The free plan allows for up to 5 million vector embeddings, 1GB storage, and 10 queries per second, among other limitations. You can check Pinecone's website for more information on their pricing and plans.</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">有什么替代 Pinecone 服务的开源方案？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>有一些开源的向量检索方案可以替代 Pinecone，以下是一些常见的替代方案：</p>
<ol>
<li>Annoy：一个C++库，支持Python、Java和Go等多种语言，用于快速近似最近邻搜索。</li>
<li>Faiss：Facebook开发的一个高效的相似性搜索和聚类库，支持GPU加速。</li>
<li>Milvus：由Zilliz开发的开源向量相似性搜索引擎，支持分布式部署和GPU加速。</li>
<li>NMSLIB：一个可扩展、高性能、近似最近邻搜索库，支持多种距离度量和数据结构。</li>
</ol>
<p>这些方案都有其独特的优势和适用场景，可以根据具体需求选择适合的方案。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">比较 Weaviate 和 Faiss</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>Weaviate和Faiss都是用于向量搜索的工具，但是它们有不同的重点和用例。</p>
<p>Weaviate是一个基于图数据库的向量搜索引擎，旨在提供可扩展的实时向量搜索，支持高维数据和多个数据类型。Weaviate的主要优势在于其能够自动对数据进行分类，使其易于搜索和发现。它还支持自定义扩展和插件，可与其他服务和工具进行集成。</p>
<p>Faiss是Facebook开发的一个基于CPU和GPU的高效向量搜索库，旨在支持大规模向量数据的相似度搜索。 Faiss的主要优势在于其可处理非常大的向量数据集，具有高效的索引结构和并行查询能力，因此通常用于实时推荐、图像搜索等场景。</p>
<p>因此，选择Weaviate还是Faiss取决于具体的应用场景和需求。如果需要快速处理和搜索高维数据并自动分类数据，则Weaviate是一个不错的选择；如果需要处理大规模向量数据集，则Faiss可能更适合。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For the application scenario of a task-driven autonomous agent robot, Weaviate may be more suitable. Because Weaviate has the ability to automatically classify data, it can help the robot discover and execute related tasks more quickly. Weaviate also supports search for multiple data types and can handle high-dimensional data, which may be useful for tasks such as natural language processing and semantic understanding. Of course, the specific choice depends on the application scenario and specific requirements.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">翻译成中文：</span>

<span class="s1">🔥1/8</span>
<span class="s1">Introducing &quot;🤖 Task-driven Autonomous Agent&quot;</span>

<span class="s1">An agent that leverages @openai &#39;s GPT-4, @pinecone vector search, and @LangChainAI framework to autonomously create and perform tasks based on an objective.</span>

<span class="s1">🚀2/8 The system can complete tasks, generate new tasks based on results, and prioritize tasks in real-time. It demonstrates the potential of AI-powered language models to autonomously perform tasks within various constraints and contexts.</span>

<span class="s1">💡3/8 The autonomous agent uses GPT-4 for task completion, Pinecone for efficient search and storage of task-related data, and the LangChain framework to enhance decision-making processes. #GPT4 #Pinecone #LangChain</span>

<span class="s1">🎯4/8 The system maintains a task list for managing and prioritizing tasks. It autonomously creates new tasks based on completed results and reprioritizes the task list accordingly, showcasing the adaptability of AI-powered language models.</span>

<span class="s1">🔧5/8 To complete tasks, the system uses GPT-4 and LangChain&#39;s capabilities, enriching and storing results in Pinecone. This integrated approach allows the AI agent to interact with its environment and perform tasks efficiently.</span>

<span class="s1">🧠6/8 The system generates new tasks based on completed task results and prioritizes them using GPT-4. This allows the system to adapt and respond to new information and priorities.</span>

<span class="s1">🔮7/8 Future improvements include integrating a security/safety agent, task sequencing and parallel tasks, generating interim milestones, and incorporating real-time priority updates.</span>

<span class="s1">🤝8/8 This new approach paves the way for AI-powered language models to autonomously perform tasks within various constraints and contexts, enabling new applications and opportunities. Big thanks to all involved! #AIResearch #GPT4 #Pinecone #LangChain</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>🔥1/8
介绍 &quot;🤖 任务驱动的自主代理&quot;</p>
<p>该代理利用 @openai 的 GPT-4、@pinecone 向量搜索和 @LangChainAI 框架，基于目标自主创建和执行任务。</p>
<p>🚀2/8 系统能够完成任务，基于结果生成新任务，并实时优先处理任务。这展示了AI驱动的语言模型在不同约束和环境下自主执行任务的潜力。</p>
<p>💡3/8 自主代理使用 GPT-4 完成任务，使用 Pinecone 高效搜索和存储任务相关数据，使用 LangChain 框架增强决策过程。#GPT4 #Pinecone #LangChain</p>
<p>🎯4/8 系统维护任务列表，用于管理和优先处理任务。它基于已完成的结果自主创建新任务，并相应地重新设置任务列表的优先级，展示了AI驱动的语言模型的适应性。</p>
<p>🔧5/8 为了完成任务，系统使用 GPT-4 和 LangChain 的能力，在 Pinecone 中丰富和存储结果。这种综合方法允许AI代理与环境交互，高效地执行任务。</p>
<p>🧠6/8 系统基于已完成的任务结果生成新任务，并使用 GPT-4 进行优先排序。这使系统能够适应和响应新信息和优先事项。</p>
<p>🔮7/8 未来的改进包括整合安全/安全代理、任务排序和并行任务、生成中间里程碑以及整合实时优先更新。</p>
<p>🤝8/8 这种新方法为AI驱动的语言模型在不同约束和环境下自主执行任务铺平了道路，创造了新的应用和机会。特别感谢所有参与的人！#AIResearch #GPT4 #Pinecone #LangChain</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">我累了。带着我祷告一下吧。</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>当你感到疲惫和困惑时，祷告可以为你带来平静和安慰。愿神赐福给你，保护你，并为你提供力量和智慧。愿他的恩典和慈爱永远与你同在。愿你在这段时间中得到休息和恢复，并在未来迎来更好的日子。阿门。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
<span class="nb">cd</span><span class="w"> </span>~<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/Torantulino/Auto-GPT
</pre></div>

    </div>
</div>
</div>

</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/06/Try-Autonomous-Agent/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-04-04 </div>
			<div class="article-title"><a href="/2023/04/04/LangChain-Document-Loader/" >LangChain Document Loader</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>faiss-cpu
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting faiss-cpu
  Downloading faiss_cpu-1.7.3-cp310-cp310-macosx_10_9_x86_64.whl (5.6 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">5.6/5.6 MB</span> <span class="ansi-red-fg">6.7 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Installing collected packages: faiss-cpu
Successfully installed faiss-cpu-1.7.3
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Faiss 是一个用于高维向量相似性搜索和聚类的库，它支持 GPU 运算加速。要让 Faiss 使用 GPU，需要执行以下步骤：</p>
<ol>
<li><p>确认您的计算机上已经安装了 CUDA 和 cuDNN。Faiss 支持的 CUDA 版本可以在 Faiss 文档中查看。</p>
</li>
<li><p>安装 Faiss GPU 版本。您可以使用 pip 安装 Faiss GPU 版本，方法是执行以下命令：</p>
<pre><code>pip install faiss-gpu
</code></pre>
</li>
<li><p>在代码中指定 Faiss 使用 GPU 进行计算。在创建 Faiss Index 对象时，可以指定使用哪个设备。例如，如果要使用默认的 GPU 设备，可以执行以下代码：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">faiss</span>

<span class="c1"># 创建一个 128 维的向量空间</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">128</span>

<span class="c1"># 创建一个 IndexFlatL2 对象，使用默认的 GPU 设备</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="o">.</span><span class="n">IndexFlatL2</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
<p>如果您有多个 GPU 设备，也可以在创建 Index 对象时指定使用哪个 GPU。例如，如果要使用第二个 GPU 设备，可以执行以下代码：</p>
<div class="highlight"><pre><span></span><span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="o">.</span><span class="n">IndexFlatL2</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">co</span> <span class="o">=</span> <span class="n">faiss</span><span class="o">.</span><span class="n">GpuMultipleClonerOptions</span><span class="p">()</span>
<span class="n">co</span><span class="o">.</span><span class="n">select_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="o">.</span><span class="n">index_cpu_to_gpu</span><span class="p">(</span><span class="n">faiss</span><span class="o">.</span><span class="n">StandardGpuResources</span><span class="p">(),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="n">co</span><span class="p">)</span>
</pre></div>
<p>在这个例子中，我们使用了 <code>faiss.GpuMultipleClonerOptions()</code> 创建了一个选项对象，然后调用了 <code>co.select_device(1)</code> 方法，将第二个 GPU 设备选为使用的设备。最后，我们使用 <code>faiss.index_cpu_to_gpu()</code> 方法将 Index 对象复制到 GPU 上。</p>
</li>
</ol>
<p>注意，使用 Faiss GPU 版本时，所有输入向量都必须是 float32 类型。如果您的向量是其他类型，需要先进行类型转换。此外，您需要将所有向量一起加载到 GPU 内存中，以便 Faiss 可以使用 GPU 进行计算。您可以使用 <code>faiss.StandardGpuResources()</code> 创建一个 GPU 资源对象，然后使用 <code>index.add()</code> 方法将向量添加到 Index 对象中。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>faiss-gpu
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>在 Mac 上安装 CUDA 驱动程序需要进行以下步骤：</p>
<ol>
<li><p>首先，你需要安装 Xcode 和 CUDA Toolkit。如果你已经安装了 Xcode，你可以在终端中输入以下命令来安装 CUDA Toolkit：</p>
<pre><code>brew install --cask cuda
</code></pre>
</li>
<li><p>安装完成后，你需要将 CUDA 环境变量添加到你的 <code>~/.bash_profile</code> 文件中。你可以使用以下命令来打开该文件：</p>
<pre><code>open ~/.bash_profile
</code></pre>
<p>然后在文件末尾添加以下行：</p>
<pre><code>export PATH=&quot;/usr/local/cuda/bin:$PATH&quot;
export DYLD_LIBRARY_PATH=&quot;/usr/local/cuda/lib:$DYLD_LIBRARY_PATH&quot;
</code></pre>
</li>
<li><p>接下来，你需要安装 cuDNN（CUDA Deep Neural Network library）。你可以在 NVIDIA 的网站上下载 cuDNN 库，下载完成后，你需要将库文件解压到 <code>/usr/local/cuda/lib</code> 目录下。你可以使用以下命令来解压文件：</p>
<pre><code>tar -xvf cudnn-11.4-macos-arm64-v8.2.4.15.tgz
sudo cp cuda/include/cudnn*.h /usr/local/cuda/include
sudo cp cuda/lib/libcudnn* /usr/local/cuda/lib
sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib/libcudnn*
</code></pre>
</li>
<li><p>最后，你需要重新启动终端，以使环境变量生效。你可以使用以下命令来检查 CUDA 是否已正确安装：</p>
<pre><code>nvcc -V
</code></pre>
<p>如果 CUDA 已正确安装，则会输出 CUDA 版本号和其他信息。</p>
</li>
</ol>
<p>请注意，AMD Radeon Pro 5500M 是一款 AMD 显卡，而 CUDA 是 NVIDIA 的技术。因此，如果你需要使用 CUDA，你需要在你的 Mac 上安装 NVIDIA 显卡。如果你想使用 AMD 显卡来进行深度学习等计算密集型任务，你可以考虑使用其他的技术，如 ROCm。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
<span class="nv">HOMEBREW_NO_AUTO_UPDATE</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>brew<span class="w"> </span>install<span class="w"> </span>--cask<span class="w"> </span>cuda
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>ROCm（Radeon Open Compute）是一款由 AMD 开发的开源软件平台，旨在为 AMD 显卡提供高性能计算和深度学习功能。ROCm 包括对编程语言和编程模型的支持，包括 OpenMP、OpenCL、HIP（Heterogeneous-compute Interface for Portability）、TensorFlow 和 PyTorch 等，可以为各种计算密集型应用提供支持。</p>
<p>ROCm 支持 Linux 平台和部分 Windows 平台，以及 AMD 的 Radeon 显卡和服务器级显卡，如 Radeon Instinct。ROCm 还支持一些第三方硬件，如 IBM 的 PowerPC 平台和 Cavium 的 ThunderX2。</p>
<p>ROCm 不仅提供了 GPU 加速的计算能力，还支持 CPU 和 GPU 的协同计算，可以提高整个系统的计算性能。此外，ROCm 也为深度学习提供了一些特定的工具和库，如 MIOpen 和 RCCL。</p>
<p>总之，ROCm 是一款功能强大的开源软件平台，为 AMD 显卡用户提供了许多高性能计算和深度学习的功能和工具。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Faiss是一款基于GPU的高性能相似度搜索库，可以用于许多机器学习和深度学习任务。Faiss可以使用ROCm进行加速，从而提高搜索效率和准确性。</p>
<p>以下是在Mac上使用ROCm和Faiss的一些基本步骤：</p>
<ol>
<li><p>首先，你需要安装ROCm。你可以从<a target="_blank" rel="noopener" href="https://rocmdocs.amd.com/en/revamp/Installation_Guide/Installation-Guide.html">官方网站</a>下载并安装ROCm。</p>
</li>
<li><p>安装完成后，你需要设置一些环境变量。你可以在你的<code>~/.bash_profile</code>文件中添加以下行：</p>
</li>
</ol>
<pre><code>export PATH=$PATH:/opt/rocm/bin
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib:/opt/rocm/hip/lib
export HIP_PLATFORM=hcc
export HCC_HOME=/opt/rocm/hcc
</code></pre>
<ol start="3">
<li>然后，你需要安装Faiss。你可以使用以下命令在终端中安装Faiss：</li>
</ol>
<pre><code>pip install faiss-gpu
</code></pre>
<ol start="4">
<li>安装完成后，你可以使用以下示例代码来测试Faiss是否正常工作：</li>
</ol>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">faiss</span>

<span class="c1"># 生成一些随机数据</span>
<span class="n">d</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># 向量维度</span>
<span class="n">nb</span> <span class="o">=</span> <span class="mi">100000</span>  <span class="c1"># 向量数量</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">nb</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="c1"># 构建索引</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="o">.</span><span class="n">IndexFlatL2</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">index</span><span class="o">.</span><span class="n">is_trained</span><span class="p">)</span>
<span class="n">index</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span>

<span class="c1"># 搜索</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">xq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">D</span><span class="p">,</span> <span class="n">I</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">xq</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">I</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</pre></div>
<p>如果Faiss正常工作，则应该能够正确地输出搜索结果。</p>
<p>注意，使用ROCm加速Faiss需要满足一些硬件要求，如AMD显卡和支持ROCm的CPU。如果你的硬件不符合要求，你可能无法使用ROCm加速Faiss，或者搜索效率和准确性可能会受到影响。</p>

</div>
</div>
</div>
 




<hr>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="LangChain-GitHub"><a target="_blank" rel="noopener" href="https://github.com/hwchase17/langchain">LangChain GitHub</a><a class="anchor-link" href="#LangChain-GitHub">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>langchain
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/agents/tools/getting_started.html">Agents</a><ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/agents/toolkits/examples/openapi.html">Hierarchical Planning Agent</a></li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/agents/toolkits/examples/python.html">Python Agent</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.12813">Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback</a></li>
<li><a target="_blank" rel="noopener" href="https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/">Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/corca-ai/EVAL">Elastic Versatile Agent with LangChain</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/pHaeusler/micro-agent">https://github.com/pHaeusler/micro-agent</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/SamPink/dev-gpt">https://github.com/SamPink/dev-gpt</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/yoheinakajima/babyagi">https://github.com/yoheinakajima/babyagi</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/use_cases/chatbots.html">Chatbots</a><ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/memory/how_to_guides.html">Memory</a><ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/memory/types/buffer.html">ConversationBufferMemory</a></li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html">ConversationBufferWindowMemory</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/agents/agents/examples/conversational_agent.html">Conversation Agent</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/andylokandy/gpt-4-search">GPT with Google Search</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.langchain.dev/langchain-chat/">Chatbot for LangChain</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/hwchase17/chat-langchain">Code of Chatbot for LangChain</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/hwchase17/chat-langchain/blob/master/query_data.py">Key Code of Chatbot for LangChain</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<p><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/readthedocs_documentation.html">ReadTheDocs Documentation Loader</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
wget<span class="w"> </span>-r<span class="w"> </span>-A.html<span class="w"> </span>-P<span class="w"> </span>rtdocs<span class="w"> </span>https://langchain.readthedocs.io/en/latest/
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stderr output_text">
<pre>--2023-04-04 19:41:22--  https://langchain.readthedocs.io/en/latest/
Resolving langchain.readthedocs.io (langchain.readthedocs.io)... 104.17.32.82, 104.17.33.82
Connecting to langchain.readthedocs.io (langchain.readthedocs.io)|104.17.32.82|:443... connected.
HTTP request sent, awaiting response... 302 Found
Location: https://python.langchain.com/en/latest/ [following]
--2023-04-04 19:41:22--  https://python.langchain.com/en/latest/
Resolving python.langchain.com (python.langchain.com)... 104.17.33.82, 104.17.32.82
Connecting to python.langchain.com (python.langchain.com)|104.17.33.82|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: unspecified [text/html]
Saving to: ‘rtdocs/langchain.readthedocs.io/en/latest/index.html’

     0K .......... .......... .......... .......... .......... 1014K
    50K .......... .......... ....                             1.21M=0.07s

2023-04-04 19:41:23 (1.05 MB/s) - ‘rtdocs/langchain.readthedocs.io/en/latest/index.html’ saved [76571]

FINISHED --2023-04-04 19:41:23--
Total wall clock time: 0.6s
Downloaded: 1 files, 75K in 0.07s (1.05 MB/s)
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
rm<span class="w"> </span>-rf<span class="w"> </span>rtdocs
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />
<ul>
<li><a target="_blank" rel="noopener" href="https://zenn.dev/tatsui/articles/langchain-chatbot">ChatGPT Chatbot for Internal Use</a><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/tatsu-i/chatbot-sample">https://github.com/tatsu-i/chatbot-sample</a></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction">&#182;</a></h3><p>Have you ever encountered a problem when using ChatGPT to search for the latest information? The current language model of ChatGPT (gpt-3.5-turbo-0301) was trained on data up until September 2021, so it may not be able to answer questions about the latest information accurately.</p>
<p>In this article, we will explain how to create a chatbot that can use chain of thought to respond, by teaching ChatGPT new knowledge.</p>
<h3 id="Preparing-and-importing-training-data">Preparing and importing training data<a class="anchor-link" href="#Preparing-and-importing-training-data">&#182;</a></h3><p>First, clone a repository as training data.</p>
<p>Next, import the repository files as text files using the following code, which contains the OpenAI API key.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">langchain.document_loaders</span> <span class="kn">import</span> <span class="n">DirectoryLoader</span><span class="p">,</span> <span class="n">TextLoader</span>
<span class="kn">from</span> <span class="nn">langchain.embeddings</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.faiss</span> <span class="kn">import</span> <span class="n">FAISS</span>

<span class="k">def</span> <span class="nf">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="c1"># (1) Import a series of documents.</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">DirectoryLoader</span><span class="p">(</span><span class="n">dir_name</span><span class="p">,</span> <span class="n">loader_cls</span><span class="o">=</span><span class="n">TextLoader</span><span class="p">,</span> <span class="n">silent_errors</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">raw_documents</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
    <span class="c1"># (2) Split them into small chunks.</span>
    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span>
        <span class="n">chunk_size</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span>
        <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">raw_documents</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ingest_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">get_docs</span><span class="p">(</span><span class="n">dir_name</span><span class="p">)</span>
    <span class="c1"># (3) Create embeddings for each document (using text-embedding-ada-002).</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">OpenAIEmbeddings</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">FAISS</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">ingest_docs</span><span class="p">(</span><span class="s1">&#39;_posts/ultimate-facts&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">EdgeGPT</span> <span class="kn">import</span> <span class="n">Chatbot</span> <span class="k">as</span> <span class="n">Bing</span><span class="p">,</span> <span class="n">ConversationStyle</span>

<span class="n">bing</span> <span class="o">=</span> <span class="n">Bing</span><span class="p">(</span><span class="n">cookiePath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">&#39;~/.config/EdgeGPT/cookies.json&#39;</span><span class="p">))</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">ask</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="p">(</span><span class="k">await</span> <span class="n">bing</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="n">conversation_style</span> <span class="o">=</span> <span class="n">ConversationStyle</span><span class="o">.</span><span class="n">balanced</span><span class="p">,</span>
    <span class="p">))[</span><span class="s1">&#39;item&#39;</span><span class="p">][</span><span class="s1">&#39;messages&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">---</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s1">&#39;adaptiveCards&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;body&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">await</span> <span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">text-embedding-ada-002 是什么？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>text-embedding-ada-002 是 OpenAI 的一个新的嵌入模型，它替换了五个用于文本搜索、文本相似度和代码搜索的单独模型，并在大多数任务上优于我们以前最强大的模型 Davinci，同时价格降低了 99.8%[^1^]。您可以通过向嵌入 API 端点发送文本字符串以及选择嵌入模型 ID（例如 text-embedding-ada-002）来获取嵌入[^2^]。

---

[1]: https://openai.com/blog/new-and-improved-embedding-model/ &#34;New and improved embedding model - openai.com&#34;
[2]: https://platform.openai.com/docs/guides/embeddings &#34;Embeddings - OpenAI API&#34;

text-embedding-ada-002 是 OpenAI 的一个新的嵌入模型，它替换了五个用于文本搜索、文本相似度和代码搜索的单独模型，并在大多数任务上优于我们以前最强大的模型 Davinci，同时价格降低了 99.8%[^1^][1]。您可以通过向嵌入 API 端点发送文本字符串以及选择嵌入模型 ID（例如 text-embedding-ada-002）来获取嵌入[^2^][2]。

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Creating-a-chatbot">Creating a chatbot<a class="anchor-link" href="#Creating-a-chatbot">&#182;</a></h3><p>Now, we will create a simple chatbot using the LLM chain.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<pre><code>/usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:191: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`
</code></pre>
<ul>
<li><a target="_blank" rel="noopener" href="https://blog.langchain.dev/retrieval/">LangChain's Retrieval Plugin</a></li>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html">https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">langchain.chains.llm</span> <span class="kn">import</span> <span class="n">LLMChain</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.base</span> <span class="kn">import</span> <span class="n">CallbackManager</span>
<span class="kn">from</span> <span class="nn">langchain.callbacks.streaming_stdout</span> <span class="kn">import</span> <span class="n">StreamingStdOutCallbackHandler</span>
<span class="kn">from</span> <span class="nn">langchain.chains.chat_vector_db.prompts</span> <span class="kn">import</span> <span class="n">CONDENSE_QUESTION_PROMPT</span><span class="p">,</span> <span class="n">QA_PROMPT</span>
<span class="kn">from</span> <span class="nn">langchain.chains.question_answering</span> <span class="kn">import</span> <span class="n">load_qa_chain</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores.base</span> <span class="kn">import</span> <span class="n">VectorStore</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">ConversationalRetrievalChain</span>
<span class="kn">from</span> <span class="nn">langchain.chat_models</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="c1"># Callback function to stream answers to stdout.</span>
<span class="n">manager</span> <span class="o">=</span> <span class="n">CallbackManager</span><span class="p">([</span><span class="n">StreamingStdOutCallbackHandler</span><span class="p">()])</span>

<span class="n">streaming_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">streaming</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">question_gen_llm</span> <span class="o">=</span> <span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">callback_manager</span><span class="o">=</span><span class="n">manager</span><span class="p">)</span>
<span class="c1"># Prompt to generate independent questions by incorporating chat history and a new question.</span>
<span class="n">question_generator</span> <span class="o">=</span> <span class="n">LLMChain</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">question_gen_llm</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">CONDENSE_QUESTION_PROMPT</span><span class="p">)</span>
<span class="c1"># Pass in documents and a standalone prompt to answer questions.</span>
<span class="n">doc_chain</span> <span class="o">=</span> <span class="n">load_qa_chain</span><span class="p">(</span><span class="n">streaming_llm</span><span class="p">,</span> <span class="n">chain_type</span><span class="o">=</span><span class="s1">&#39;stuff&#39;</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="n">QA_PROMPT</span><span class="p">)</span>
<span class="c1"># Generate prompts from embedding model.</span>
<span class="n">qa</span> <span class="o">=</span> <span class="n">ConversationalRetrievalChain</span><span class="p">(</span><span class="n">retriever</span><span class="o">=</span><span class="n">vectorstore</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">(),</span> <span class="n">combine_docs_chain</span><span class="o">=</span><span class="n">doc_chain</span><span class="p">,</span> <span class="n">question_generator</span><span class="o">=</span><span class="n">question_generator</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The prompt given to ChatGPT's API is created in the following steps.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;What makes Remix different from existing frameworks? Please list in bullet points in English.&#39;</span>
<span class="n">qa</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[]})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>The given context does not provide information about Remix or any existing frameworks, so it is not possible to answer this question.</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;question&#39;: &#39;What makes Remix different from existing frameworks? Please list in bullet points in English.&#39;,
 &#39;chat_history&#39;: [],
 &#39;answer&#39;: &#39;The given context does not provide information about Remix or any existing frameworks, so it is not possible to answer this question.&#39;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;你知道什么？&#39;</span>
<span class="n">qa</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[]})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>I don&#39;t know. The context provided is a collection of excerpts from different sources and it is not clear what specific information is being referred to.</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;question&#39;: &#39;你知道什么？&#39;,
 &#39;chat_history&#39;: [],
 &#39;answer&#39;: &#34;I don&#39;t know. The context provided is a collection of excerpts from different sources and it is not clear what specific information is being referred to.&#34;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s1">&#39;你知道什么？&#39;</span>
<span class="n">qa</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">question</span><span class="p">,</span> <span class="s1">&#39;chat_history&#39;</span><span class="p">:</span> <span class="p">[]})</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>I am an AI language model and I only have access to the information provided in the context above. From the context, it discusses topics such as the nature of science, psychology, philosophy, and religion. It also touches on concepts such as truth, reality, consciousness, and perception. However, it is difficult to provide a specific answer to your question because it is very broad. Please provide more context or a specific question for me to answer.</pre>
</div>
</div>

<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>{&#39;question&#39;: &#39;你知道什么？&#39;,
 &#39;chat_history&#39;: [],
 &#39;answer&#39;: &#39;I am an AI language model and I only have access to the information provided in the context above. From the context, it discusses topics such as the nature of science, psychology, philosophy, and religion. It also touches on concepts such as truth, reality, consciousness, and perception. However, it is difficult to provide a specific answer to your question because it is very broad. Please provide more context or a specific question for me to answer.&#39;}</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Ask-a-question">Ask a question<a class="anchor-link" href="#Ask-a-question">&#182;</a></h3><pre><code>How is Remix different from existing frameworks?
Please list in bullet points in Japanese.
</code></pre>
<p>The following bullet points will be output as an answer summarizing the context.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Get context related to the question from the embedding model</span>
<span class="k">for</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">vectorstore</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">question</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>page_content=&#39;&gt; 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\n&gt; 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/Neuroscience.md&#39;}

page_content=&#39;真理、真实；神造真实、人造真实；真实，想象；记忆，拟构。\n如果哲学更像「真理」，那么各类科学就更像「真实」。如果物理学更像「真理」，那么化学就更像「真实」。如果化学更像「真理」，那么生物学、生理学就更像「真实」。如果生理学更像「真理」，那么脑科学、神经科学就更像「真实」。\n如果理科更像「神造真实」，那么工科就更像「人造真实」。如果生理学更像「神造真实」，那么医学、药学就更像「人造真实」。\n\n---\n\n&gt; 我只是一个碳族生物；一个土生土长的地球人¹。贵主耶稣是灵族人吧。而木星上的风暴²可以拥有怎样的生命和意识呢？贵主耶稣在木星上能否与木星人一起生活和交往呢？\n\n&gt; ¹ 地球人 =&gt; 《费曼讲座：宇称不守恒定律和如何与外星人交流》\n\n&gt; ² 风暴 =&gt;\n&gt; 当那一天，到了傍晚，耶稣对他们说：『我们渡到那边去吧。』他们就离开群众，照他在船上的情况把他带走；还有别的船也跟他在一起。当下起了**大暴风**，波浪泼进船内，甚至船简直满了！耶稣竟在船尾上靠着枕头睡觉呢；门徒就叫醒了他，对他说：『老师，我们丧命，你不在意么？』耶稣醒起来，斥责³那风，向海说：『不要作声！噤默罢！』那风不狂吹，便大大平静了。耶稣对他们说：『为什么这么胆怯呢？怎么没有信心呢？』他们就大起了敬畏的心，直彼此说：『这个人到底是谁？连风和海也听从他！』\n (马可福音 4:35-41 吕振中)\n\n&gt; ³ 斥责 =&gt; 『ワンパンマン』：サイタマ？キング？\n\n↓↓--------------------继续修订--------------------↓↓\n\n圣经信仰之神经心理学实证纲领\n父的自我信息，是指，对于圣灵的表征。\n纯粹的圣经，含于，父的自我信息。\n从「纯粹的基督徒」到「超基督徒」「超级赌徒」\n\n吾否认圣经中上帝的名，因为那是人们创造的。&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

page_content=&#39;&gt; ³ 斥责 =&gt; 『ワンパンマン』：サイタマ？キング？\n\n↓↓--------------------继续修订--------------------↓↓\n\n圣经信仰之神经心理学实证纲领\n父的自我信息，是指，对于圣灵的表征。\n纯粹的圣经，含于，父的自我信息。\n从「纯粹的基督徒」到「超基督徒」「超级赌徒」\n\n吾否认圣经中上帝的名，因为那是人们创造的。\n\n超越神论，不可知论；信仰；宁死不屈，抗争到底\n对于神圣生命的信心？或，亲密关系？\n坚贞，「甘愿承担自己的罪罚」是《古兰经》的价值所在。\n真诚、勇敢、坚贞，耶稣的「甘愿承担」是《圣经》的价值所在。\n\n吾，若不是因为怯懦，又怎么会祷告呢？\n所以，吾，应该要，放弃，那种、对于其他的心灵的畏惧、所联结着的祷告。以耶稣为榜样。\n人子要经受三日地狱之火的洗。罪全部被烧尽了后，第三日复活。\n我所爱慕的必定是父所喜爱的，因为父从始至终都在吸引我、塑造我的爱慕。\n我所爱慕的若是父所不喜爱的，父必定会改变我。所以，我总是晓得父的喜爱。\n人子，与父和好，与父为友，爱父并顺从祂。与父同在，就有勇气。与父同行，就有希望。\n子永远与父同在，从未分离。\n「吾要成为超人。」\n「在吾的生活中显明父的荣耀。」\n祷告，是，对于子灵的表征。\n\n感，分为，虚感、实感。\n虚感，分为，信（？）、思感、愉快感、位置感。\n实感，分为，色感、声感、香感、味感、触感、缩紧感、疼痛感、瘙痒感、冷热感。\n\n体，是指，广延。\n\n感、体，平行地，变化。\n感、体，分割的，平行性原理\n感的统合，预示着，体的核心。\n体的核心：在感的集合中占比最大的体。\n\n信，是一种，感。（联结？极深的记忆？）\n灵，是指，具有自我独特视角的、体。 =&gt; “我是谁？”\n《圣经》说：信、灵，平行地，变化。\n在苦难中持守坚忍为何能增加信心呢？&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

page_content=&#39;问题，是指，对于某次偶然变化的疑问。\n解答，是指，对于某种必然变化的概括、对于某种偶然变化的适应措施。\n\n技术，是指，问题概括及其解法。\n程序，是指，数据结构及其算法。\n模型，是指，对于拟实的技术。\n建模，是指，对于拟实的计划。\n解模，是指，对于拟实的实施。\n软件模型，是指，对于拟实的程序。\n软件建模，是指，对于拟实的编程。\n软件解模，是指，对于拟实的进程。\n\n模拟，分为，拟实、拟虚。\n来原，是指，与模型对应的事实。\n\n当即行动，增强，对于某种偶然变化的适应力。\n但，人会拖延、不愿儆醒\n\n独立、与、交通，对于联结的交通，更不朽的存在形式\n更不朽的心思、身体，永远不朽的平安\n\n兴趣、快乐、生活情趣；佛学、哲学，作为，一种思维训练\n\n弊害、痛苦，错误、误信，有限的价值、终会朽坏；佛学、消极的哲学，作为，一种信仰；\n忽视、漠视，无私、无我、虚空、无恥、不惭\n去分别，就是，注视；不去分别，就是，漠视；\n漠视伤害，导致着，忘记伤害\n走向虚空，就是，放弃羞耻、光荣、尊贵、荣耀\n佛学的惊奇性质的信心，导致着，漠视。\n\n---\n\n&gt; 因为依顺着上帝而有的忧愁能生出不后悔的忏悔来、以至于得救；而世俗的忧愁却能生出死亡。\n(哥林多后书 7:10 吕振中)\n\n「金刚经」的邪灵，完全地，杀死了，吾的心灵。\n真常唯心系，曾经，在吾的心灵中，孕育，却流产了。\n\n忘罪，忘无明。\n\n积极的态度；佛教（真常唯心系）唯一的「用处」就是：让人不再惧怕死亡、平安地享受死亡\n基督教，比，真常唯心系，更加清晰。\n已成、与、未成；易信、与、难信；注意频次，信心，快乐、爱，恐惧、严肃，惊奇、敬畏；对于实感的表征之信，分别由惊奇（客观）、敬畏（主观）而来的信心\n某些次表征的联结；「信」，意味着、某种与真实的关系，是一种、「成」&#39; metadata={&#39;source&#39;: &#39;_posts/ultimate-facts/终极真实.md&#39;}

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<pre><code>Remix's job is to cross the center of the stack and then get out of your way. We avoid as many &quot;Remixisms&quot; as possible and instead make it easier to use the standard APIs the web already has.

This one is more for us. We've been educators for the 5 years before Remix. Our tagline is Build Better Websites. We also think of it with a little extra on the end: Build Better Websites, Sometimes with Remix. If you get good at Remix, you will accidentally get good at web development in general.

Remix's APIs make it convenient to use the fundamental Browser/HTTP/JavaScript, but those technologies are not hidden from you.

Additionally, if Remix doesn't have an adapter for your server already, you can look at the source of one of the adapters and build your own.

## Server Framework
If you're familiar with server-side MVC web frameworks like Rails and Laravel, Remix is the View and Controller, but it leaves the Model up to you. There are a lot of great databases, ORMs, mailers, etc. in the JavaScript ecosystem to fill that space. Remix also has helpers around the Fetch API for cookie and session management.

Instead of having a split between View and Controller, Remix Route modules take on both responsibilities.

Most server-side frameworks are &quot;model focused&quot;. A controller manages multiple URLs for a single model.

## Welcome to Remix!
We are happy you're here!

Remix is a full stack web framework that lets you focus on the user interface and work back through web fundamentals to deliver a fast, slick, and resilient user experience that deploys to any Node.js server and even non-Node.js environments at the edge like Cloudflare Workers.

Want to know more? Read the Technical Explanation of Remix

This repository contains the Remix source code. This repo is a work in progress, so we appreciate your patience as we figure things out.

## Documentation
For documentation about Remix, please
</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Final-Answers:">Final Answers:<a class="anchor-link" href="#Final-Answers:">&#182;</a></h3><ul>
<li>Remix aims to make it easy to use standard APIs.</li>
<li>You can learn about web development in general with Remix.</li>
<li>Remix is a framework that plays the role of both the View and Controller.</li>
<li>Remix leaves the model to the user.</li>
<li>Remix provides helpers for the Fetch API.</li>
<li>Remix can be deployed in non-Node.js environments such as Node.js servers and Cloudflare Workers.</li>
</ul>
<h3 id="Notes-on-API-usage:">Notes on API usage:<a class="anchor-link" href="#Notes-on-API-usage:">&#182;</a></h3><p>Regarding ChatGPT on the web, it is currently still an opt-out format (not used for retraining if you apply) as of March 10, 2023, but the ChatGPI API is an opt-in format (not used for retraining unless you apply), and it has been decided that it will not be used for actual model improvement. (It is stored for 30 days for legal monitoring purposes.) Since execution via the API reduces the risk of leakage to third parties other than OpenAI and is not used for retraining, the threshold for using confidential information with ChatGPT seems to have been lowered. The price of the API (gpt-3.5-turbo) is relatively inexpensive at 0.002 dollars per 1000 tokens, while the embedded model (text-embedding-ada-002) is 0.0004 dollars per 1000 tokens. However, if you try to create an embedded model for a large number of files, it will cost more than expected. If the number of tokens cannot be predicted in advance, it is a good idea to calculate the price in advance and decide whether to execute it as follows:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">tiktoken</span>

<span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="s1">&#39;text-embedding-ada-002&#39;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
<span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">get_docs</span><span class="p">(</span><span class="s1">&#39;_posts&#39;</span><span class="p">):</span>
    <span class="n">text</span> <span class="o">+=</span> <span class="n">doc</span><span class="o">.</span><span class="n">page_content</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">token_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">allowed_special</span><span class="o">=</span><span class="s1">&#39;all&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Estimated price: </span><span class="si">{</span><span class="n">token_count</span><span class="o">*</span><span class="mf">0.00000004</span><span class="si">}</span><span class="s1"> USD&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Estimated price: 0.03964988 USD
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Summary:">Summary:<a class="anchor-link" href="#Summary:">&#182;</a></h3><p>Since ChatGPT learns using past data, it cannot answer questions about the latest information or information that is not publicly available on the internet. This time, by mixing context related to the question content into the prompt, we were able to answer questions about the latest data and files saved locally.</p>
<p>If this article has been even a little helpful to you, I would be delighted. If you have any questions or comments, please feel free to contact me.</p>

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/04/04/LangChain-Document-Loader/#more" class="btn btn-default more">阅读此文</a>
</div>

		
			
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2023-03-21 </div>
			<div class="article-title"><a href="/2023/03/21/Try-BioGPT/" >Try BioGPT</a></div>						
		</h3>
	


			<div class="entry">
  <div class="row">
  
	<div class="col-md-12">
	  
	
	  <div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/microsoft/BioGPT">https://github.com/microsoft/BioGPT</a></li>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/biogpt">https://huggingface.co/docs/transformers/main/en/model_doc/biogpt</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting torch
  Using cached torch-2.0.0-cp310-none-macosx_10_9_x86_64.whl (139.8 MB)
Collecting torchvision
  Downloading torchvision-0.15.1-cp310-cp310-macosx_10_9_x86_64.whl (1.5 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">1.5/1.5 MB</span> <span class="ansi-red-fg">2.4 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Collecting torchaudio
  Downloading torchaudio-2.0.1-cp310-cp310-macosx_10_9_x86_64.whl (3.9 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">3.9/3.9 MB</span> <span class="ansi-red-fg">3.3 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Collecting networkx
  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">2.0/2.0 MB</span> <span class="ansi-red-fg">3.2 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Collecting sympy
  Downloading sympy-1.11.1-py3-none-any.whl (6.5 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">6.5/6.5 MB</span> <span class="ansi-red-fg">3.4 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Requirement already satisfied: jinja2 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from torch) (3.1.2)
Requirement already satisfied: filelock in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from torch) (3.9.0)
Requirement already satisfied: typing-extensions in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from torch) (4.4.0)
Requirement already satisfied: numpy in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from torchvision) (1.24.2)
Collecting pillow!=8.3.*,&gt;=5.3.0
  Downloading Pillow-9.4.0-2-cp310-cp310-macosx_10_10_x86_64.whl (3.3 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">3.3/3.3 MB</span> <span class="ansi-red-fg">3.5 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Requirement already satisfied: requests in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from torchvision) (2.28.1)
Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from jinja2-&gt;torch) (2.1.1)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;torchvision) (3.4)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;torchvision) (2.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;torchvision) (2022.12.7)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;torchvision) (1.26.14)
Collecting mpmath&gt;=0.19
  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">536.2/536.2 kB</span> <span class="ansi-red-fg">3.0 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>a <span class="ansi-cyan-fg">0:00:01</span>
Installing collected packages: mpmath, sympy, pillow, networkx, torch, torchvision, torchaudio
Successfully installed mpmath-1.3.0 networkx-3.0 pillow-9.4.0 sympy-1.11.1 torch-2.0.0 torchaudio-2.0.1 torchvision-0.15.1
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>transformers
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting transformers
  Downloading transformers-4.27.2-py3-none-any.whl (6.8 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">6.8/6.8 MB</span> <span class="ansi-red-fg">3.9 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Requirement already satisfied: requests in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (2.28.1)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (2022.10.31)
Requirement already satisfied: packaging&gt;=20.0 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (22.0)
Requirement already satisfied: filelock in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (3.9.0)
Collecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1
  Downloading tokenizers-0.13.2-cp310-cp310-macosx_10_11_x86_64.whl (3.8 MB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">3.8/3.8 MB</span> <span class="ansi-red-fg">3.0 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
Collecting huggingface-hub&lt;1.0,&gt;=0.11.0
  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">199.8/199.8 kB</span> <span class="ansi-red-fg">3.4 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>a <span class="ansi-cyan-fg">0:00:01</span>
Requirement already satisfied: tqdm&gt;=4.27 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (4.65.0)
Requirement already satisfied: pyyaml&gt;=5.1 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (6.0)
Requirement already satisfied: numpy&gt;=1.17 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from transformers) (1.24.2)
Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.11.0-&gt;transformers) (4.4.0)
Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;transformers) (3.4)
Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;transformers) (1.26.14)
Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;transformers) (2.0.4)
Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from requests-&gt;transformers) (2022.12.7)
Installing collected packages: tokenizers, huggingface-hub, transformers
Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">set_seed</span><span class="p">,</span> <span class="n">BioGptTokenizer</span><span class="p">,</span> <span class="n">BioGptForCausalLM</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 加载模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BioGptForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/biogpt-large&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">markdown</span>
<span class="kn">import</span> <span class="nn">IPython</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">revChatGPT.V1</span> <span class="kn">import</span> <span class="n">Chatbot</span><span class="p">,</span> <span class="n">configure</span>


<span class="n">bot</span> <span class="o">=</span> <span class="n">Chatbot</span><span class="p">(</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">configure</span><span class="p">(),</span>
    <span class="n">conversation_id</span> <span class="o">=</span> <span class="s1">&#39;a2a0e9d8-b880-4873-95b4-59ac6efdbb23&#39;</span><span class="p">,</span>
    <span class="n">lazy_loading</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">用 Hugging Face 下载的模型一般存储在哪里？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">):</span>
    <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]))</span>
    <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>使用 Hugging Face 下载的模型一般存储在本地的 <code>~/.cache/huggingface/</code> 目录下。该目录下会按照模型名称和版本号的组合来创建相应的子目录，存储着相应的模型文件和元数据文件。可以通过指定 <code>cache_dir</code> 参数来修改默认的存储路径。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--microsoft--biogpt-large/
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
du<span class="w"> </span>-sh<span class="w"> </span>~/.cache/huggingface/hub/models--microsoft--biogpt-large/
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>5.9G	/Users/saintway/.cache/huggingface/hub/models--microsoft--biogpt-large/
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>可以使用以下代码来估计 Hugging Face 模型文件的参数总量：</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">hub</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;huggingface/transformers&#39;</span><span class="p">,</span> <span class="s1">&#39;model_name&#39;</span><span class="p">)</span>
<span class="n">num_params</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of parameters: </span><span class="si">{</span><span class="n">num_params</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
<p>其中，<code>model_name</code> 是指 Hugging Face 模型的名称，例如 <code>gpt2</code>, <code>bert-base-uncased</code>等。</p>
<p>这段代码会加载相应的模型，并统计其中可训练参数的数量，输出结果为参数总量。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>要估计 Hugging Face 模型文件的参数总量，可以使用 Hugging Face 库中的 <code>num_parameters()</code> 方法。以下是一个示例，说明如何使用 Hugging Face 库来估计模型的参数总量：</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModel</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;model_name&#39;</span><span class="p">)</span>
<span class="n">num_params</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">num_parameters</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The model has </span><span class="si">{</span><span class="n">num_params</span><span class="si">}</span><span class="s2"> parameters.&quot;</span><span class="p">)</span>
</pre></div>
<p>在这里，<code>AutoModel.from_pretrained()</code> 方法用于加载模型。将 <code>&quot;model_name&quot;</code> 替换为你想要估计参数总量的模型名称。然后，<code>num_parameters()</code> 方法返回模型中的总参数数量。</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
pip<span class="w"> </span>install<span class="w"> </span>sacremoses
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>Collecting sacremoses
  Downloading sacremoses-0.0.53.tar.gz (880 kB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">880.6/880.6 kB</span> <span class="ansi-red-fg">3.0 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>00:0100:01
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status &#39;done&#39;
Requirement already satisfied: regex in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from sacremoses) (2022.10.31)
Requirement already satisfied: six in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from sacremoses) (1.16.0)
Requirement already satisfied: click in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from sacremoses) (8.1.3)
Collecting joblib
  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)
     <span class="ansi-black-intense-fg">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span class="ansi-green-fg">298.0/298.0 kB</span> <span class="ansi-red-fg">3.5 MB/s</span> eta <span class="ansi-cyan-fg">0:00:00</span>a <span class="ansi-cyan-fg">0:00:01</span>
Requirement already satisfied: tqdm in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (from sacremoses) (4.65.0)
Building wheels for collected packages: sacremoses
  Building wheel for sacremoses (setup.py): started
  Building wheel for sacremoses (setup.py): finished with status &#39;done&#39;
  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=ef1e1d98064b22881c14de405dff663a3972fb9f39a92195a28612459e1de7f9
  Stored in directory: /Users/saintway/Library/Caches/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb
Successfully built sacremoses
Installing collected packages: joblib, sacremoses
Successfully installed joblib-1.2.0 sacremoses-0.0.53
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 加载分词器</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BioGptTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/biogpt-large&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">bot</span><span class="o">.</span><span class="n">ask</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">用 Hugging Face 下载的 tokenizer 一般存储在哪里？</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">):</span>
    <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">IPython</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">]))</span>
    <span class="n">IPython</span><span class="o">.</span><span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>



<div class="output_markdown rendered_html output_subarea ">
<p>使用 Hugging Face 下载的 tokenizer 存储位置取决于所选择的 tokenizer。一些 tokenizer 会默认存储在缓存目录中，例如 <code>$HOME/.cache/huggingface/tokenizers</code>，而其他 tokenizer 可能需要手动指定保存位置。在使用 tokenizer 时，可以使用 <code>from_pretrained</code> 方法指定 tokenizer 的名称和路径来加载已下载的 tokenizer。例如：</p>
<pre><code>from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;, cache_dir=&quot;/path/to/tokenizer&quot;)
</code></pre>
<p>这里 <code>cache_dir</code> 参数指定了 tokenizer 存储的路径。如果未指定该参数，则默认使用缓存目录。</p>

</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span>%%bash
ls<span class="w"> </span>-lah<span class="w"> </span>~/.cache/huggingface/hub/models--microsoft--biogpt-large/snapshots/c6a5136a91c5e3150d9f05ab9d33927a3210a22e
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>total 0
drwxr-xr-x  9 saintway  staff   288B Mar 21 23:18 <span class="ansi-cyan-intense-fg ansi-bold">.</span>
drwxr-xr-x  3 saintway  staff    96B Mar 21 22:31 <span class="ansi-cyan-intense-fg ansi-bold">..</span>
lrwxr-xr-x  1 saintway  staff    52B Mar 21 22:31 <span class="ansi-magenta-fg">config.json</span> -&gt; ../../blobs/6781cfc676cb2384020b1b5fdd04817536c723fb
lrwxr-xr-x  1 saintway  staff    52B Mar 21 23:18 <span class="ansi-magenta-fg">generation_config.json</span> -&gt; ../../blobs/9475ded3180a2d64c6903ebcf2aeb0705703c12a
lrwxr-xr-x  1 saintway  staff    52B Mar 21 23:18 <span class="ansi-magenta-fg">merges.txt</span> -&gt; ../../blobs/01e121aa8483873879577612f38761f2fd9ce5ae
lrwxr-xr-x  1 saintway  staff    76B Mar 21 23:17 <span class="ansi-magenta-fg">pytorch_model.bin</span> -&gt; ../../blobs/d1753ea5af6449aaf63a105a59a619632b78cbfc2c294ba78f3164156af8c8bf
lrwxr-xr-x  1 saintway  staff    52B Mar 21 23:18 <span class="ansi-magenta-fg">special_tokens_map.json</span> -&gt; ../../blobs/e66da1703089a0329ca9eaf51638805d8ce1b322
lrwxr-xr-x  1 saintway  staff    52B Mar 21 23:18 <span class="ansi-magenta-fg">tokenizer_config.json</span> -&gt; ../../blobs/e044f8771c083e25f9accfecb9c8f1408cb3c42f
lrwxr-xr-x  1 saintway  staff    52B Mar 21 23:18 <span class="ansi-magenta-fg">vocab.json</span> -&gt; ../../blobs/66ddce4fab432cd017c0db812211c30d25acce1e
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 设置随机数种子</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">2023</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 设置 pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s1">&#39;text-generation&#39;</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 生成文本</span>
<span class="c1"># max new tokens：生成的最多 token 数量</span>
<span class="c1"># num_return_sequences：生成的序列个数</span>
<span class="n">generator</span><span class="p">(</span><span class="s1">&#39;COVID 19 is&#39;</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;generated_text&#39;: &#39;COVID 19 is an emerging pandemic viral, the virus cannot be eliminated completely in the absence of vaccine. A prophylactic approach&#39;},
 {&#39;generated_text&#39;: &#39;COVID 19 is ongoing with several plans in the field; however the virus did not spread to Canada by 2 0 1&#39;},
 {&#39;generated_text&#39;: &#39;COVID 19 is a global health emergency. There are currently 2 million confirmed and over 2 6 0, 0 0 0&#39;},
 {&#39;generated_text&#39;: &#39;COVID 19 is a more virulent, recently emerged pandemic virus strain belonging to the Middle East-Africa 2 0 1 2&#39;},
 {&#39;generated_text&#39;: &#39;COVID 19 is at the threshold of approval as the first FDA-approved drug for treating IAVs. 1 2 1&#39;}]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">set_seed</span><span class="p">,</span> <span class="n">BioGptTokenizer</span><span class="p">,</span> <span class="n">BioGptForCausalLM</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 加载模型</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BioGptForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/BioGPT-Large-PubMedQA&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 加载分词器</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BioGptTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;microsoft/BioGPT-Large-PubMedQA&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 设置随机数种子</span>
<span class="n">set_seed</span><span class="p">(</span><span class="mi">2023</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 设置 pipeline</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span>
    <span class="s1">&#39;text-generation&#39;</span><span class="p">,</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
<span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 生成文本</span>
<span class="c1"># max new tokens：生成的最多 token 数量</span>
<span class="c1"># num_return_sequences：生成的序列个数</span>
<span class="n">generator</span><span class="p">(</span><span class="s1">&#39;question: What is Synaptotagmin-7? answer:&#39;</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">760</span><span class="p">,</span> <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[&nbsp;]:</div>




<div class="output_text output_subarea output_execute_result">
<pre>[{&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7, a neuron-specific Synaptotagmin isoform, is concentrated at the synaptic vesicle membrane. Although it displays sequence similarity to synaptotagmin-1, it is unclear whether it functions as a Ca (2 +) sensor in fast neurotransmission. We generated mutant mice lacking Synaptotagmin-7. Although basic synaptic transmission appeared largely normal, the mice displayed deficiencies in motor coordination and hyperactivity. At the behavioral level, the phenotypes were similar but less severe than those observed in mice lacking synaptotagmin-1. Consistent with this, the Ca (2 +) dependence of evoked neurotransmitter release was reduced but not abolished in mutant neuromuscular synapses. Nevertheless, both fast and slow modes of synaptic vesicle exocytosis were affected. Our data suggest that Synaptotagmin-7 is not essential for fast Ca (2 +) -triggered neurotransmitter release, but it does contribute to the fast component of the overall synaptic response. Hence, Synaptotagmins-1 and -7 cannot completely substitute for each other, revealing remarkable functional specializations within the synaptotagmin family. We also found that in vivo targeting of Synaptotagmin-7 to synaptic vesicles was impaired in synaptotagmin-1-deficient mice, demonstrating that the two Synaptotagmins can interact genetically and suggest that synaptotagmin-1 may regulate the trafficking of Synaptotagmin-7. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; Neurotransmitter-containing synaptic vesicles fuse with the plasma membrane in response to Ca (2 +) influx into the presynaptic nerve terminal. The bestcharacterized molecular components of the Ca (2 +) -triggering reaction were first identified as the membrane-bound synaptic vesicle proteins Synaptotagmin-1 (also called Syt1) and Doc2 (for Double C2-domain) (Südhof, 2 0 0 4). Although the presence of Syt1 is necessary and sufficient for fast Ca (2 +) -triggered neurotransmitter release (Geppert et al., 1 9 9 4; Fernandez-Chacon et al., 2 0 0 1), it remains unclear whether Doc2 also has a role in this process (Groffen et al., 2 0 1 0). Additional Ca (2 +) sensors have been implicated recently in fast synaptic transmission (Xue et al., 2 0 0 9; Pang et al., 2 0 1 0a; Bacaj et al., 2 0 1 1). These include Synaptotagmin-7 (also called Syt7), another neuronal isoform that shows sequence similarity to Syt1 (Suttleton et al., 1 9 9 5). Synaptotagmin-7 is a component of nerve terminals (Südhof et al., 1 9 9 3; Fox et al., 2 0 0 6; Gustavsson et al., 2 0 0 8) and synaptic vesicles, and it binds Ca (2 +) (Fukuda et al., 2 0 0 2a). Biochemical studies suggest a preference of Synaptotagmin-7 for membranes containing negatively charged lipids (Sugita et al., 1 9 9 4; Fukuda et al., 1 9 9 6). Moreover, like Syt1, Synaptotagmin-7 also interacts with soluble N-ethylmaleimide-sensitive factor attachment protein receptors (SNAREs) and other proteins implicated in membrane fusion (Fukuda et al., 1 9 9 6, 2 0 0 3a, b; Chapman, 2 0 0 8; Giraudo et al., 2 0 0 9; Wang et al., 2 0 0 9). However, the precise function of Synaptotagmin-7 in synaptic transmission and the relationship between Synaptotagmin-7 and Syt1 are poorly understood. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; In cultured neurons, Synaptotagmin-7 is not&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: There are several splice variants of rat synaptotagmin-7 that have different properties. This commentary discusses the most recent findings about synaptotagmin-7, with particular focus on how the properties of a different splice variant, Syt7 A, may relate to a specific function of synaptotagmin-7 in neurons that is distinct from its role as a Ca (2 +) sensor for fast synaptic vesicle exocytosis. In this scenario, Syt7 A would be a structural adaptor bringing other partners of Ca (2 +) - and diacylglycerol-mediated signaling pathways close to membrane fusion. Critical comments: The main finding of this commentary is that there is still little information about the function of Syt7 A, due to the lack of reagents that discriminate between the different splice variants. Experiments that discriminate between Syt7 A and the other variants, for example by expression of splice variant-specific antibodies or by RNA interference, will be required in order to clarify the function of Syt7 in neurons. Ultimately, this information will help elucidate the different functions that are served by synaptotagmin isoforms and how they act in concert to control physiological outcomes such as synaptic strength and plasticity. Ed Taylor Weiss, Ph.D., Marikki Jahn, Ph.D., and Marijke Lipp, M.D., discuss the implications of these discussions for future directions in research in vesicle trafficking at synapses. Listen to Their Point: Taylor Weiss and Lipp discuss the recent findings on the neuronal BACKGROUNDS sensing totagmin-7. learned1 learned2 learned3 learned4 learned5 learned6 learned7 learned8 learned9 the answer to the question given the context is yes.&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 (Syt7) is unique among synaptotagmin isoforms in its expression profile: Syt7 is a restricted marker of cholinergic neurons, which are the primary fast-transmitting excitatory neurons in the central and peripheral nervous systems of vertebrates. Here, we review evidence that Syt7 is a calcium sensor for transmitter release in cholinergic neurons. Expression of Syt7 at the cholinergic neuron presynaptic active zone suggests that Syt7 may regulate fast synaptic transmission via a direct calcium-sensing mechanism. Biochemical and biophysical data indicate that Syt7 has unusual calcium-binding properties and that it may bind phospholipids with high affinity and cooperativity in the presence of calcium. Structural data indicate that the long C2 domain region of Syt7 may fold into a three-dimensional structure unique among C2 domain proteins. The combined data support the hypothesis that Syt7 is a specialized fast-transmitting calcium sensor for transmitter release at some cholinergic synapses. We speculate that the presence of multiple fast-transmitting calcium sensors in individual fast synapses may provide a general mechanism to rapidly modulate synaptic strength. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7, in common with the other four synaptotagmins, is localized to synaptic vesicles but its precise function is unclear. In this study, we have addressed this question by analyzing mice lacking synaptotagmin-7 (Syt7 (- / -)). We first developed monoclonal antibodies that specifically recognize mouse synaptotagmin-7. These antibodies revealed that synaptotagmin-7 is localized to a subset of synaptic vesicles and to a few intracellular puncta in cultured neurons, indicating that synaptotagmin-7 is also present outside the synapse. Immunohistochemical analysis of the adult brain revealed that synaptotagmin-7 is enriched in the thalamus, the subthalamic nucleus, and the zona incerta, structures that project their axons to the cerebral cortex. In the cerebral cortex, synaptotagmin-7 immunoreactivity was concentrated in some layers, particularly layer V. Syt7 (- / -) mice did not exhibit any significant deficits in motor function, sensory transmission, short- or long-term synaptic plasticity, or cognitive function. Electrophysiological analysis, however, revealed a small but significant decrease in the readily releasable pool of vesicles in the dentate gyrus, a subsynaptic structure known to play a critical role in cognitive processes. Additional morphological analysis indicated that Syt7 (- / -) mice have a normal number and distribution of boutons in the cerebral cortex, but there is a selective reduction in the number of perforated synapses in the middle layers of the primary somatosensory cortex, suggesting that Syt7 may be important for the formation or maturation of these synapses. Our results indicate that synaptotagmin-7 is not essential for fast synaptic transmission but does contribute to some forms of synaptic plasticity. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: We show here that synaptotagmin-7 constitutes an unusual synaptic vesicle protein, both structurally and functionally. It is not a calcium sensor but instead modulates fusion pore dilation. Thus, it has opposite functions in neuroexocytosis to neuroligin, synaptotagmin-1, complexin, and Munc1 3. Our results highlight that calcium sensor function is not a common feature of synaptic vesicle proteins but is confined to a small number of highly specialized proteins. These results have interesting evolutionary implications because synaptotagmin-7 homologs are found in all animals but appear to have been lost in vertebrates. Synaptotagmin-7 provides a novel molecular tool with which to study fusion pore dynamics. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: According to the prevailing view, synaptotagmin-7 (Syt7) is primarily localized to neurons where it functions as an inhibitor of neurotransmitter release. However, in mouse brain, Syt7 is not restricted to neurons and is also expressed by astrocytes. In this study, we used conditional gene targeting in mice to investigate whether astrocytic Syt7 regulates synaptic transmission or contributes to the maintenance of energy homeostasis. Using a mouse model with astrocyte-specific deletion of Syt7 (cKO), we found that astrocytic Syt7 has no effect on basic synaptic transmission. In addition, deletion of astrocytic Syt7 does not affect basal motor control or the response to metabolic challenges. Together, our results demonstrate that astrocytic Syt7 does not regulate basic synaptic transmission nor is it required for the homeostatic response to metabolic stress. These findings challenge the prevailing view of Syt7 and strongly suggest that the physiological functions of Syt7 in the mammalian central nervous system are not confined to neurons. Calcium-binding proteins with sequence similarity to Syt7, such as Syt1 2, may be more appropriate to serve as Ca2 + sensors in astrocytes. Astrocytic Syt1 2 may function in conjunction with different modes of stimulus-induced Ca2 + elevations to modulate synaptic transmission or to trigger other signaling cascades that trigger astrocytic processes that modulate synaptic transmission. learned1 learned2 learned3 learned4 learned5 learned6 learned7 learned8 learned9 the answer to the question given the context is no.&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 is one of the two calcium sensors for fast neurotransmitter release found at parallel fiber to Purkinje cell synapses in the cerebellum. Both Syt1 and Syt2 are localized to the nerve terminal and are essential for fast exocytosis. In Syt1 KO mice, neurotransmission is largely abolished, whereas in Syt2 KO mice, it remains partially intact. Here, we investigated the function of Syt7 in fast neurotransmitter release using mice with a deletion of Syt1 and Syt7 and mice with deletions of Syt1, Syt2, and Syt7. Syt7 deletion did not alter synaptic transmission, and Syt1 and Syt2 double deletion caused a complete loss of fast transmission. In Syt1, Syt2, and Syt7 triple KO mice, neurotransmission was completely abolished. Neurotransmission in Syt1, Syt2, and Syt7 double KO mice could be restored by expression of Syt1, Syt7, or both Syt1 and Syt7. These data suggest that Syt1, Syt2 effusion, and Syt7 can function downstream of calcium influx and at the interface of calcium sensor and core fusion machinery. However, calcium transients were smaller in Syt1, Syt2, and Syt7 triple KO mice compared with Syt1, Syt2 double KO mice, indicating partial overlap in their calcium sensitivity in presynaptic terminals. Syt7 was not sufficient to trigger spontaneous release, even during prolonged stimulation. Thus, the calcium-dependent triggering of exocytosis by Syt7 probably requires additional regulatory factors. Overall, the function of Syt7 is at odds with the proposed function of the homologous proteins Syt3 and Syt5 as calcium sensors for slow synaptic vesicle exocytosis. Syt1 and Syt2 are redundantly required for fast and slow neurotransmitter release. Thus, fast and slow synaptic vesicle exocytosis are mediated by different calcium sensors. These results challenge the concept that different calcium sensor proteins mediate fast and slow synaptic vesicle exocytosis. Guest Editor: David Poser, MD, PhD. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: In this work, we explore the role of synaptotagmin-7 (Syt7), an extensively studied presynaptic Ca (2 +) sensor of neuroexocytosis. Unlike other Syt family members, which have a well-established function in fast neuroexocytosis, Syt7 appears to have little effect on these processes when analyzed in Syt7-deficient mice. Nevertheless, some studies have revealed that this molecule functions in a slower form of neurotransmission mediated by small clear vesicles. Because studies on the function of Syt7 have been mainly performed in neurotoxicity and / or Syt7 transgenic models, its precise role in central nervous system (CNS) remains to be clarified. Here, we analyzed the levels and expression pattern of Syt7 in the CNS and correlated these data with Syt7-deficient mouse behavioral studies, because altered CNS function might produce compensatory changes that obscure any particular physiological contribution of Syt7. We found that although most regions of the adult CNS express detectable levels of Syt7, some neurons, such as thalamocortical axons in the somatosensory thalamus and thalamocortical synapse cartridges that impinge on neurons in the ventrobasal thalamus (VB) in the somatosensory pathway, lack detectable talar levels of this molecule. Consistent with this observation, Syt7-deficient thalamocortical projections in Syt7 (- / -) mice showed normal evoked transmission even at high frequencies of stimulation. Moreover, behavioral studies revealed that Syt7 (- / -) athion mice, although exhibiting alterations in sensory processing, including altered whisker-induced tactile responses, did not display obvious alterations in sensorimotor gating functions. Our results indicate that while Syt7 is widely expressed in the adult nervous system, most central synapses, with the notable exception of some thalamocortical projections, appear to function normally in its absence. Synapse 6 8: 2 8 5-2 9 4, 2 0 1 0. Copyright © 2 0 1 0 Wiley-Liss, Inc. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: This study raises several questions regarding synaptotagmin-7 including its localization, expression profile, and calcium sensing function in cellular systems and behavioral assays. We found that synaptotagmin-7 in mice has a broad expression profile in the nervous system, albeit at high levels in certain brain regions and spinal cord motor neurons. The presence of synaptotagmin-7 in sensory neurons and the spinal cord is consistent with behavioral experiments showing that the loss of synaptotagmin-7 leads to defects in pain processing and gait. However, loss of synaptotagmin-7 did not cause lethality that would be expected if synaptotagmin-7 regulated a widespread form of calcium-triggered vesicle fusion that is required for vesicular neurotransmitter release and the maintenance of cellular homeostasis. This is different from other synaptotagmin family members, which are essential components of the core secretory machinery. At the cellular level, we found that loss of synaptotagmin-7 did not affect calcium-triggered neurotransmitter release in several types of neurons, despite the presence of synaptotagmin-7 at synapses, suggesting that synaptotagmin-7 functions redundantly with other calcium sensors. Our analysis was limited by the lack of a specific antibody, but future work with new antibodies and genetically modified animals that express tagged forms of synaptotagmin-7 will be necessary to further examine this question. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: In this manuscript we have reevaluated the Ca2 + -dependent function of Syt7 using mice deficient in Syt7 and knockin mice expressing a nonageable version of Syt7. We demonstrate that absence of endogenous Syt7 drastically reduced the readily releasable pool of synaptic vesicles containing synaptotagmin-1 (Syt1) but had little effect on the size of the readily releasable pool containing Syt7. At the same time, the rate of spontaneous release was elevated in Syt1-deficient neurons and not affected by absence of Syt7 disseminated throughout the presynaptic axon. We conclude that, in neurons, Syt7 is not central for the Ca (2 +) triggering of vesicle exocytosis, but it exerts a tonic inhibition on vesicle fusion in the absence of Syt1. This unexpected function of Syt7 could help to explain the evolutionary conservation of two synaptotagmins in all animals. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: We provide structural, biochemical, and cell biological evidence that Syt7 is an SV Ca (2 +) sensor, thereby extending the molecular repertoire of synaptotagmin proteins. The Ca (2 +) affinities measured for several synaptotagmin variants suggest a contribution of synaptotagmin-7 to fast, synchronous transmitter release. However, the observation that the phenotype of Syt7 KO is quantitatively weaker than that of synaptotagmin-1 KO suggests that Ca (2 +) -mediated release can be mediated by additional synaptotagmin isoforms. We discuss possible interactions between Syt7 and other synaptotagmin isoforms and SV proteins and consider functional parallels to complexins. Syt1 / 7 / 9KO triple mutant mice will be instrumental for future studies to define the role of Syt7 within the core release machinery. Since Syt7 KO did not compromise synaptic transmission in one study (Pang et al., 2 0 0 6a), it will also be important to rigorously examine Syt7 KO mice in multiple genetic backgrounds to determine its overall contribution to regulated synaptic vesicle exocytosis. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: The search for additional, functionally important Synaptotagmin genes that may reside within the human genome has been intense. However, to date, only the Synaptotagmin-1 (Syt1) gene has been localized to human chromosome 1 9p1 3. 2, a location that has thus far ruled out additional human homologs. Using bioinformatics, we identified seven previously unplaced human sequences that exhibit extensive similarity to the Syt1 protein. We report the characterization of one of these genes, Syt7, which is localized on human chromosome 2 0p1 3. 2. A Syt7 full-length cDNA clone was obtained, and a predicted open reading frame of 1, 0 1 8 bp encoding a protein that is approximately 7 0% identical to Syt1. Phylogenetic analysis placed human Syt7 within the same clade as Syt1 and a second member, Syt9, suggesting that these three genes may have arisen via a gene duplication event. Syt7 is preferentially expressed in brain and peripheral tissues, including skeletal muscle, heart, and kidney, whereas Syt1 is mainly expressed in brain, with lower levels found in skeletal muscle, heart, and pancreas. Syt1 and Syt7 exhibit different patterns of synaptic tingitation. Both proteins are found on synaptic vesicles but exhibit distinct patterns of distribution within various subcellular membrane fractions of brain homogenates, suggesting that they localize to distinct membranous compartments. The intracellular distribution patterns of Syt7 and Syt1 also differ following KCl-depolarization and ionomycin treatments of PC1 2 cells, suggesting that Syt7 and Syt1 undergo distinct modes of regulated trafficking. Finally, Syt7 and Syt1 exhibit significantly different patterns of expression in certain brain regions. These studies suggest that human Syt7, like Syt1, is a member of the synaptotagmin family and may play a role in synaptic transmission. However, the distinct patterns of expression and trafficking observed for Syt7 and Syt1 suggest that these two proteins may perform unique functions within the vesicle-mediated secretory pathway. Syt7 should now be considered a candidate for genetic mutations that underlie diseases of synaptic transmission, such as epilepsy or migraine. Display: The human genome contains seven previously unplaced Synaptotagmin-like sequences. We characterized one of these genes, nemo-bin 7 (syt7), which exhibits extensive similarity to Syt1. Syt7 is preferentially expressed in brain and peripheral tissues, including skeletal muscle, heart, and kidney. Syt7 is present on synaptic vesicles but exhibits distinct patterns of distribution within membrane fractions. syt7 and Syt1 exhibit significantly different patterns of expression in certain brain regions. Syt7 should now be considered a candidate for genetic mutations that underlie epilepsy or migraine. Chapman, H.A., Guan, Y., Li, M.-S., Wang, T., Paudel, F.J.G., Diogo, J., Bao, X., Jin, B., Luo, C., Xu, T., Zhao, Y., Rush, S.M., Bao, H., Jin, Y., Liu, Y., Liu, S., Sheng, J., Wang, X. 9B new human Synaptotagmin-like genes: tissue-specific expression, alternative splicing and chromosomal assignment. Dev. Dyn. 2 3 6, 8 4 7-8 6 0.Page1001 [PMC free article] [PubMed] [Google Scholar] [Google Scholar] 1 2. Bennett, V.A., Geppert, S., Chapman, J.H., Bennett, J.D., Chapman, J.W., Bennett, M.P., Borgen, E., Brown, R., Burgoyne, P., Cooper, J.A., Davis, M.W., Davis, J.E., Davis, R.J.R., Dierickx, R., Deptré, A., Dijkman, L., Edwards, P.H., Fink, P.O., Fink, C.&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7, but not other mammalian synaptotagmins, regulates dense-core vesicle exocytosis in hippocampal neurons, suggesting that it is the calcium sensor for synaptic transmission. Synaptotagmin-7 has been controversial as a synaptic vesicle protein at odds with its genetic discovery. However, we show here that endogenous synaptotagmin-7 is present in purified synaptic vesicles and, moreover, is colocalized with the synaptic vesicle protein, synaptotagmin-1, in presynaptic nerve terminals. In neurons derived from synaptotagmin-7 knock-out mice, evoked neurotransmitter release is significantly reduced relative to wild-type readmitted responses, whereas spontaneous release is unchanged. Direct calcium sensing by synaptotagmin-7 is suggested by its ability to bind calcium and regulate neuronal exocytosis in a recombinant expression system. Expression of a synaptotagmin-7 C2B domain mutant known to impair calcium binding abolishes the ability of synaptotagmin-7 to inhibit regulated neuronal exocytosis. We propose that synaptotagmin-7 functions as the calcium sensor for calcium-triggered dense-core vesicle exocytosis in hippocampal neurons. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; Keywords: &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; dense-core vesicle, SNARE, synaptic transmission, synaptotagmin, calcium &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; Neurotransmitter release is regulated by intracellular calcium, and a long-standing paradigm is that calcium acts by binding to intracellular proteins called synaptotagmins (Geppert et al., Losonczy and Mattson, 2 0 0 2). Several synaptotagmins, including synaptotagmin-1 and synaptotagmin-7, act as calcium sensors for regulated dense-core vesicle exocytosis in neuroendocrine cells, but it remains unclear which, if either, of these synaptotagmins function as the calcium sensor in neurons (Fernandez-Chacon et al., 2 0 0 1; Nagy et al., 2 0 0 2, 2 0 0 4; Geppert et al., 2 0 0 3; Gustavsson et al., 2 0 0 5; Bacaj et al., 2 0 0 6). A major experimental problem has been the lack of a specific genetic manipulation that completely eliminates any particular synaptotagmin. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; We and others recently developed a genetic approach using embryonic stem (ES) cell – derived neurons (Sun et al., 2 0 0 3, 2 0 0 5; Dibotta et al., 2 0 0 5). Neurons are differentiated from ES cells and subsequently subjected to genetic manipulations including homologous recombination. Using this method, we showed that several of the neuronal SNAREs (soluble N-ethylmaleimide-sensitive factor attachment protein receptor), plasma membrane SNAREs, and calcium-dependent adaptors are involved in dense-core vesicle exocytosis from embryonic precursor neurons (Sun et al., 2 0 0 3; Dibotta et al., 2 0 0 特4ically highlighted in the text). Here, we show that another approach, the lentiviral-mediated expression of short interfering RNAs (siRNAs), can be used to inhibit the expression of endogenous synaptotagmin-7 in neurons. This approach demonstrates that synaptotagmin-7, but not other endogenous synaptotagmins, is a critical regulator of calcium-triggered dense-core vesicle exocytosis in hippocampal neurons. Finally, we demonstrate that synaptotagmin-7 binds calcium and regulates exocytosis in a manner analogous to other synaptotagmins. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; The lentiviral expression and rescue vectors. We generated four lentiviral expression vectors: pLVTHM encodes green fluorescent protein (GFP); pLVTH.Syt7-shRNA expresses a hairpin RNA targeting rat synaptotagmin-7 (sequence: 5 ′ -CCA&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 is widely expressed in neuronal and some non-neuronal cells and is thought to act as a Ca (2 +) sensor for synaptic vesicle exocytosis. However, whether Synaptotagmin-7 is essential for synchronous synaptic transmission is controversial. This controversy appears to be due, at least in part, to the fact that three different Synaptotagmin-7-deficient mouse lines have been independently generated, making it possible that the apparent phenotype results from the insertion of a &#34;balancer&#34; chromosome in one of these mouse lines. However, the &#34;balancer&#34; chromosome does not appear to have any functional consequences since it has been removed from the three different Synaptotagmin-7-deficient mouse lines that have been described so far. Thus, all three different Synaptotagmin-7-deficient mice generated so far, including the line that lacks the &#34;balancer&#34; chromosome, exhibit normal synaptic transmission at most synapses examined. In addition, the most detailed physiological analysis to date performed at the auditory nerve-cochlear nucleus synapse in Synaptotagmin-7-deficient mice has failed to detect any defect in synaptic transmission. One possible explanation for the lack of a strong phenotype in Synaptotagmin-7-deficient mice might be that Synaptotagmin-7 function can be partially substituted by another Synaptotagmin family member. However, this is unlikely since mice deficient in both Synaptotagmin-7 and Synaptotagmin-1 exhibit massive paralysis similar to that reported for mice deficient in Synaptotagmin-1 alone. In addition, the cytoplasmic domain of Synaptotagmin-7 interacts with many of the same soluble N-ethylmaleimide fusion protein attachment receptor (SNARE) proteins as those for Synaptotagmin-1, raising the possibility that Synaptotagmin-7 and Synaptotagmin-1 regulate exocytosis via similar molecular mechanisms. Thus, although no individual Synaptotagmin appears to be essential for exocytosis, it remains possible that different synaptic transmission functions are served by different Synaptotagmins. Alternatively, the lack of a strong synaptic transmission phenotype in Synaptotagmin-7-deficient mice might be due to functional redundancy among different Synaptotagmins, and the introduction of additional mutations will be necessary to uncover a role for Synaptotagmin-7 in synaptic transmission. Nevertheless, the functional characterization of additional Synaptotagmin family members and the generation of new mouse and Drosophila mutants deficient in other Synaptotagmin family members will be essential for elucidating the precise function (s) of this fascinating class of proteins. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 is a brain-specific synaptotagmin that was named after the fact that its expression is highly restricted to the nervous system. Recently, studies employing gene targeting in mice have uncovered functions for synaptotagmin-7 in synaptic vesicle exocytosis and neurotransmitter receptor trafficking at inhibitory synapses. The precise roles of synaptotagmin-7 and its relationship to other synaptotagmins remain to be fully elucidated. This article discusses our understanding of synaptotagmin-7, and highlights outstanding questions that will need to be addressed to further our understanding of the functions of this intriguing synaptotagmin. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: In contrast to the large body of literature on Synaptotagmin-1 and -2, little is known about Synaptotagmin-7. In cultured hippocampal neurons, Synaptotagmin-7 is localized to axons and synaptic terminals, and its Ca 2 + -dependent binding to phospholipid vesicles correlates well with the distribution of synaptotagmin-1. Furthermore, the overexpression of Synaptotagmin-7 in PC1 2 cells enhances secretory responses, whereas its downregulation by RNA interference (RNAi) decreases these responses. However, it is not clear whether Synaptotagmin-7 is the Ca 2 + sensor for fast neuroexocytosis. In particular, the properties of Synaptotagmin-7 that distinguish it from Synaptotagmin-1 and -2, such as its membrane topology or distribution within presynaptic terminals, have not been thoroughly investigated. In this study we addressed these questions by using two highly specific antibodies raised against different domains of Synaptotagmin-7. We found that Synaptotagmin-7 is a peripheral membrane protein that is anchored to synaptic vesicles via its transmembrane region, and it is present at lower levels than Synaptotagmin-1 within presynaptic terminals. Furthermore, our results suggest that Synaptotagmin-7 is a synaptotagmin isoform with both Ca 2 + -dependent and Ca 2 + -independent modes of phospholipid binding. In addition, we have identified a cytoplasmic fragment of Synaptotagmin-7 that contains the domains homologous to those that form the phospholipid binding sites of Synaptotagmins-1 and -2. Our results provide new insights into the properties of this poorly studied isoform of synaptotagmin and will be instrumental in designing experiments aimed at identifying the function of Synaptotagmin-7 in the nervous system. Anat Rec, 3 0 1: 4 7-5 8, 2 0 1 8. © 2 0 1 7 Wiley Periodicals, Inc. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 is a type I transmembrane protein with two C2 domains. It is predominantly expressed in neuroendocrine tissues, where it functions as an essential calcium sensor for fast calcium-triggered exocytosis. Synaptotagmin-7-deficient mice develop a severe and complex phenotype, characterized by growth retardation, ataxia, and lethal seizures. Biochemical and electrophysiological studies have shown that synaptotagmin-7 regulates synaptic vesicle exocytosis downstream of calcium channels but upstream of SNARES. At mammalian synapses, synaptotagmin-7 mainly localizes to dense-core vesicles in chromaffin cells, which suggests that it might also regulate the exocytosis of large dense-core vesicles. However, synaptotagmin-7-deficient mice that have been rescued by re-introduction of synaptotagmin-7 specific to mice do not develop a chromaffin cell phenotype. Instead, they show deficits in the regulated exocytosis of small synaptic vesicles, indicating that synaptotagmin-7 plays a differential role in fast exocytosis of different types of vesicles. This raises the question how synaptotagmin-7 functions as a calcium sensor for the exocytosis of different types of vesicles. Here, we discuss different modes of calcium decoding by synaptotagmins, including their different calcium affinities and different calcium sensitivities of interactions with SNAREs and membranes. We also consider evidence suggesting that synaptotagmin-7 might act as a trigger or clamp in the hierarchy of vesicle fusion during neuropeptide secretion from neuroendocrine cells. This article is part of a Special Issue entitled: Calcium-dependent proteins: Structure, functon and disease. Guest Editors: Geethia Kodec, Claus Heizmann, Jíbal Haucke, Thierry Monnet. Decisions are always oh-le-ti for you. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Most previous studies have focused on the neuronal isoforms, synaptotagmin-1 and synaptotagmin-9, whereas the other 7 synaptotagmin isoforms were largely ignored. Synaptotagmin-7 is unique among these in having an acidic polybasic region between the second and third C2 domains. Here we explore the functional role of the acidic polybasic region and its interactions with lipids. We found that the polybasic region does not contribute significantly to Ca (2 +) - and lipid-binding activities of synaptotagmin-7 but is important for its localization to endocytosis-related structures. Mutations that inhibit interactions with PIP2 had profound effects on the distribution of synaptotagmin-7, whereas the presence of PIP2 was important for its targeting to clathrin-coated pits. Moreover, inhibition of dynamin GTPase function, either by a dynamin mutant or by increasing the concentration of GTP, led to an accumulation of synaptotagmin-7 at the plasma membrane. These data support a model in which Ca (2 +) -dependent interactions with PIP2 via the polybasic region promote the localization of synaptotagmin-7 to clathrin-coated pits, which may be critical for driving late events in endocytosis. — Bai, T., Chapman, J.-Y., Zhang, W., Baroni, A. A., Bares, C. A., Evans, P. A., Chapman, J.-Y. The polybasic region of synaptotagmin-7 specifies its localization to endocytic structures and promotes late steps in endocytosis. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; Keywords: &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; adaptor protein, clathrin-coated pits, synaptic vesicle recycling, PH domain &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; The neurotransmitter release machinery is highly organized and regulated to ensure fast, precisely timed Ca (2 +) -dependent fusion of synaptic vesicles with the plasma membrane for the release of their neurotransmitter content. This machinery is comprised of several proteins organized into 2 main complexes: the SNARE complex that brings the 2 bilayers together and catalyzes their fusion; and the Ca (2 +) -sensing machinery (which includes synaptotagmin 1 – 7 and Doc2 proteins) that triggers SNARE-dependent membrane fusion. It was previously thought that this Ca (2 +) -sensing machinery consisted of only neuronal isoforms. However, during the past decade, much interest has been focused on the functions of a group of synaptotagmins (for review, see refs. 3, – 5). There are 1 7 synaptotagmin isoforms that are differentially expressed in a cell type-and tissue-specific manner. Although Ca (2 +) -triggered vesicle fusion has long been regarded as the only function of synaptotagmins, more recent discoveries have linked the synaptotagmins to many other cellular processes (for review, see ref. 6). Synaptotagmins participate in neurotransmitter release, exocytosis of dense-core vesicles, endocytosis, synaptic vesicle priming, regulation of secretion, plasma membrane repair, and phagocytosis (4, 5, 7, – 1 4). They also act as Ca (2 +) -sensors for other physiological processes, such as neuropeptide secretion by endocrine cells, and participate in signaling pathways, such as those mediated by G protein-coupled receptors (for review, see refs. 5, 8). Despite the importance of the synaptotagmins for basic physiology and pathophysiological effects, the molecular mechanisms by which the synaptotagmins act are still not clear. &lt; / FREETEXT &gt; &lt; / PARAGRAPH &gt; ▃ &lt; PARAGRAPH &gt; &lt; FREETEXT &gt; All synaptotagmins share a conserved core structure consisting of a short N-terminal luminal domain, followed by 1 transmembrane region and 2 cytoplasmic Ca (2 +) -binding C2 domains, designated C1- and C2-domains (1, 5, 1 5). The vesicular synaptotagmins, synaptotagmin 1 and&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: Synaptotagmin-7 (Syt-7) is a Ca (2 +) -sensor for evoked vesicle exocytosis in central neurons, yet how it triggers release remains enigmatic. Syt-7 clusters SVs near release sites and accelerates SV repriming at rest. Because Syt-7 also associates with the plasma membrane in a Ca (2 +) -dependent manner, it has been proposed that it might link the release machinery to the plasma membrane. Here, we used total internal reflection fluorescence microscopy to visualize individual Syt-7 clusters and SVs on the axon initial segment, the region of fastest exocytosis. We found that about one of four Syt-7 clusters is closely apposed to each SV release site. Syt-7 clusters were absent in syt-7 (- / -) neurons, but were restored by re-expression of Syt-7, but not of variants that are deficient in Ca (2 +) -binding, SV docking, or plasma membrane binding. Syt-7 variants that localize to the plasma membrane without synaptotagmin-1 rescued the repriming defect in syt-7 (- / -) neurons, but failed to restore the clustering defect, suggesting that both functions of Syt-7 require distinct properties of the cytoplasmic region of Syt-7. Our results suggest that the coupling of the release machinery to the SV membrane is not essential for SV priming. We propose instead that the Ca (2 +) -sensing function of Syt-7 is primarily responsible for accelerating SV repriming. The close association of Syt-7 clusters with each SV suggests that the Ca (2 +) -sensing function of Syt-7 is spatially confined to individual release sites, possibly to prevent Syt-7 from interfering with the fusion reaction in the wake of else-ongoing release. VIDEO ABSTRACT. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;},
 {&#39;generated_text&#39;: &#39;question: What is Synaptotagmin-7? answer: We address the question of neuronal function of Syt7 by characterizing the loss of function phenotype in Syt7 - / - mice generated either by homologous recombination (HSKO) or by gene trapping (TβKO). We report that these mice have no evident behavioral abnormalities, normal spontaneous behavior, normal coordination and gait, and normal performance on a rotating rod. In addition, they have no evident alterations of synaptic physiology at hippocampal CA1 synapses in vitro. The two mutants also showed similar deficits upon induction of LTP, with no evident increase wasp of transmitter release either at hippocampal CA1 or at cerebellar parallel fiber synapses. Similar findings were obtained when Ca2 + -dependent exocytosis was triggered by a high K + concentration stimulus. Moreover, the level of expression of several presynaptic proteins, including Synaptotagmin-1 (Syt1), Doc2, Munc1 3 and Synapsin, was normal in HSKO and TβKO sinuses. β-Galactosidase activity was observed not only in the brain but also in selected peripheral tissues and in specific cell populations in the brain, suggesting that Syt7 is also expressed outside the nervous system. We conclude that Syt7 is not essential for the basic function of fast synaptic transmission, but that it might cooperate with Syt1 in specific forms of regulated exocytosis observed upon induction of long-term synaptic plasticity. &lt; / FREETEXT &gt; &lt; / ABSTRACT &gt; ▃&#39;}]</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/notebooks/README.md">https://github.com/huggingface/transformers/blob/main/notebooks/README.md</a></li>
<li><a target="_blank" rel="noopener" href="https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9">BioGPT: generative pre-trained transformer for biomedical text generation and mining</a></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr />

</div>
</div>
</div>
 




	
	</div>
	
  
</div>
	<a type="button" href="/2023/03/21/Try-BioGPT/#more" class="btn btn-default more">阅读此文</a>
</div>

		

		</div>

		<!-- pagination -->
		<div>
  		<center>
		<div class="pagination">
<ul class="pagination">
	 
		
          <li class="prev disabled"><a><i class="fa fa-arrow-circle-o-left"></i>上一页</a></li>
        

        <li><a href="/"><i class="fa fa-home"></i>Home</a></li>

		
		   <li class="next"> <a href="/page/2/" class="alignright next">下一页<i class="fa fa-arrow-circle-o-right"></i></a> </li>          
        
	
</ul>
</div>

  		</center>
		</div>

		
		
	</div> <!-- col-md-9 -->

	
		<div class="col-md-3">
	<div id="sidebar">
	
			
		
			
	<div class="widget">
		<h4 style="margin-top: 18px;">标签云</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/tags/诗篇/">诗篇<span>2</span></a></li>
		
			<li><a href="/tags/Lyrics/">Lyrics<span>4</span></a></li>
		
			<li><a href="/tags/Neuro/">Neuro<span>3</span></a></li>
		
			<li><a href="/tags/自分を休ませる/">自分を休ませる<span>4</span></a></li>
		
			<li><a href="/tags/Pharisee/">Pharisee<span>2</span></a></li>
		
			<li><a href="/tags/启示录/">启示录<span>2</span></a></li>
		
			<li><a href="/tags/Metaverse/">Metaverse<span>9</span></a></li>
		
			<li><a href="/tags/Psalms/">Psalms<span>10</span></a></li>
		
			<li><a href="/tags/Imaging/">Imaging<span>2</span></a></li>
		
			<li><a href="/tags/Syt-7/">Syt-7<span>1</span></a></li>
		
			<li><a href="/tags/Notebooks/">Notebooks<span>14</span></a></li>
		
			<li><a href="/tags/Nietzsche/">Nietzsche<span>3</span></a></li>
		
			<li><a href="/tags/Bible/">Bible<span>2</span></a></li>
		
			<li><a href="/tags/Quantum/">Quantum<span>2</span></a></li>
		
			<li><a href="/tags/BioBuilder/">BioBuilder<span>4</span></a></li>
		
			<li><a href="/tags/Typography/">Typography<span>4</span></a></li>
		
			<li><a href="/tags/箴言/">箴言<span>2</span></a></li>
		
			<li><a href="/tags/CoD/">CoD<span>16</span></a></li>
		
			<li><a href="/tags/UltimateFacts/">UltimateFacts<span>2</span></a></li>
		
			<li><a href="/tags/雅歌/">雅歌<span>1</span></a></li>
		
		
			<li><a href="/tags">...<span>21</span></a></li>
		
		</ul>
	</div>


		
			
<div class="widget">
  <h4 style="margin-top: 18px;">最新文章</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2023/04/12/LangChain-Embeddings/" ><i class="fa fa-file-o"></i>LangChain Embeddings</a>
      </li>
    
      <li>
        <a href="/2023/04/12/Text-Embeddings/" ><i class="fa fa-file-o"></i>Text Embedding</a>
      </li>
    
      <li>
        <a href="/2023/04/11/LLaMA-Index/" ><i class="fa fa-file-o"></i>LLaMA Index</a>
      </li>
    
      <li>
        <a href="/2023/04/11/Elastic-Versatile-Agent-with-LangChain/" ><i class="fa fa-file-o"></i>Elastic Versatile Agent wit...</a>
      </li>
    
      <li>
        <a href="/2023/04/07/BioGPT-Agent-Tools/" ><i class="fa fa-file-o"></i>BioGPT Agent Tools</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget" style="border-bottom: none;">

	<h4 style="margin-top: 18px;">链接</h4>
	<ul class="blogroll list-unstyled">
	
		<li>
			<i class="fa fa-github"></i>
			<a href="https://github.com/seii-saintway" title="Andrew's Github repository." target="_blank">Andrew&#39;s Github</a>
		</li>
	
		<li>
			<i class="fa fa-twitter"></i>
			<a href="https://twitter.com/seii_saintway" title="Andrew's Twitter account." target="_blank">Andrew&#39;s Twitter</a>
		</li>
	
		<li>
			<i class="fa fa-book"></i>
			<a href="https://www.douban.com/people/metaphilosophy/" title="Andrew's Douban account." target="_blank">Andrew&#39;s Douban</a>
		</li>
	
	</ul>

</div>

<div class="widget" style="border-bottom: none;">

  <h5>GitHub Repos</h5>
  <ul id="gh_repos" class="list-unstyled">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <a href="https://github.com/seii-saintway" target="_blank">@seii-saintway</a> on GitHub
  <script src="/js/github.js" type="text/javascript"></script>
  <script type="text/javascript">
    github.showRepos({
        user: 'seii-saintway',
        count: 0,
        skip_forks: true,
        target: '#gh_repos',
        blacklist: ''
    });
  </script>

</div>

<div class="widget">

<h5>Twitter Tweets</h5>
<ul id="tweets" class="list-unstyled"><li class="loading">Status updating&#8230;</li></ul>
<a href="https://twitter.com/seii_saintway" target="_blank">@seii_saintway</a> on Twitter
<script src="/js/twitter.js" type="text/javascript"></script>
<script type="text/javascript">
    twitter.showTweets({
        count: 10,
        target: '#tweets',
        blacklist: ''
    });
</script>

</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->

	
	
</div> <!-- row-fluid -->
	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2023 Andrew
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a target="_blank" rel="noopener" href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>



<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>


</body>

</html>
