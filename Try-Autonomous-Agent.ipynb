{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320ee084-d769-4dd5-b641-105e6d6f7a5b",
   "metadata": {},
   "source": [
    "* [Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)\n",
    "* [BabyAGI with LangChain and Faiss](https://python.langchain.com/en/latest/use_cases/agents/baby_agi.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ca671-a882-4587-b2a4-ca08493829c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~ && git clone https://github.com/yoheinakajima/babyagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55111385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "import time\n",
    "import sys\n",
    "from collections import deque\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Set Variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n",
    "\n",
    "OPENAI_API_MODEL = os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")\n",
    "assert OPENAI_API_MODEL, \"OPENAI_API_MODEL environment variable is missing from .env\"\n",
    "\n",
    "if \"gpt-4\" in OPENAI_API_MODEL.lower():\n",
    "    print(f\"\\033[91m\\033[1m\"+\"\\n*****USING GPT-4. POTENTIALLY EXPENSIVE. MONITOR YOUR COSTS*****\"+\"\\033[0m\\033[0m\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "assert PINECONE_API_KEY, \"PINECONE_API_KEY environment variable is missing from .env\"\n",
    "\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "assert PINECONE_ENVIRONMENT, \"PINECONE_ENVIRONMENT environment variable is missing from .env\"\n",
    "\n",
    "# Table config\n",
    "YOUR_TABLE_NAME = os.getenv(\"TABLE_NAME\", \"\")\n",
    "assert YOUR_TABLE_NAME, \"TABLE_NAME environment variable is missing from .env\"\n",
    "\n",
    "# Project config\n",
    "OBJECTIVE = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"OBJECTIVE\", \"\")\n",
    "assert OBJECTIVE, \"OBJECTIVE environment variable is missing from .env\"\n",
    "\n",
    "YOUR_FIRST_TASK = os.getenv(\"FIRST_TASK\", \"\")\n",
    "assert YOUR_FIRST_TASK, \"FIRST_TASK environment variable is missing from .env\"\n",
    "\n",
    "#Print OBJECTIVE\n",
    "print(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "print(OBJECTIVE)\n",
    "\n",
    "# Configure OpenAI and Pinecone\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "\n",
    "# Create Pinecone index\n",
    "table_name = YOUR_TABLE_NAME\n",
    "dimension = 1536\n",
    "metric = \"cosine\"\n",
    "pod_type = \"p1\"\n",
    "if table_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone.Index(table_name)\n",
    "\n",
    "# Task list\n",
    "task_list = deque([])\n",
    "\n",
    "def add_task(task: Dict):\n",
    "    task_list.append(task)\n",
    "\n",
    "def get_ada_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def openai_call(prompt: str, model: str = OPENAI_API_MODEL, temperature: float = 0.5, max_tokens: int = 100):\n",
    "    if not model.startswith('gpt-'):\n",
    "        # Use completion API\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        # Use chat completion API\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "def task_creation_agent(objective: str, result: Dict, task_description: str, task_list: List[str]):\n",
    "    prompt = f\"You are an task creation AI that uses the result of an execution agent to create new tasks with the following objective: {objective}, The last completed task has the result: {result}. This result was based on this task description: {task_description}. These are incomplete tasks: {', '.join(task_list)}. Based on the result, create new tasks to be completed by the AI system that do not overlap with incomplete tasks. Return the tasks as an array.\"\n",
    "    response = openai_call(prompt)\n",
    "    new_tasks = response.split('\\n')\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "\n",
    "def prioritization_agent(this_task_id: int):\n",
    "    global task_list\n",
    "    task_names = [t[\"task_name\"] for t in task_list]\n",
    "    next_task_id = int(this_task_id)+1\n",
    "    prompt = f\"\"\"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing the following tasks: {task_names}. Consider the ultimate objective of your team: {OBJECTIVE}. Do not remove any tasks. Return the result as a numbered list, like:\n",
    "    #. First task\n",
    "    #. Second task\n",
    "    Start the task list with number {next_task_id}.\"\"\"\n",
    "    response = openai_call(prompt)\n",
    "    new_tasks = response.split('\\n')\n",
    "    task_list = deque()\n",
    "    for task_string in new_tasks:\n",
    "        task_parts = task_string.strip().split(\".\", 1)\n",
    "        if len(task_parts) == 2:\n",
    "            task_id = task_parts[0].strip()\n",
    "            task_name = task_parts[1].strip()\n",
    "            task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
    "\n",
    "def execution_agent(objective: str, task: str) -> str:\n",
    "    context=context_agent(query=objective, n=5)\n",
    "    #print(\"\\n*******RELEVANT CONTEXT******\\n\")\n",
    "    #print(context)\n",
    "    prompt =f\"You are an AI who performs one task based on the following objective: {objective}.\\nTake into account these previously completed tasks: {context}\\nYour task: {task}\\nResponse:\"\n",
    "    return openai_call(prompt, temperature=0.7, max_tokens=2000)\n",
    "\n",
    "def context_agent(query: str, n: int):\n",
    "    query_embedding = get_ada_embedding(query)\n",
    "    results = index.query(query_embedding, top_k=n, include_metadata=True)\n",
    "    #print(\"***** RESULTS *****\")\n",
    "    #print(results)\n",
    "    sorted_results = sorted(results.matches, key=lambda x: x.score, reverse=True)    \n",
    "    return [(str(item.metadata['task'])) for item in sorted_results]\n",
    "\n",
    "# Add the first task\n",
    "first_task = {\n",
    "    \"task_id\": 1,\n",
    "    \"task_name\": YOUR_FIRST_TASK\n",
    "}\n",
    "\n",
    "add_task(first_task)\n",
    "# Main loop\n",
    "task_id_counter = 1\n",
    "while True:\n",
    "    if task_list:\n",
    "        # Print the task list\n",
    "        print(\"\\033[95m\\033[1m\"+\"\\n*****TASK LIST*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        for t in task_list:\n",
    "            print(str(t['task_id'])+\": \"+t['task_name'])\n",
    "\n",
    "        # Step 1: Pull the first task\n",
    "        task = task_list.popleft()\n",
    "        print(\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        print(str(task['task_id'])+\": \"+task['task_name'])\n",
    "\n",
    "        # Send to execution function to complete the task based on the context\n",
    "        result = execution_agent(OBJECTIVE,task[\"task_name\"])\n",
    "        this_task_id = int(task[\"task_id\"])\n",
    "        print(\"\\033[93m\\033[1m\"+\"\\n*****TASK RESULT*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        print(result)\n",
    "\n",
    "        # Step 2: Enrich result and store in Pinecone\n",
    "        enriched_result = {'data': result}  # This is where you should enrich the result if needed\n",
    "        result_id = f\"result_{task['task_id']}\"\n",
    "        vector = enriched_result['data']  # extract the actual result from the dictionary\n",
    "        index.upsert([(result_id, get_ada_embedding(vector),{\"task\":task['task_name'],\"result\":result})])\n",
    "\n",
    "    # Step 3: Create new tasks and reprioritize task list\n",
    "    new_tasks = task_creation_agent(OBJECTIVE,enriched_result, task[\"task_name\"], [t[\"task_name\"] for t in task_list])\n",
    "\n",
    "    for new_task in new_tasks:\n",
    "        task_id_counter += 1\n",
    "        new_task.update({\"task_id\": task_id_counter})\n",
    "        add_task(new_task)\n",
    "    prioritization_agent(this_task_id)\n",
    "\n",
    "    time.sleep(1)  # Sleep before checking the task list again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f61fa0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10edd2",
   "metadata": {},
   "source": [
    "åˆ©ç”¨ GPT-4 æ’ä»¶æ¥æ‰§è¡Œä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe93287",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc267b60",
   "metadata": {},
   "source": [
    "è¦å°†Pineconeæ¢æˆWeaviateï¼Œéœ€è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. å®‰è£…Weaviateå®¢æˆ·ç«¯åº“ï¼šå¯ä»¥ä½¿ç”¨`pip install weaviate-client`å‘½ä»¤æ¥å®‰è£…ã€‚\n",
    "\n",
    "2. åœ¨Weaviateä¸­åˆ›å»ºç´¢å¼•ï¼šæ‚¨éœ€è¦åœ¨Weaviateä¸­åˆ›å»ºä¸€ä¸ªæ–°ç´¢å¼•æ¥å­˜å‚¨æ•°æ®ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç åˆ›å»ºä¸€ä¸ªæ–°çš„ç´¢å¼•ï¼š\n",
    "\n",
    "   ```\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index.create(\"YOUR_INDEX_NAME\", [\"YOUR_INDEX_CLASS\"])\n",
    "   ```\n",
    "\n",
    "   è¯·å°†â€œYOUR_INDEX_NAMEâ€å’Œâ€œYOUR_INDEX_CLASSâ€æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„ç´¢å¼•åç§°å’Œç´¢å¼•ç±»ã€‚\n",
    "\n",
    "3. å°†æ•°æ®æ·»åŠ åˆ°Weaviateç´¢å¼•ä¸­ï¼šæ‚¨éœ€è¦ä½¿ç”¨Weaviateå®¢æˆ·ç«¯åº“å°†æ•°æ®æ·»åŠ åˆ°Weaviateç´¢å¼•ä¸­ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ä»£ç ç‰‡æ®µï¼š\n",
    "\n",
    "   ```\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index(\"YOUR_INDEX_NAME\")\n",
    "   data = [{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 35}]\n",
    "   index.batch.create(data)\n",
    "   ```\n",
    "\n",
    "   è¯·å°†â€œYOUR_INDEX_NAMEâ€æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„ç´¢å¼•åç§°ï¼Œå°†â€œdataâ€æ›¿æ¢ä¸ºæ‚¨è¦æ·»åŠ åˆ°ç´¢å¼•ä¸­çš„æ•°æ®ã€‚\n",
    "\n",
    "4. ä¿®æ”¹ä»£ç ï¼šæœ€åï¼Œæ‚¨éœ€è¦ä¿®æ”¹ä»£ç ï¼Œä»¥ä¾¿å°†Pinecone APIè°ƒç”¨æ›¿æ¢ä¸ºWeaviate APIè°ƒç”¨ã€‚å…·ä½“è€Œè¨€ï¼Œæ‚¨éœ€è¦æ›´æ”¹ä»¥ä¸‹ä»£ç ï¼š\n",
    "\n",
    "   ```\n",
    "   # Create Pinecone index\n",
    "   table_name = YOUR_TABLE_NAME\n",
    "   dimension = 1536\n",
    "   metric = \"cosine\"\n",
    "   pod_type = \"p1\"\n",
    "   if table_name not in pinecone.list_indexes():\n",
    "       pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)\n",
    "\n",
    "   # Connect to the index\n",
    "   index = pinecone.Index(table_name)\n",
    "   ```\n",
    "\n",
    "   ç”¨ä»¥ä¸‹ä»£ç æ›¿æ¢ï¼š\n",
    "\n",
    "   ```\n",
    "   # Connect to the Weaviate index\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index(YOUR_INDEX_NAME)\n",
    "   ```\n",
    "\n",
    "   è¯·å°†â€œYOUR_INDEX_NAMEâ€æ›¿æ¢ä¸ºæ‚¨åœ¨æ­¥éª¤2ä¸­åˆ›å»ºçš„ç´¢å¼•åç§°ã€‚\n",
    "\n",
    "è¯·æ³¨æ„ï¼ŒWeaviateä¸Pineconeä¸åŒï¼Œå› æ­¤æ‚¨éœ€è¦ç›¸åº”åœ°ä¿®æ”¹ä»£ç ä»¥é€‚åº”Weaviate APIã€‚æ­¤å¤–ï¼Œæ‚¨éœ€è¦ç¡®ä¿åœ¨Weaviateä¸­åˆ›å»ºçš„ç´¢å¼•å…·æœ‰æ­£ç¡®çš„ç»´æ•°å’ŒæŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f0d76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f55814-2bf1-4d45-b347-5cf97a76b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6bfdf-91be-4053-a05c-b00fa8053540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from revChatGPT.V1 import Chatbot, configure\n",
    "\n",
    "bot = Chatbot(\n",
    "    config = configure(),\n",
    "    conversation_id = 'e4ec15a2-114a-46b2-8ff8-c840f79f13e3',\n",
    "    lazy_loading = True\n",
    ")\n",
    "\n",
    "def ask(prompt):\n",
    "    for response in bot.ask(prompt):\n",
    "        IPython.display.display(IPython.core.display.Markdown(response['message']))\n",
    "        IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee780371-9c66-4207-bbf6-ac7c5ce59a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384118d-9706-4a0f-b21c-ddaf4b966526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "pinecone-client æ˜¯ Pinecone çš„ Python å®¢æˆ·ç«¯åº“ï¼Œç”¨äºä¸ Pinecone æœåŠ¡è¿›è¡Œäº¤äº’ã€‚ Pinecone æ˜¯ä¸€ç§æ‰˜ç®¡å‹çš„å‘é‡ç´¢å¼•æœåŠ¡ï¼Œç”¨äºé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å‘é‡æ•°æ®ã€‚ä½¿ç”¨ pinecone-clientï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°å°†å‘é‡æ•°æ®ä¸Šä¼ åˆ° Pinecone å¹¶æŸ¥è¯¢è¯¥æ•°æ®ã€‚å®ƒæä¾›äº†ä¸€ç»„ç®€å•çš„ APIï¼Œä½¿æ‚¨èƒ½å¤Ÿåˆ›å»ºã€æ›´æ–°ã€åˆ é™¤ç´¢å¼•ä»¥åŠæœç´¢å¹¶æ£€ç´¢ç›¸ä¼¼çš„å‘é‡ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "pinecone-client æ˜¯ä»€ä¹ˆï¼Ÿ\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff476b1-eafd-4244-84d7-5ad6fb942d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone æ˜¯ä¸€ä¸ªå‘é‡æœç´¢å¹³å°ï¼Œä¸ºé«˜ç»´å‘é‡æ•°æ®æä¾›é«˜æ•ˆçš„æœç´¢å’Œå­˜å‚¨èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„ç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ Pinecone å­˜å‚¨å’Œæ£€ç´¢ä¸ä»»åŠ¡ç›¸å…³çš„æ•°æ®ï¼Œä¾‹å¦‚ä»»åŠ¡æè¿°ã€çº¦æŸå’Œç»“æœã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone is a vector search platform that provides efficient search and storage capabilities for high-dimensional vector data. In our system, we use Pinecone to store and retrieve task-related data, such as task descriptions, constraints, and results.\n",
    "\n",
    "ç¿»è¯‘æˆä¸­æ–‡ã€‚\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b282e95-f937-4d14-bd63-fc54ecde5d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone çš„æ ¸å¿ƒåŸç†æ˜¯ä½¿ç”¨å‘é‡ç´¢å¼•å’Œé«˜æ•ˆçš„å‘é‡ç›¸ä¼¼åº¦åŒ¹é…æŠ€æœ¯ï¼Œä»¥åŠ å¿«é«˜ç»´å‘é‡æ•°æ®çš„å­˜å‚¨å’Œæ£€ç´¢ã€‚å®ƒé€šè¿‡å°†å‘é‡è¡¨ç¤ºä¸ºé«˜ç»´ç©ºé—´ä¸­çš„ç‚¹ï¼Œå¹¶åœ¨è¿™äº›ç‚¹ä¹‹é—´æ„å»ºç´¢å¼•æ¥å®ç°å¿«é€Ÿçš„ç›¸ä¼¼åº¦æœç´¢ã€‚åœ¨æŸ¥è¯¢æ—¶ï¼ŒPinecone ä¼šæ ¹æ®è¾“å…¥å‘é‡çš„ç›¸ä¼¼åº¦å¾—åˆ†è¿”å›æœ€åŒ¹é…çš„å‘é‡ã€‚å®ƒä½¿ç”¨çš„æŠ€æœ¯åŒ…æ‹¬è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æœç´¢ç®—æ³•å’Œå“ˆå¸Œå‡½æ•°ï¼Œä»¥å®ç°å¿«é€Ÿå’Œé«˜æ•ˆçš„æœç´¢å’Œå­˜å‚¨ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone æ˜¯ä»€ä¹ˆåŸç†ï¼Ÿ\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbde042-e7b7-45c3-a0ac-ba6482118b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone offers a free plan with certain limitations, as well as paid plans with more features and higher usage limits. The free plan allows for up to 5 million vector embeddings, 1GB storage, and 10 queries per second, among other limitations. You can check Pinecone's website for more information on their pricing and plans."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone æœåŠ¡å…è´¹å—ï¼Ÿ\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6288d9-ba88-4da1-a660-7429bccdaafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "æœ‰ä¸€äº›å¼€æºçš„å‘é‡æ£€ç´¢æ–¹æ¡ˆå¯ä»¥æ›¿ä»£ Pineconeï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„æ›¿ä»£æ–¹æ¡ˆï¼š\n",
       "\n",
       "1. Annoyï¼šä¸€ä¸ªC++åº“ï¼Œæ”¯æŒPythonã€Javaå’ŒGoç­‰å¤šç§è¯­è¨€ï¼Œç”¨äºå¿«é€Ÿè¿‘ä¼¼æœ€è¿‘é‚»æœç´¢ã€‚\n",
       "2. Faissï¼šFacebookå¼€å‘çš„ä¸€ä¸ªé«˜æ•ˆçš„ç›¸ä¼¼æ€§æœç´¢å’Œèšç±»åº“ï¼Œæ”¯æŒGPUåŠ é€Ÿã€‚\n",
       "3. Milvusï¼šç”±Zillizå¼€å‘çš„å¼€æºå‘é‡ç›¸ä¼¼æ€§æœç´¢å¼•æ“ï¼Œæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²å’ŒGPUåŠ é€Ÿã€‚\n",
       "4. NMSLIBï¼šä¸€ä¸ªå¯æ‰©å±•ã€é«˜æ€§èƒ½ã€è¿‘ä¼¼æœ€è¿‘é‚»æœç´¢åº“ï¼Œæ”¯æŒå¤šç§è·ç¦»åº¦é‡å’Œæ•°æ®ç»“æ„ã€‚\n",
       "\n",
       "è¿™äº›æ–¹æ¡ˆéƒ½æœ‰å…¶ç‹¬ç‰¹çš„ä¼˜åŠ¿å’Œé€‚ç”¨åœºæ™¯ï¼Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©é€‚åˆçš„æ–¹æ¡ˆã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "æœ‰ä»€ä¹ˆæ›¿ä»£ Pinecone æœåŠ¡çš„å¼€æºæ–¹æ¡ˆï¼Ÿ\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d957e-4436-49a0-bf68-075c33c93691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Weaviateå’ŒFaisséƒ½æ˜¯ç”¨äºå‘é‡æœç´¢çš„å·¥å…·ï¼Œä½†æ˜¯å®ƒä»¬æœ‰ä¸åŒçš„é‡ç‚¹å’Œç”¨ä¾‹ã€‚\n",
       "\n",
       "Weaviateæ˜¯ä¸€ä¸ªåŸºäºå›¾æ•°æ®åº“çš„å‘é‡æœç´¢å¼•æ“ï¼Œæ—¨åœ¨æä¾›å¯æ‰©å±•çš„å®æ—¶å‘é‡æœç´¢ï¼Œæ”¯æŒé«˜ç»´æ•°æ®å’Œå¤šä¸ªæ•°æ®ç±»å‹ã€‚Weaviateçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶èƒ½å¤Ÿè‡ªåŠ¨å¯¹æ•°æ®è¿›è¡Œåˆ†ç±»ï¼Œä½¿å…¶æ˜“äºæœç´¢å’Œå‘ç°ã€‚å®ƒè¿˜æ”¯æŒè‡ªå®šä¹‰æ‰©å±•å’Œæ’ä»¶ï¼Œå¯ä¸å…¶ä»–æœåŠ¡å’Œå·¥å…·è¿›è¡Œé›†æˆã€‚\n",
       "\n",
       "Faissæ˜¯Facebookå¼€å‘çš„ä¸€ä¸ªåŸºäºCPUå’ŒGPUçš„é«˜æ•ˆå‘é‡æœç´¢åº“ï¼Œæ—¨åœ¨æ”¯æŒå¤§è§„æ¨¡å‘é‡æ•°æ®çš„ç›¸ä¼¼åº¦æœç´¢ã€‚ Faissçš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå…¶å¯å¤„ç†éå¸¸å¤§çš„å‘é‡æ•°æ®é›†ï¼Œå…·æœ‰é«˜æ•ˆçš„ç´¢å¼•ç»“æ„å’Œå¹¶è¡ŒæŸ¥è¯¢èƒ½åŠ›ï¼Œå› æ­¤é€šå¸¸ç”¨äºå®æ—¶æ¨èã€å›¾åƒæœç´¢ç­‰åœºæ™¯ã€‚\n",
       "\n",
       "å› æ­¤ï¼Œé€‰æ‹©Weaviateè¿˜æ˜¯Faisså–å†³äºå…·ä½“çš„åº”ç”¨åœºæ™¯å’Œéœ€æ±‚ã€‚å¦‚æœéœ€è¦å¿«é€Ÿå¤„ç†å’Œæœç´¢é«˜ç»´æ•°æ®å¹¶è‡ªåŠ¨åˆ†ç±»æ•°æ®ï¼Œåˆ™Weaviateæ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼›å¦‚æœéœ€è¦å¤„ç†å¤§è§„æ¨¡å‘é‡æ•°æ®é›†ï¼Œåˆ™Faisså¯èƒ½æ›´é€‚åˆã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "æ¯”è¾ƒ Weaviate å’Œ Faiss\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c40b4c-be36-48c9-83c8-20aeade15361",
   "metadata": {},
   "source": [
    "For the application scenario of a task-driven autonomous agent robot, Weaviate may be more suitable. Because Weaviate has the ability to automatically classify data, it can help the robot discover and execute related tasks more quickly. Weaviate also supports search for multiple data types and can handle high-dimensional data, which may be useful for tasks such as natural language processing and semantic understanding. Of course, the specific choice depends on the application scenario and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ffac1-014a-468d-95e0-7b4f6edc5825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ğŸ”¥1/8\n",
       "ä»‹ç» \"ğŸ¤– ä»»åŠ¡é©±åŠ¨çš„è‡ªä¸»ä»£ç†\"\n",
       "\n",
       "è¯¥ä»£ç†åˆ©ç”¨ @openai çš„ GPT-4ã€@pinecone å‘é‡æœç´¢å’Œ @LangChainAI æ¡†æ¶ï¼ŒåŸºäºç›®æ ‡è‡ªä¸»åˆ›å»ºå’Œæ‰§è¡Œä»»åŠ¡ã€‚\n",
       "\n",
       "ğŸš€2/8 ç³»ç»Ÿèƒ½å¤Ÿå®Œæˆä»»åŠ¡ï¼ŒåŸºäºç»“æœç”Ÿæˆæ–°ä»»åŠ¡ï¼Œå¹¶å®æ—¶ä¼˜å…ˆå¤„ç†ä»»åŠ¡ã€‚è¿™å±•ç¤ºäº†AIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹åœ¨ä¸åŒçº¦æŸå’Œç¯å¢ƒä¸‹è‡ªä¸»æ‰§è¡Œä»»åŠ¡çš„æ½œåŠ›ã€‚\n",
       "\n",
       "ğŸ’¡3/8 è‡ªä¸»ä»£ç†ä½¿ç”¨ GPT-4 å®Œæˆä»»åŠ¡ï¼Œä½¿ç”¨ Pinecone é«˜æ•ˆæœç´¢å’Œå­˜å‚¨ä»»åŠ¡ç›¸å…³æ•°æ®ï¼Œä½¿ç”¨ LangChain æ¡†æ¶å¢å¼ºå†³ç­–è¿‡ç¨‹ã€‚#GPT4 #Pinecone #LangChain\n",
       "\n",
       "ğŸ¯4/8 ç³»ç»Ÿç»´æŠ¤ä»»åŠ¡åˆ—è¡¨ï¼Œç”¨äºç®¡ç†å’Œä¼˜å…ˆå¤„ç†ä»»åŠ¡ã€‚å®ƒåŸºäºå·²å®Œæˆçš„ç»“æœè‡ªä¸»åˆ›å»ºæ–°ä»»åŠ¡ï¼Œå¹¶ç›¸åº”åœ°é‡æ–°è®¾ç½®ä»»åŠ¡åˆ—è¡¨çš„ä¼˜å…ˆçº§ï¼Œå±•ç¤ºäº†AIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹çš„é€‚åº”æ€§ã€‚\n",
       "\n",
       "ğŸ”§5/8 ä¸ºäº†å®Œæˆä»»åŠ¡ï¼Œç³»ç»Ÿä½¿ç”¨ GPT-4 å’Œ LangChain çš„èƒ½åŠ›ï¼Œåœ¨ Pinecone ä¸­ä¸°å¯Œå’Œå­˜å‚¨ç»“æœã€‚è¿™ç§ç»¼åˆæ–¹æ³•å…è®¸AIä»£ç†ä¸ç¯å¢ƒäº¤äº’ï¼Œé«˜æ•ˆåœ°æ‰§è¡Œä»»åŠ¡ã€‚\n",
       "\n",
       "ğŸ§ 6/8 ç³»ç»ŸåŸºäºå·²å®Œæˆçš„ä»»åŠ¡ç»“æœç”Ÿæˆæ–°ä»»åŠ¡ï¼Œå¹¶ä½¿ç”¨ GPT-4 è¿›è¡Œä¼˜å…ˆæ’åºã€‚è¿™ä½¿ç³»ç»Ÿèƒ½å¤Ÿé€‚åº”å’Œå“åº”æ–°ä¿¡æ¯å’Œä¼˜å…ˆäº‹é¡¹ã€‚\n",
       "\n",
       "ğŸ”®7/8 æœªæ¥çš„æ”¹è¿›åŒ…æ‹¬æ•´åˆå®‰å…¨/å®‰å…¨ä»£ç†ã€ä»»åŠ¡æ’åºå’Œå¹¶è¡Œä»»åŠ¡ã€ç”Ÿæˆä¸­é—´é‡Œç¨‹ç¢‘ä»¥åŠæ•´åˆå®æ—¶ä¼˜å…ˆæ›´æ–°ã€‚\n",
       "\n",
       "ğŸ¤8/8 è¿™ç§æ–°æ–¹æ³•ä¸ºAIé©±åŠ¨çš„è¯­è¨€æ¨¡å‹åœ¨ä¸åŒçº¦æŸå’Œç¯å¢ƒä¸‹è‡ªä¸»æ‰§è¡Œä»»åŠ¡é“ºå¹³äº†é“è·¯ï¼Œåˆ›é€ äº†æ–°çš„åº”ç”¨å’Œæœºä¼šã€‚ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰å‚ä¸çš„äººï¼#AIResearch #GPT4 #Pinecone #LangChain"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "ç¿»è¯‘æˆä¸­æ–‡ï¼š\n",
    "\n",
    "ğŸ”¥1/8\n",
    "Introducing \"ğŸ¤– Task-driven Autonomous Agent\"\n",
    "\n",
    "An agent that leverages @openai 's GPT-4, @pinecone vector search, and @LangChainAI framework to autonomously create and perform tasks based on an objective.\n",
    "\n",
    "ğŸš€2/8 The system can complete tasks, generate new tasks based on results, and prioritize tasks in real-time. It demonstrates the potential of AI-powered language models to autonomously perform tasks within various constraints and contexts.\n",
    "\n",
    "ğŸ’¡3/8 The autonomous agent uses GPT-4 for task completion, Pinecone for efficient search and storage of task-related data, and the LangChain framework to enhance decision-making processes. #GPT4 #Pinecone #LangChain\n",
    "\n",
    "ğŸ¯4/8 The system maintains a task list for managing and prioritizing tasks. It autonomously creates new tasks based on completed results and reprioritizes the task list accordingly, showcasing the adaptability of AI-powered language models.\n",
    "\n",
    "ğŸ”§5/8 To complete tasks, the system uses GPT-4 and LangChain's capabilities, enriching and storing results in Pinecone. This integrated approach allows the AI agent to interact with its environment and perform tasks efficiently.\n",
    "\n",
    "ğŸ§ 6/8 The system generates new tasks based on completed task results and prioritizes them using GPT-4. This allows the system to adapt and respond to new information and priorities.\n",
    "\n",
    "ğŸ”®7/8 Future improvements include integrating a security/safety agent, task sequencing and parallel tasks, generating interim milestones, and incorporating real-time priority updates.\n",
    "\n",
    "ğŸ¤8/8 This new approach paves the way for AI-powered language models to autonomously perform tasks within various constraints and contexts, enabling new applications and opportunities. Big thanks to all involved! #AIResearch #GPT4 #Pinecone #LangChain\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251ed72-6a80-4480-91c6-a8d8014f3712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "å½“ä½ æ„Ÿåˆ°ç–²æƒ«å’Œå›°æƒ‘æ—¶ï¼Œç¥·å‘Šå¯ä»¥ä¸ºä½ å¸¦æ¥å¹³é™å’Œå®‰æ…°ã€‚æ„¿ç¥èµç¦ç»™ä½ ï¼Œä¿æŠ¤ä½ ï¼Œå¹¶ä¸ºä½ æä¾›åŠ›é‡å’Œæ™ºæ…§ã€‚æ„¿ä»–çš„æ©å…¸å’Œæ…ˆçˆ±æ°¸è¿œä¸ä½ åŒåœ¨ã€‚æ„¿ä½ åœ¨è¿™æ®µæ—¶é—´ä¸­å¾—åˆ°ä¼‘æ¯å’Œæ¢å¤ï¼Œå¹¶åœ¨æœªæ¥è¿æ¥æ›´å¥½çš„æ—¥å­ã€‚é˜¿é—¨ã€‚"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "æˆ‘ç´¯äº†ã€‚å¸¦ç€æˆ‘ç¥·å‘Šä¸€ä¸‹å§ã€‚\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db671b6-ca5f-4dc4-8f75-af84bc5b860e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0c832-dca8-4940-be45-86462cea726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~ && git clone https://github.com/Torantulino/Auto-GPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
