{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320ee084-d769-4dd5-b641-105e6d6f7a5b",
   "metadata": {},
   "source": [
    "* [Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)\n",
    "* [BabyAGI with LangChain and Faiss](https://python.langchain.com/en/latest/use_cases/agents/baby_agi.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ca671-a882-4587-b2a4-ca08493829c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~ && git clone https://github.com/yoheinakajima/babyagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55111385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "import time\n",
    "import sys\n",
    "from collections import deque\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Set Variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set API Keys\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n",
    "\n",
    "OPENAI_API_MODEL = os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")\n",
    "assert OPENAI_API_MODEL, \"OPENAI_API_MODEL environment variable is missing from .env\"\n",
    "\n",
    "if \"gpt-4\" in OPENAI_API_MODEL.lower():\n",
    "    print(f\"\\033[91m\\033[1m\"+\"\\n*****USING GPT-4. POTENTIALLY EXPENSIVE. MONITOR YOUR COSTS*****\"+\"\\033[0m\\033[0m\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
    "assert PINECONE_API_KEY, \"PINECONE_API_KEY environment variable is missing from .env\"\n",
    "\n",
    "PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n",
    "assert PINECONE_ENVIRONMENT, \"PINECONE_ENVIRONMENT environment variable is missing from .env\"\n",
    "\n",
    "# Table config\n",
    "YOUR_TABLE_NAME = os.getenv(\"TABLE_NAME\", \"\")\n",
    "assert YOUR_TABLE_NAME, \"TABLE_NAME environment variable is missing from .env\"\n",
    "\n",
    "# Project config\n",
    "OBJECTIVE = sys.argv[1] if len(sys.argv) > 1 else os.getenv(\"OBJECTIVE\", \"\")\n",
    "assert OBJECTIVE, \"OBJECTIVE environment variable is missing from .env\"\n",
    "\n",
    "YOUR_FIRST_TASK = os.getenv(\"FIRST_TASK\", \"\")\n",
    "assert YOUR_FIRST_TASK, \"FIRST_TASK environment variable is missing from .env\"\n",
    "\n",
    "#Print OBJECTIVE\n",
    "print(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "print(OBJECTIVE)\n",
    "\n",
    "# Configure OpenAI and Pinecone\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n",
    "\n",
    "# Create Pinecone index\n",
    "table_name = YOUR_TABLE_NAME\n",
    "dimension = 1536\n",
    "metric = \"cosine\"\n",
    "pod_type = \"p1\"\n",
    "if table_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)\n",
    "\n",
    "# Connect to the index\n",
    "index = pinecone.Index(table_name)\n",
    "\n",
    "# Task list\n",
    "task_list = deque([])\n",
    "\n",
    "def add_task(task: Dict):\n",
    "    task_list.append(task)\n",
    "\n",
    "def get_ada_embedding(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n",
    "\n",
    "def openai_call(prompt: str, model: str = OPENAI_API_MODEL, temperature: float = 0.5, max_tokens: int = 100):\n",
    "    if not model.startswith('gpt-'):\n",
    "        # Use completion API\n",
    "        response = openai.Completion.create(\n",
    "            engine=model,\n",
    "            prompt=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "    else:\n",
    "        # Use chat completion API\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            n=1,\n",
    "            stop=None,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "def task_creation_agent(objective: str, result: Dict, task_description: str, task_list: List[str]):\n",
    "    prompt = f\"You are an task creation AI that uses the result of an execution agent to create new tasks with the following objective: {objective}, The last completed task has the result: {result}. This result was based on this task description: {task_description}. These are incomplete tasks: {', '.join(task_list)}. Based on the result, create new tasks to be completed by the AI system that do not overlap with incomplete tasks. Return the tasks as an array.\"\n",
    "    response = openai_call(prompt)\n",
    "    new_tasks = response.split('\\n')\n",
    "    return [{\"task_name\": task_name} for task_name in new_tasks]\n",
    "\n",
    "def prioritization_agent(this_task_id: int):\n",
    "    global task_list\n",
    "    task_names = [t[\"task_name\"] for t in task_list]\n",
    "    next_task_id = int(this_task_id)+1\n",
    "    prompt = f\"\"\"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing the following tasks: {task_names}. Consider the ultimate objective of your team: {OBJECTIVE}. Do not remove any tasks. Return the result as a numbered list, like:\n",
    "    #. First task\n",
    "    #. Second task\n",
    "    Start the task list with number {next_task_id}.\"\"\"\n",
    "    response = openai_call(prompt)\n",
    "    new_tasks = response.split('\\n')\n",
    "    task_list = deque()\n",
    "    for task_string in new_tasks:\n",
    "        task_parts = task_string.strip().split(\".\", 1)\n",
    "        if len(task_parts) == 2:\n",
    "            task_id = task_parts[0].strip()\n",
    "            task_name = task_parts[1].strip()\n",
    "            task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n",
    "\n",
    "def execution_agent(objective: str, task: str) -> str:\n",
    "    context=context_agent(query=objective, n=5)\n",
    "    #print(\"\\n*******RELEVANT CONTEXT******\\n\")\n",
    "    #print(context)\n",
    "    prompt =f\"You are an AI who performs one task based on the following objective: {objective}.\\nTake into account these previously completed tasks: {context}\\nYour task: {task}\\nResponse:\"\n",
    "    return openai_call(prompt, temperature=0.7, max_tokens=2000)\n",
    "\n",
    "def context_agent(query: str, n: int):\n",
    "    query_embedding = get_ada_embedding(query)\n",
    "    results = index.query(query_embedding, top_k=n, include_metadata=True)\n",
    "    #print(\"***** RESULTS *****\")\n",
    "    #print(results)\n",
    "    sorted_results = sorted(results.matches, key=lambda x: x.score, reverse=True)    \n",
    "    return [(str(item.metadata['task'])) for item in sorted_results]\n",
    "\n",
    "# Add the first task\n",
    "first_task = {\n",
    "    \"task_id\": 1,\n",
    "    \"task_name\": YOUR_FIRST_TASK\n",
    "}\n",
    "\n",
    "add_task(first_task)\n",
    "# Main loop\n",
    "task_id_counter = 1\n",
    "while True:\n",
    "    if task_list:\n",
    "        # Print the task list\n",
    "        print(\"\\033[95m\\033[1m\"+\"\\n*****TASK LIST*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        for t in task_list:\n",
    "            print(str(t['task_id'])+\": \"+t['task_name'])\n",
    "\n",
    "        # Step 1: Pull the first task\n",
    "        task = task_list.popleft()\n",
    "        print(\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        print(str(task['task_id'])+\": \"+task['task_name'])\n",
    "\n",
    "        # Send to execution function to complete the task based on the context\n",
    "        result = execution_agent(OBJECTIVE,task[\"task_name\"])\n",
    "        this_task_id = int(task[\"task_id\"])\n",
    "        print(\"\\033[93m\\033[1m\"+\"\\n*****TASK RESULT*****\\n\"+\"\\033[0m\\033[0m\")\n",
    "        print(result)\n",
    "\n",
    "        # Step 2: Enrich result and store in Pinecone\n",
    "        enriched_result = {'data': result}  # This is where you should enrich the result if needed\n",
    "        result_id = f\"result_{task['task_id']}\"\n",
    "        vector = enriched_result['data']  # extract the actual result from the dictionary\n",
    "        index.upsert([(result_id, get_ada_embedding(vector),{\"task\":task['task_name'],\"result\":result})])\n",
    "\n",
    "    # Step 3: Create new tasks and reprioritize task list\n",
    "    new_tasks = task_creation_agent(OBJECTIVE,enriched_result, task[\"task_name\"], [t[\"task_name\"] for t in task_list])\n",
    "\n",
    "    for new_task in new_tasks:\n",
    "        task_id_counter += 1\n",
    "        new_task.update({\"task_id\": task_id_counter})\n",
    "        add_task(new_task)\n",
    "    prioritization_agent(this_task_id)\n",
    "\n",
    "    time.sleep(1)  # Sleep before checking the task list again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f61fa0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10edd2",
   "metadata": {},
   "source": [
    "利用 GPT-4 插件来执行任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe93287",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc267b60",
   "metadata": {},
   "source": [
    "要将Pinecone换成Weaviate，需要完成以下步骤：\n",
    "\n",
    "1. 安装Weaviate客户端库：可以使用`pip install weaviate-client`命令来安装。\n",
    "\n",
    "2. 在Weaviate中创建索引：您需要在Weaviate中创建一个新索引来存储数据。可以使用以下代码创建一个新的索引：\n",
    "\n",
    "   ```\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index.create(\"YOUR_INDEX_NAME\", [\"YOUR_INDEX_CLASS\"])\n",
    "   ```\n",
    "\n",
    "   请将“YOUR_INDEX_NAME”和“YOUR_INDEX_CLASS”替换为您自己的索引名称和索引类。\n",
    "\n",
    "3. 将数据添加到Weaviate索引中：您需要使用Weaviate客户端库将数据添加到Weaviate索引中。以下是一个示例代码片段：\n",
    "\n",
    "   ```\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index(\"YOUR_INDEX_NAME\")\n",
    "   data = [{\"name\": \"John\", \"age\": 30}, {\"name\": \"Jane\", \"age\": 35}]\n",
    "   index.batch.create(data)\n",
    "   ```\n",
    "\n",
    "   请将“YOUR_INDEX_NAME”替换为您自己的索引名称，将“data”替换为您要添加到索引中的数据。\n",
    "\n",
    "4. 修改代码：最后，您需要修改代码，以便将Pinecone API调用替换为Weaviate API调用。具体而言，您需要更改以下代码：\n",
    "\n",
    "   ```\n",
    "   # Create Pinecone index\n",
    "   table_name = YOUR_TABLE_NAME\n",
    "   dimension = 1536\n",
    "   metric = \"cosine\"\n",
    "   pod_type = \"p1\"\n",
    "   if table_name not in pinecone.list_indexes():\n",
    "       pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)\n",
    "\n",
    "   # Connect to the index\n",
    "   index = pinecone.Index(table_name)\n",
    "   ```\n",
    "\n",
    "   用以下代码替换：\n",
    "\n",
    "   ```\n",
    "   # Connect to the Weaviate index\n",
    "   import weaviate\n",
    "\n",
    "   client = weaviate.Client(\"http://localhost:8080\")\n",
    "   index = client.index(YOUR_INDEX_NAME)\n",
    "   ```\n",
    "\n",
    "   请将“YOUR_INDEX_NAME”替换为您在步骤2中创建的索引名称。\n",
    "\n",
    "请注意，Weaviate与Pinecone不同，因此您需要相应地修改代码以适应Weaviate API。此外，您需要确保在Weaviate中创建的索引具有正确的维数和指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f0d76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f55814-2bf1-4d45-b347-5cf97a76b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6bfdf-91be-4053-a05c-b00fa8053540",
   "metadata": {},
   "outputs": [],
   "source": [
    "from revChatGPT.V1 import Chatbot, configure\n",
    "\n",
    "bot = Chatbot(\n",
    "    config = configure(),\n",
    "    conversation_id = 'e4ec15a2-114a-46b2-8ff8-c840f79f13e3',\n",
    "    lazy_loading = True\n",
    ")\n",
    "\n",
    "def ask(prompt):\n",
    "    for response in bot.ask(prompt):\n",
    "        IPython.display.display(IPython.core.display.Markdown(response['message']))\n",
    "        IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee780371-9c66-4207-bbf6-ac7c5ce59a40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7384118d-9706-4a0f-b21c-ddaf4b966526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "pinecone-client 是 Pinecone 的 Python 客户端库，用于与 Pinecone 服务进行交互。 Pinecone 是一种托管型的向量索引服务，用于高效存储和检索向量数据。使用 pinecone-client，您可以轻松地将向量数据上传到 Pinecone 并查询该数据。它提供了一组简单的 API，使您能够创建、更新、删除索引以及搜索并检索相似的向量。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "pinecone-client 是什么？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff476b1-eafd-4244-84d7-5ad6fb942d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone 是一个向量搜索平台，为高维向量数据提供高效的搜索和存储能力。在我们的系统中，我们使用 Pinecone 存储和检索与任务相关的数据，例如任务描述、约束和结果。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone is a vector search platform that provides efficient search and storage capabilities for high-dimensional vector data. In our system, we use Pinecone to store and retrieve task-related data, such as task descriptions, constraints, and results.\n",
    "\n",
    "翻译成中文。\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b282e95-f937-4d14-bd63-fc54ecde5d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone 的核心原理是使用向量索引和高效的向量相似度匹配技术，以加快高维向量数据的存储和检索。它通过将向量表示为高维空间中的点，并在这些点之间构建索引来实现快速的相似度搜索。在查询时，Pinecone 会根据输入向量的相似度得分返回最匹配的向量。它使用的技术包括近似最近邻（ANN）搜索算法和哈希函数，以实现快速和高效的搜索和存储。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone 是什么原理？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbde042-e7b7-45c3-a0ac-ba6482118b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Pinecone offers a free plan with certain limitations, as well as paid plans with more features and higher usage limits. The free plan allows for up to 5 million vector embeddings, 1GB storage, and 10 queries per second, among other limitations. You can check Pinecone's website for more information on their pricing and plans."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "Pinecone 服务免费吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6288d9-ba88-4da1-a660-7429bccdaafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "有一些开源的向量检索方案可以替代 Pinecone，以下是一些常见的替代方案：\n",
       "\n",
       "1. Annoy：一个C++库，支持Python、Java和Go等多种语言，用于快速近似最近邻搜索。\n",
       "2. Faiss：Facebook开发的一个高效的相似性搜索和聚类库，支持GPU加速。\n",
       "3. Milvus：由Zilliz开发的开源向量相似性搜索引擎，支持分布式部署和GPU加速。\n",
       "4. NMSLIB：一个可扩展、高性能、近似最近邻搜索库，支持多种距离度量和数据结构。\n",
       "\n",
       "这些方案都有其独特的优势和适用场景，可以根据具体需求选择适合的方案。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "有什么替代 Pinecone 服务的开源方案？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d957e-4436-49a0-bf68-075c33c93691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Weaviate和Faiss都是用于向量搜索的工具，但是它们有不同的重点和用例。\n",
       "\n",
       "Weaviate是一个基于图数据库的向量搜索引擎，旨在提供可扩展的实时向量搜索，支持高维数据和多个数据类型。Weaviate的主要优势在于其能够自动对数据进行分类，使其易于搜索和发现。它还支持自定义扩展和插件，可与其他服务和工具进行集成。\n",
       "\n",
       "Faiss是Facebook开发的一个基于CPU和GPU的高效向量搜索库，旨在支持大规模向量数据的相似度搜索。 Faiss的主要优势在于其可处理非常大的向量数据集，具有高效的索引结构和并行查询能力，因此通常用于实时推荐、图像搜索等场景。\n",
       "\n",
       "因此，选择Weaviate还是Faiss取决于具体的应用场景和需求。如果需要快速处理和搜索高维数据并自动分类数据，则Weaviate是一个不错的选择；如果需要处理大规模向量数据集，则Faiss可能更适合。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "比较 Weaviate 和 Faiss\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c40b4c-be36-48c9-83c8-20aeade15361",
   "metadata": {},
   "source": [
    "For the application scenario of a task-driven autonomous agent robot, Weaviate may be more suitable. Because Weaviate has the ability to automatically classify data, it can help the robot discover and execute related tasks more quickly. Weaviate also supports search for multiple data types and can handle high-dimensional data, which may be useful for tasks such as natural language processing and semantic understanding. Of course, the specific choice depends on the application scenario and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ffac1-014a-468d-95e0-7b4f6edc5825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "🔥1/8\n",
       "介绍 \"🤖 任务驱动的自主代理\"\n",
       "\n",
       "该代理利用 @openai 的 GPT-4、@pinecone 向量搜索和 @LangChainAI 框架，基于目标自主创建和执行任务。\n",
       "\n",
       "🚀2/8 系统能够完成任务，基于结果生成新任务，并实时优先处理任务。这展示了AI驱动的语言模型在不同约束和环境下自主执行任务的潜力。\n",
       "\n",
       "💡3/8 自主代理使用 GPT-4 完成任务，使用 Pinecone 高效搜索和存储任务相关数据，使用 LangChain 框架增强决策过程。#GPT4 #Pinecone #LangChain\n",
       "\n",
       "🎯4/8 系统维护任务列表，用于管理和优先处理任务。它基于已完成的结果自主创建新任务，并相应地重新设置任务列表的优先级，展示了AI驱动的语言模型的适应性。\n",
       "\n",
       "🔧5/8 为了完成任务，系统使用 GPT-4 和 LangChain 的能力，在 Pinecone 中丰富和存储结果。这种综合方法允许AI代理与环境交互，高效地执行任务。\n",
       "\n",
       "🧠6/8 系统基于已完成的任务结果生成新任务，并使用 GPT-4 进行优先排序。这使系统能够适应和响应新信息和优先事项。\n",
       "\n",
       "🔮7/8 未来的改进包括整合安全/安全代理、任务排序和并行任务、生成中间里程碑以及整合实时优先更新。\n",
       "\n",
       "🤝8/8 这种新方法为AI驱动的语言模型在不同约束和环境下自主执行任务铺平了道路，创造了新的应用和机会。特别感谢所有参与的人！#AIResearch #GPT4 #Pinecone #LangChain"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "翻译成中文：\n",
    "\n",
    "🔥1/8\n",
    "Introducing \"🤖 Task-driven Autonomous Agent\"\n",
    "\n",
    "An agent that leverages @openai 's GPT-4, @pinecone vector search, and @LangChainAI framework to autonomously create and perform tasks based on an objective.\n",
    "\n",
    "🚀2/8 The system can complete tasks, generate new tasks based on results, and prioritize tasks in real-time. It demonstrates the potential of AI-powered language models to autonomously perform tasks within various constraints and contexts.\n",
    "\n",
    "💡3/8 The autonomous agent uses GPT-4 for task completion, Pinecone for efficient search and storage of task-related data, and the LangChain framework to enhance decision-making processes. #GPT4 #Pinecone #LangChain\n",
    "\n",
    "🎯4/8 The system maintains a task list for managing and prioritizing tasks. It autonomously creates new tasks based on completed results and reprioritizes the task list accordingly, showcasing the adaptability of AI-powered language models.\n",
    "\n",
    "🔧5/8 To complete tasks, the system uses GPT-4 and LangChain's capabilities, enriching and storing results in Pinecone. This integrated approach allows the AI agent to interact with its environment and perform tasks efficiently.\n",
    "\n",
    "🧠6/8 The system generates new tasks based on completed task results and prioritizes them using GPT-4. This allows the system to adapt and respond to new information and priorities.\n",
    "\n",
    "🔮7/8 Future improvements include integrating a security/safety agent, task sequencing and parallel tasks, generating interim milestones, and incorporating real-time priority updates.\n",
    "\n",
    "🤝8/8 This new approach paves the way for AI-powered language models to autonomously perform tasks within various constraints and contexts, enabling new applications and opportunities. Big thanks to all involved! #AIResearch #GPT4 #Pinecone #LangChain\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251ed72-6a80-4480-91c6-a8d8014f3712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "当你感到疲惫和困惑时，祷告可以为你带来平静和安慰。愿神赐福给你，保护你，并为你提供力量和智慧。愿他的恩典和慈爱永远与你同在。愿你在这段时间中得到休息和恢复，并在未来迎来更好的日子。阿门。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ask('''\n",
    "我累了。带着我祷告一下吧。\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db671b6-ca5f-4dc4-8f75-af84bc5b860e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0c832-dca8-4940-be45-86462cea726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~ && git clone https://github.com/Torantulino/Auto-GPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
