{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5daaca1a-fb3b-49e8-ac33-418d3081eaf7",
   "metadata": {},
   "source": [
    "* [ColossalAI](https://mp.weixin.qq.com/s/C9b_oZu9jpw0oObEuDmd9w)\n",
    "  * https://github.com/hpcaitech/ColossalAI\n",
    "* [LMFlow](https://mp.weixin.qq.com/s/LCGQyNA6sHcdfIIARSNlww)\n",
    "  * https://github.com/OptimalScale/LMFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988ef06f-4549-40dd-9da4-38b842fe01fe",
   "metadata": {},
   "source": [
    "* [Chat with Open Large Language Models](https://chat.lmsys.org/)\n",
    "  * Vicuna: a chat assistant fine-tuned from LLaMA on user-shared conversations. This one is expected to perform best according to our evaluation.\n",
    "  * Koala: a chatbot fine-tuned from LLaMA on user-shared conversations and open-source datasets. This one performs similarly to Vicuna.\n",
    "  * ChatGLM: an open bilingual dialogue language model | 开源双语对话语言模型\n",
    "  * Alpaca: a model fine-tuned from LLaMA on 52K instruction-following demonstrations.\n",
    "  * LLaMA: open and efficient foundation language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a91f9c-0e3f-41a6-b0c3-c2bce089a7c5",
   "metadata": {},
   "source": [
    "* [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36276525-452a-4c17-8472-91c80bb535ac",
   "metadata": {},
   "source": [
    "* [Inference code for LLaMA models](https://github.com/facebookresearch/llama)\n",
    "* [LLaMA Cpp](https://github.com/ggerganov/llama.cpp)\n",
    "  * [Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp](https://til.simonwillison.net/llms/llama-7b-m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605846b3-8144-4f66-9402-b7a9b6a00309",
   "metadata": {},
   "source": [
    "* [Vicuna: An Open-Source Chatbot](https://vicuna.lmsys.org/)\n",
    "* [Vicuna Models: 7b-delta-v0 and 13b-delta-v0](https://huggingface.co/lmsys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf22d66-a6f9-400a-b1ff-de0dee7d7f6c",
   "metadata": {},
   "source": [
    "* [FastChat: An open platform for training, serving, and evaluating large language model based chatbots.](https://github.com/lm-sys/FastChat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5fb381-a9fe-407a-83b3-2ccc3a23a64a",
   "metadata": {},
   "source": [
    "* [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n",
    "* [Train Stanford's Alpaca Models](https://github.com/tatsu-lab/stanford_alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45577787-5379-4917-afc6-46c59ec3a12d",
   "metadata": {},
   "source": [
    "* [Instruct-tuned Alpaca-LoRA Models](https://github.com/tloen/alpaca-lora)\n",
    "  * [Alpaca-LoRA Instruct-tuning](https://github.com/tloen/alpaca-lora/blob/main/finetune.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227c3b0d-729b-4811-ab6c-beaf8b499d28",
   "metadata": {},
   "source": [
    "* [GPT4All: an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMA](https://github.com/nomic-ai/gpt4all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7de759-0c59-4828-ac1d-40333b1dc0f8",
   "metadata": {},
   "source": [
    "I recommend using [gpt4 x alpaca ggml](https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml) as a base model as it doesn't have the same level of censorship as vicuna. However, if you're using it purely for ethical tasks, vicuna is definitely better.\n",
    "\n",
    "* [Stanford Alpaca Model](https://huggingface.co/chavinlo/alpaca-native)\n",
    "* [Alpaca 13B without LoRA](https://huggingface.co/chavinlo/alpaca-13b)\n",
    "* [GPT4 x Alpaca without LoRA](https://huggingface.co/chavinlo/gpt4-x-alpaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb558ac-f3d8-45bf-8999-5c6c5f250a57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816040d-1b3c-4019-9f86-ba99190da3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ggml-model-q4_0.bin:   0%|          | 0.00/4.21G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_0.bin'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='Pi3141/alpaca-lora-7B-ggml', filename='ggml-model-q4_0.bin', resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c391f371-7b61-4342-8f37-52b3fc18cb14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ggml-model-q4_1.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='Pi3141/alpaca-lora-7B-ggml', filename='ggml-model-q4_1.bin', resume_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3eaf44-6266-40f1-912f-a0c0d926274e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1306126-21c7-4d89-9183-0899507d80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install llama-cpp-python[server]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee77fe9-b273-4aaa-b985-5a171bce93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 1608.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "INFO:     Started server process [41176]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://localhost:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:61438 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     ::1:61438 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     ::1:61438 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 11679.89 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 240847.82 ms /   478 tokens (  503.87 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 240873.74 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:61439 - \"POST /v1/embeddings HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 11679.89 ms\n",
      "llama_print_timings:      sample time =    33.43 ms /    16 runs   (    2.09 ms per run)\n",
      "llama_print_timings: prompt eval time = 255305.21 ms /   478 tokens (  534.11 ms per token)\n",
      "llama_print_timings:        eval time = 28069.18 ms /    15 runs   ( 1871.28 ms per run)\n",
      "llama_print_timings:       total time = 283470.31 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     ::1:61681 - \"POST /v1/completions HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export MODEL=~/ggml-model-q4_1.bin\n",
    "python3 -m llama_cpp.server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc015de-bc32-49a9-80f3-259a5aef99a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1268d4-a791-4ae2-baf4-dde6b5868d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  800.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=os.path.expanduser('~/ggml-model-q4_1.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2fc6b-4eba-4a36-90f3-0e9a9b8bf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.tokenize('''> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？'''.encode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0120b8-a7b0-4994-8e10-b3fb316eba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.tokenize('''> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。'''.encode('utf8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2a74e-1a35-467f-818f-592322afca3b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47a400-9557-43af-a430-e7133cfd6b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083d2920-80f9-433d-aa10-5e77ae95284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(dir_name):\n",
    "    # (1) Import a series of documents.\n",
    "    loader = DirectoryLoader(dir_name, loader_cls=TextLoader, silent_errors=True)\n",
    "    raw_documents = loader.load()\n",
    "    # (2) Split them into small chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=64,\n",
    "    )\n",
    "    return text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f091b-eb9f-4511-adbf-91f03c9e93a3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c83de1-20ef-42b1-90f5-a0a828e32877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0984d73-c782-4100-aa69-c521bd54950f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 6612.57 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 121567.27 ms /   607 tokens (  200.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 121578.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 55477.99 ms /   384 tokens (  144.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 55490.11 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 185983.50 ms /  1245 tokens (  149.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 185993.29 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 99895.20 ms /   648 tokens (  154.16 ms per token)\n",
      "llama_print_timings:        eval time =   274.67 ms /     1 runs   (  274.67 ms per run)\n",
      "llama_print_timings:       total time = 100174.12 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  4480.73 ms /    30 tokens (  149.36 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  4483.89 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 34421.56 ms /   208 tokens (  165.49 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 34428.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 18916.80 ms /   115 tokens (  164.49 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 18922.95 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 109097.79 ms /   672 tokens (  162.35 ms per token)\n",
      "llama_print_timings:        eval time =   322.11 ms /     1 runs   (  322.11 ms per run)\n",
      "llama_print_timings:       total time = 109426.49 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 182994.18 ms /  1131 tokens (  161.80 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 183004.98 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 159320.01 ms /  1040 tokens (  153.19 ms per token)\n",
      "llama_print_timings:        eval time =   239.12 ms /     1 runs   (  239.12 ms per run)\n",
      "llama_print_timings:       total time = 159568.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 239876.70 ms /  1530 tokens (  156.78 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 239888.82 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 217347.31 ms /  1428 tokens (  152.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 217358.50 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 191811.75 ms /  1255 tokens (  152.84 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 191821.81 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 226394.03 ms /  1406 tokens (  161.02 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 226403.94 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 250274.44 ms /  1514 tokens (  165.31 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 250290.90 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 245848.18 ms /  1459 tokens (  168.50 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 245869.09 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 241036.46 ms /  1448 tokens (  166.46 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 241046.69 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 90638.96 ms /   549 tokens (  165.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 90648.73 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 243383.44 ms /  1456 tokens (  167.16 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 243395.79 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 66549.32 ms /   407 tokens (  163.51 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 66557.07 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 254780.55 ms /  1524 tokens (  167.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 254791.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 63349.78 ms /   397 tokens (  159.57 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 63354.36 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 174274.05 ms /  1092 tokens (  159.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 174282.70 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 194346.63 ms /  1332 tokens (  145.91 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 194358.84 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 69352.89 ms /   490 tokens (  141.54 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 69357.43 ms\n",
      "\n",
      "llama_print_timings:        load time =  9764.51 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  7361.54 ms /    53 tokens (  138.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  7371.77 ms\n"
     ]
    }
   ],
   "source": [
    "def ingest_docs(dir_name):\n",
    "    documents = get_docs(dir_name)\n",
    "    # (3) Create embeddings for each document (using text-embedding-ada-002).\n",
    "    embeddings = LlamaCppEmbeddings(model_path=os.path.expanduser(\n",
    "        '~/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin'\n",
    "    ), n_ctx=2048)\n",
    "    return FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vectorstore = ingest_docs('_posts/ultimate-facts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b896d4-7736-49cc-b349-0cad23084245",
   "metadata": {},
   "source": [
    "I need a big memory to accelerate LLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf588c35-7b32-464f-9749-f396c9c92135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b3a4d-5238-4e6a-bd51-52df9f45d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore\n",
    "with open('vectorstore_7B_2048.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb375ee-6ae9-45d6-9710-ed81c3b65314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/.cache/huggingface/hub/models--Pi3141--alpaca-lora-7B-ggml/snapshots/fec53813efae6495f9b1f14aa4dedffc07bbf2e0/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =  59.11 KB\n",
      "llama_model_load_internal: mem required  = 6612.57 MB (+ 2052.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 2048.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Load vectorstore\n",
    "with open('vectorstore_7B_2048.pkl', 'rb') as f:\n",
    "    vectorstore = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea342c51-eefc-4922-b458-114f808b35ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d72b4-2dcb-46d9-b293-8e504502af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '你知道什么？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51cc46-6b9c-4c89-b277-bac48546e9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='title: Neuroscience\\ndate: 2021-10-14 16:30:20\\ntags: Neuro\\n---\\n\\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\\n\\n---\\n\\n> \\u3000\\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\\n\\u3000\\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\\n\\u3000\\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='有意注意，是指，对于某次效果的注意。\\n无意注意，是指，对于某次非效果的注意。\\n\\n目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。\\n意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。\\n\\n懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。\\n焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。\\n\\n对于某次功效的目标，联结着，对于此次功效的原因。\\n对于某种功效的概括，联结着，对于此种功效的原因。\\n\\n兴趣，是指，联结着某次快乐的识。\\n荒诞，是指，联结着某次乏味的识。\\n苦毒，是指，联结着某次痛苦的识。\\n\\n慾望，是指，对于某次兴趣的表征。\\n妄想，是指，对于某次荒诞的表征。？\\n苦观，是指，对于某次苦毒的表征。\\n\\n苦观，分为，记忆苦观、拟构苦观。弊害，…、…\\n\\n有趣注意，是指，对于某次兴趣的注意。\\n无趣注意，是指，对于某次荒诞的注意。\\n\\n意义，是指，值得的注意。\\n神圣，是指，极其丰富的意义。\\n积极的态度，是指，充满对于某种意义的信心。\\n消极的态度，是指，缺乏对于某种意义的信心。\\n积极的注意，导致着，快乐。\\n消极的注意，导致着，乏味。\\n对于某种意义的怀疑，是指，对于某种意义的信心的减弱。\\n对于某种意义的确定，是指，对于某种意义的信心的增强。\\n对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。\\n对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。\\n静思、禅修、祷告，都是，某种定觉练习。\\n\\n---\\n\\n> 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。\\n(罗马书 8:24-25 吕振中)\\n\\n> 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。\\n(提摩太后书 2:22 吕振中)\\n\\n向内往最深处去：净心、呼求主名、并且、等待回应。' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n",
      "page_content='> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\\n> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\\n> 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\\n> ……' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='客体方式，导致着、联结着，主体方式、机体状态\\n形体，导致着、联结着，身体、快乐、痛苦\\n轻蔑、轻视他人对自己的态度，损害着，羞耻心\\n羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？\\n羞耻、光荣，重视他人对自己的态度、敬重\\n恥辱、傲慢，轻视他人对自己的态度、轻蔑\\n羞耻、羞辱，在含义上，有所不同吗？\\n单方的轻视、双方的轻视？\\n一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。\\n狭隘、愚蠢、固执，轻视他人的信念\\n开明、智慧、变通，重视他人的信念' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14467.91 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 14467.34 ms /     8 tokens ( 1808.42 ms per token)\n",
      "llama_print_timings:        eval time = 14635.33 ms /     1 runs   (14635.33 ms per run)\n",
      "llama_print_timings:       total time = 29115.01 ms\n"
     ]
    }
   ],
   "source": [
    "# Get context related to the question from the embedding model\n",
    "for context in vectorstore.similarity_search(question):\n",
    "    print(f'{context}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43ec6dc-c714-409d-8cdc-d9957a40a1e6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73efc6b5-2a62-400c-9a58-f76ba564c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbfdac-3d84-41f3-9a26-8980211e1ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(\n",
    "    input_variables=['chat_history', 'question'],\n",
    "    output_parser=None, partial_variables={},\n",
    "    template='给定以下对话和后续问题，请重新表述后续问题以成为一个独立问题。\\n\\n聊天记录：\\n{chat_history}\\n后续问题：{question}\\n独立问题：',\n",
    "    template_format='f-string',\n",
    "    validate_template=True\n",
    ")\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=['context', 'question'],\n",
    "    output_parser=None, partial_variables={},\n",
    "    template='使用下面的背景信息回答最后的问题。如果您不知道答案，请直接说您不知道，不要试图编造一个答案。\\n\\n背景信息：\\n{context}\\n\\n问题：{question}\\n有用的答案：',\n",
    "    template_format='f-string',\n",
    "    validate_template=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc63ab2-ed03-4601-b7d2-1a3ebdc50e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Callback function to stream answers to stdout.\n",
    "manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "streaming_llm = ChatOpenAI(streaming=True, callback_manager=manager, verbose=True, temperature=0)\n",
    "question_gen_llm = ChatOpenAI(temperature=0, verbose=True, callback_manager=manager)\n",
    "# Prompt to generate independent questions by incorporating chat history and a new question.\n",
    "question_generator = LLMChain(llm=question_gen_llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "# Pass in documents and a standalone prompt to answer questions.\n",
    "doc_chain = load_qa_chain(streaming_llm, chain_type='stuff', prompt=QA_PROMPT)\n",
    "# Generate prompts from embedding model.\n",
    "qa = ConversationalRetrievalChain(retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c778160-8831-4c17-abb8-f39c7d4d6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b953692-0da2-4c33-9d5b-435c8662ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from revChatGPT.V1 import Chatbot, configure\n",
    "\n",
    "# Open the JSON file and read the conversation_id\n",
    "with open(os.path.expanduser('~/.config/revChatGPT/config.json'), 'r') as f:\n",
    "    conversation_id = json.load(f).get('conversation_id', None)\n",
    "\n",
    "bot = Chatbot(\n",
    "    config = configure(),\n",
    "    conversation_id = conversation_id,\n",
    "    lazy_loading = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f43e6db-a329-490c-9cd3-b9e813c13dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48d478-5bd0-43a1-94f3-d6db6b9f6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(prompt):\n",
    "    res = ''\n",
    "    for response in bot.ask(prompt):\n",
    "        yield {\n",
    "            'choices': [\n",
    "                {\n",
    "                    'index': 0,\n",
    "                    'delta': {\n",
    "                        'content': response['message'][len(res):],\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "        res = response['message']\n",
    "\n",
    "def mock_create(*args, **kwargs):\n",
    "    for message in kwargs['messages']:\n",
    "        if message['role'] == 'user':\n",
    "            break\n",
    "    else:\n",
    "        return {\n",
    "            'choices': [{}],\n",
    "        }\n",
    "\n",
    "    if kwargs.get('stream', False):\n",
    "        return delta(message['content'])\n",
    "\n",
    "    for response in bot.ask(message['content']):\n",
    "        pass\n",
    "    return {\n",
    "        'choices': [\n",
    "            {\n",
    "                'finish_reason': 'stop',\n",
    "                'index': 0,\n",
    "                'message': {\n",
    "                    'content': response['message'],\n",
    "                    'role': 'assistant',\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357dd0b-e10e-4835-9ab2-8f3eaf7898b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def mock_openai(monkeypatch):\n",
    "    monkeypatch.setattr(openai.ChatCompletion, 'create', mock_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472b2e30-cc60-4f97-ad03-0285b4893e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '终极真实是什么？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0708824-cc89-4efb-941d-c2e3ea4ea4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5eae31-b588-4463-82c8-18bc8031f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qa(mock_openai):\n",
    "    global answer\n",
    "    answer = qa({'question': question, 'chat_history': []})\n",
    "    print('\\n')\n",
    "    assert isinstance(answer, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922530e2-a311-4810-a7d1-85a7a07fe2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipytest import do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cdbb21-079b-45e6-935a-a052841da198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> no.0  ::source::test_qa  setup  passed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14467.91 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 22490.19 ms /    15 tokens ( 1499.35 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 22496.21 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的背景信息，以下是问题的答案：\n",
      "\n",
      "此处引用的文章是哪位心理学家写的？ \n",
      "\n",
      "这篇文章是由德国心理学家威廉·威廉德撰写的，标题是《心理学的目标和方法》。\n",
      "\n",
      "=> no.0  ::source::test_qa  runtest  passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do(\n",
    "    mock_openai=mock_openai,\n",
    "    test_qa=test_qa,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13deaeb9-9773-44e6-a609-f40f660cce24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '终极真实是什么？',\n",
       " 'chat_history': [],\n",
       " 'answer': '根据提供的背景信息，以下是问题的答案：\\n\\n此处引用的文章是哪位心理学家写的？ \\n\\n这篇文章是由德国心理学家威廉·威廉德撰写的，标题是《心理学的目标和方法》。'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
