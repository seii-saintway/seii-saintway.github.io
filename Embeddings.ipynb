{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23755b6d-07a7-458a-b7b7-266f9e54a736",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ceca84-680f-4ca4-bf96-5311a222d9c6",
   "metadata": {},
   "source": [
    "* [OpenAI Embedding Model](https://openai.com/blog/new-and-improved-embedding-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e26494-5145-48b7-a915-dc4202f75459",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013113e-cdb8-489d-a00f-cf60c5263f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from EdgeGPT import Chatbot as Bing, ConversationStyle\n",
    "\n",
    "bing = Bing(cookiePath = os.path.expanduser('~/.config/EdgeGPT/cookies.json'))\n",
    "\n",
    "async def ask(prompt):\n",
    "    res = (await bing.ask(\n",
    "        prompt = prompt,\n",
    "        conversation_style = ConversationStyle.balanced,\n",
    "    ))['item']['messages'][1]\n",
    "\n",
    "    print(res['text'])\n",
    "    print('\\n---\\n')\n",
    "    print(res['adaptiveCards'][0]['body'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d4faa-7ea5-42c8-8e60-8186f5d08b99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26832a-1ea8-478c-accb-a8fdaa8a088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002 是 OpenAI 提供的一种嵌入模型，用于将文本转换为向量表示。[^1^] 你可以将文本字符串发送到嵌入 API 端点，同时选择嵌入模型 ID（例如 text-embedding-ada-002），响应将包含一个嵌入，你可以提取、保存和使用。[^1^] \n",
      "\n",
      "text-embedding-ada-002 模型是 OpenAI 提供的第二代嵌入模型之一，它比第一代模型更好、更便宜、更简单易用。[^5^] \n",
      "\n",
      "如果你想了解更多关于 text-embedding-ada-002 的信息，可以参考 OpenAI 的官方文档。[^5^]\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://platform.openai.com/docs/guides/embeddings/second-generation-models \"Embeddings - OpenAI API\"\n",
      "[2]: https://juejin.cn/post/7216319218505220151 \"OpenAI的Embedding - 掘金 - 稀土掘金\"\n",
      "[3]: https://cloud.tencent.com/developer/article/2222888 \"GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试 - 腾讯云开发者社区-腾讯云\"\n",
      "[4]: https://openai.com/blog/new-and-improved-embedding-model/ \"New and improved embedding model - openai.com\"\n",
      "[5]: https://platform.openai.com/docs/guides/embeddings \"Embeddings - OpenAI API\"\n",
      "\n",
      "text-embedding-ada-002 是 OpenAI 提供的一种嵌入模型，用于将文本转换为向量表示。[^1^][1] 你可以将文本字符串发送到嵌入 API 端点，同时选择嵌入模型 ID（例如 text-embedding-ada-002），响应将包含一个嵌入，你可以提取、保存和使用。[^1^][1] \n",
      "\n",
      "text-embedding-ada-002 模型是 OpenAI 提供的第二代嵌入模型之一，它比第一代模型更好、更便宜、更简单易用。[^2^][5] \n",
      "\n",
      "如果你想了解更多关于 text-embedding-ada-002 的信息，可以参考 OpenAI 的官方文档。[^2^][5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "text-embedding-ada-002 是什么？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a2515-d0be-407f-b758-397f1bbabb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入都是常见的文本嵌入模型。其中，GPT-3 是 OpenAI 开发的一种大型语言模型，可以生成自然语言文本[^1^]。GloVe 和 Word2vec 都是基于神经网络的词向量模型，用于将单词转换为向量表示，以便计算机可以更好地理解自然语言[^1^] [^2^]。MPNet 是一种基于多任务学习的预训练语言模型，可以用于各种自然语言处理任务[^1^] [^4^]。\n",
      "\n",
      "据腾讯云开发者社区的一篇文章所述，该作者使用了 text-embedding-ada-002（GPT-3 的一个 Embeddings）与三种传统文本嵌入技术生成的嵌入进行比较，即 GloVe、Word2vec 和 MPNet。结果显示，GPT-3 嵌入在所有模型中获得了最高的精度[^1^]。但是，这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://cloud.tencent.com/developer/article/2222888 \"GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试 - 腾讯云开发者社区-腾讯云\"\n",
      "[2]: https://developer.aliyun.com/article/1159268 \"GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试-阿里云开发者社区\"\n",
      "[3]: https://new.qq.com/rain/a/20230212A01IWP00 \"GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试_腾讯新闻\"\n",
      "[4]: https://blog.csdn.net/tMb8Z9Vdm66wH68VX1/article/details/129153025 \"GPT-3 vs Bert vs GloVe vs Word2vec 文本嵌入技术的性能对比测试\"\n",
      "[5]: https://zhuanlan.zhihu.com/p/605603003 \"GPT-3 vs Bert vs GloVe 文本嵌入技术的性能对比测试 - 知乎\"\n",
      "\n",
      "GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入都是常见的文本嵌入模型。其中，GPT-3 是 OpenAI 开发的一种大型语言模型，可以生成自然语言文本[^1^][1]。GloVe 和 Word2vec 都是基于神经网络的词向量模型，用于将单词转换为向量表示，以便计算机可以更好地理解自然语言[^1^][1] [^2^][2]。MPNet 是一种基于多任务学习的预训练语言模型，可以用于各种自然语言处理任务[^1^][1] [^3^][4]。\n",
      "\n",
      "据腾讯云开发者社区的一篇文章所述，该作者使用了 text-embedding-ada-002（GPT-3 的一个 Embeddings）与三种传统文本嵌入技术生成的嵌入进行比较，即 GloVe、Word2vec 和 MPNet。结果显示，GPT-3 嵌入在所有模型中获得了最高的精度[^1^][1]。但是，这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "比较 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入、MPNet 嵌入\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872052a-8718-4db7-a0ad-1a0fe4bd0cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "除了 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入之外，还有许多其他文本嵌入模型。例如，ELMo、Skip-Thoughts、Quick-Thoughts、InferSent、MILA/MSR 的通用句子表示和 Google 的通用句子编码器等模型都是最先进的模型之一[^1^]。这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://cloud.tencent.com/developer/article/1167972 \"干货 | 文本嵌入的经典模型与最新进展 - 腾讯云开发者社区-腾讯云\"\n",
      "[2]: https://blog.csdn.net/lingpy/article/details/80809926 \"文本嵌入的经典模型与最新进展 - CSDN博客\"\n",
      "[3]: https://zhuanlan.zhihu.com/p/459295186 \"5分钟 NLP系列—— 11 个词嵌入模型总结 - 知乎 - 知乎专栏\"\n",
      "\n",
      "除了 GPT-3 嵌入、GloVe 嵌入、Word2vec 嵌入和 MPNet 嵌入之外，还有许多其他文本嵌入模型。例如，ELMo、Skip-Thoughts、Quick-Thoughts、InferSent、MILA/MSR 的通用句子表示和 Google 的通用句子编码器等模型都是最先进的模型之一[^1^][1]。这些模型的性能可能会因应用场景而异。如果你需要更多信息，请告诉我！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "还有哪些文本嵌入模型？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc4477-8d40-427e-a461-3d1366986d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The English term for \"文本嵌入模型\" is \"text embedding models\". Some of the most common text embedding models include Word2Vec, GloVe, FastText and Embeddings from Language Models (ELMo)[^1^] [^2^]. If you need more information about these models or any other text embedding models, please let me know!\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://techblog.ezra.com/different-embedding-models-7874197dc410 \"What are the common word embeddings? | The Ezra Tech Blog - Medium\"\n",
      "[2]: https://openai.com/blog/introducing-text-and-code-embeddings/ \"Introducing text and code embeddings - OpenAI\"\n",
      "[3]: https://www.deepset.ai/blog/the-beginners-guide-to-text-embeddings \"The Beginner’s Guide to Text Embeddings | deepset\"\n",
      "[4]: https://platform.openai.com/docs/guides/embeddings/embedding-models \"Embeddings - OpenAI API\"\n",
      "\n",
      "The English term for \"文本嵌入模型\" is \"text embedding models\". Some of the most common text embedding models include Word2Vec, GloVe, FastText and Embeddings from Language Models (ELMo)[^1^][1] [^2^][2]. If you need more information about these models or any other text embedding models, please let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "文本嵌入模型的英文？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe838a2-5d6f-4b74-a1f7-6139afcd27a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InstructorEmbedding is a text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning[^1^]. It is a single embedder that can generate text embeddings tailored to different downstream tasks and domains[^1^]. InstructorEmbedding is available as a Python package[^2^]. If you need more information about InstructorEmbedding or any other text embedding models, please let me know!\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://instructor-embedding.github.io/ \"Instructor Text Embedding\"\n",
      "[2]: https://pypi.org/project/InstructorEmbedding/ \"InstructorEmbedding · PyPI\"\n",
      "[3]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html \"langchain.embeddings.huggingface — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.135\"\n",
      "\n",
      "InstructorEmbedding is a text embedding model that can generate text embeddings tailored to any task (e.g., classification, retrieval, clustering, text evaluation, etc.) and domains (e.g., science, finance, etc.) by simply providing the task instruction, without any finetuning[^1^][1]. It is a single embedder that can generate text embeddings tailored to different downstream tasks and domains[^1^][1]. InstructorEmbedding is available as a Python package[^2^][2]. If you need more information about InstructorEmbedding or any other text embedding models, please let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "InstructorEmbedding 是什么？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13a6d3-7d71-476b-ba46-d74e00ce1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceInstructEmbeddings and HuggingFaceHubEmbeddings are both wrappers around sentence_transformers embedding models[^2^] [^3^]. HuggingFaceInstructEmbeddings is a wrapper around InstructorEmbedding[^2^], while HuggingFaceHubEmbeddings is a wrapper around Hugging Face Hub embedding models[^3^]. To use HuggingFaceInstructEmbeddings, you should have the `sentence_transformers` and `InstructorEmbedding` Python packages installed[^2^]. To use HuggingFaceHubEmbeddings, you should have the `huggingface_hub` Python package installed and the environment variable `HUGGINGFACEHUB_API_TOKEN` set with your API token, or pass it as a named parameter to the constructor[^3^]. If you need more information about HuggingFaceInstructEmbeddings or HuggingFaceHubEmbeddings or any other text embedding models, please let me know!\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://huggingface.co/blog/getting-started-with-embeddings \"Getting Started With Embeddings - Hugging Face\"\n",
      "[2]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html \"langchain.embeddings.huggingface — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.135\"\n",
      "[3]: https://sj-langchain.readthedocs.io/en/latest/reference/modules/embeddings.html \"Embeddings — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.94\"\n",
      "[4]: https://stackoverflow.com/questions/64685243/getting-sentence-embedding-from-huggingface-feature-extraction-pipeline \"machine learning - Getting sentence embedding from huggingface Feature ...\"\n",
      "\n",
      "HuggingFaceInstructEmbeddings and HuggingFaceHubEmbeddings are both wrappers around sentence_transformers embedding models[^1^][2] [^2^][3]. HuggingFaceInstructEmbeddings is a wrapper around InstructorEmbedding[^1^][2], while HuggingFaceHubEmbeddings is a wrapper around Hugging Face Hub embedding models[^2^][3]. To use HuggingFaceInstructEmbeddings, you should have the `sentence_transformers` and `InstructorEmbedding` Python packages installed[^1^][2]. To use HuggingFaceHubEmbeddings, you should have the `huggingface_hub` Python package installed and the environment variable `HUGGINGFACEHUB_API_TOKEN` set with your API token, or pass it as a named parameter to the constructor[^2^][3]. If you need more information about HuggingFaceInstructEmbeddings or HuggingFaceHubEmbeddings or any other text embedding models, please let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "比较 HuggingFaceInstructEmbeddings 和 HuggingFaceHubEmbeddings\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903cc6a-0300-4229-8d76-63a301bc07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the GitHub link for sentence_transformers: https://github.com/UKPLab/sentence-transformers [^1^] [^5^]. You can find more information about sentence_transformers and how to use it in your project by visiting this link. Let me know if you have any other questions!\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://github.com/taishan1994/sbert_text_similarity \"GitHub - taishan1994/sbert_text_similarity: 使用sentence-transformers ...\"\n",
      "[2]: https://github.com/zhangyi24/sentence_transformer_zh \"GitHub - zhangyi24/sentence_transformer_zh\"\n",
      "[3]: https://blog.csdn.net/RinaWong/article/details/122890394 \"安装sentence-transformers_竹霖聴雨的博客-CSDN博客\"\n",
      "[4]: https://github.com/UKPLab/sentence-transformers \"UKPLab/sentence-transformers - Github\"\n",
      "[5]: https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained_models.md \"sentence-transformers/pretrained_models.md at master · UKPLab ... - GitHub\"\n",
      "[6]: https://github.com/huggingface/blog/blob/main/how-to-train-sentence-transformers.md \"blog/how-to-train-sentence-transformers.md at main - Github\"\n",
      "\n",
      "Here is the GitHub link for sentence_transformers: https://github.com/UKPLab/sentence-transformers [^1^][1] [^2^][5]. You can find more information about sentence_transformers and how to use it in your project by visiting this link. Let me know if you have any other questions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "sentence_transformers 的 GitHub 链接？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476b0d2-3487-44a1-b6e5-642442a30275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the GitHub link for InstructorEmbedding: https://github.com/HKUNLP/instructor-embedding [^1^] [^2^]. You can find more information about InstructorEmbedding and how to use it in your project by visiting this link. Let me know if you have any other questions!\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://pypi.org/project/InstructorEmbedding/ \"InstructorEmbedding · PyPI\"\n",
      "[2]: https://github.com/HKUNLP/instructor-embedding \"GitHub - HKUNLP/instructor-embedding: One Embedder, Any Task ...\"\n",
      "[3]: https://github.com/basujindal/chatPDF \"GitHub - basujindal/chatPDF: chat with PDF\"\n",
      "\n",
      "Here is the GitHub link for InstructorEmbedding: https://github.com/HKUNLP/instructor-embedding [^1^][1] [^2^][2]. You can find more information about InstructorEmbedding and how to use it in your project by visiting this link. Let me know if you have any other questions!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "InstructorEmbedding 的 GitHub 链接？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf8a27-438b-4a38-b2d3-0021b1728e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings 都是用于自然语言处理的嵌入式模型。HuggingFaceInstructEmbeddings 是 sentence_transformers 的包装器，而 LlamaCppEmbeddings 是 llama.cpp 嵌入式模型的包装器。这两个模型都可以用于生成文本嵌入，但是它们的实现方式和性能可能有所不同。如果您需要更多的信息，请访问它们的 GitHub 页面：https://github.com/HKUNLP/instructor-embedding [^1^] [^2^] 和 https://github.com/ggerganov/llama.cpp [^3^] 。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/llamacpp.html \"langchain.embeddings.llamacpp — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.136\"\n",
      "[2]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html \"langchain.embeddings.huggingface — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.135\"\n",
      "[3]: https://sj-langchain.readthedocs.io/en/latest/_modules/langchain/embeddings/huggingface.html \"langchain.embeddings.huggingface — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.94\"\n",
      "\n",
      "HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings 都是用于自然语言处理的嵌入式模型。HuggingFaceInstructEmbeddings 是 sentence_transformers 的包装器，而 LlamaCppEmbeddings 是 llama.cpp 嵌入式模型的包装器。这两个模型都可以用于生成文本嵌入，但是它们的实现方式和性能可能有所不同。如果您需要更多的信息，请访问它们的 GitHub 页面：https://github.com/HKUNLP/instructor-embedding [^1^][1] [^2^][2] 和 https://github.com/ggerganov/llama.cpp [^3^][3] 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "比较 HuggingFaceInstructEmbeddings 和 LlamaCppEmbeddings\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b463cb4-2311-4dc0-ad23-27fc1e8ed682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceEmbeddings 的模型下载路径是本地缓存。当您第一次使用该模型时，它会自动缓存在本地。如果您想查看缓存的位置，可以运行以下代码：\n",
      "```\n",
      "from transformers import AutoTokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "print(tokenizer.cache_dir)\n",
      "```\n",
      "如果您想将模型下载到特定的文件夹中，可以使用以下代码：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModel.from_pretrained(\"bert-base-uncased\", cache_dir=\"my_folder\")\n",
      "```\n",
      "希望这可以帮助到您！\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://python.langchain.com/en/latest/_modules/langchain/embeddings/huggingface.html \"langchain.embeddings.huggingface — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.135\"\n",
      "[2]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface \"How to download model from huggingface? - Stack Overflow\"\n",
      "[3]: https://github.com/hwchase17/langchain/blob/master/langchain/embeddings/huggingface.py \"langchain/huggingface.py at master · hwchase17/langchain\"\n",
      "\n",
      "HuggingFaceEmbeddings 的模型下载路径是本地缓存。当您第一次使用该模型时，它会自动缓存在本地。如果您想查看缓存的位置，可以运行以下代码：\n",
      "```\n",
      "from transformers import AutoTokenizer\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "print(tokenizer.cache_dir)\n",
      "```\n",
      "如果您想将模型下载到特定的文件夹中，可以使用以下代码：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModel\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModel.from_pretrained(\"bert-base-uncased\", cache_dir=\"my_folder\")\n",
      "```\n",
      "希望这可以帮助到您！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "执行下面这段代码之后，HuggingFaceEmbeddings 的模型下载到哪里了？\n",
    "```\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 准备文本\n",
    "text = '这是一个测试文档。'\n",
    "\n",
    "# 使用 HuggingFace 生成文本嵌入\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "query_result = embeddings.embed_query(text)\n",
    "doc_result = embeddings.embed_documents([text])\n",
    "```\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06a076-c12b-4196-9c9a-90f92fb4fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfd0788-f7a4-44c2-9091-819d3ccf649e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021775be-12f3-4b30-a8bb-78c90c05f790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "total 0\n",
      "drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 \u001b[1m\u001b[36m.no_exist\u001b[m\u001b[m\n",
      "drwxr-xr-x  6 saintway  staff   192B Apr 12 14:43 \u001b[1m\u001b[36mblobs\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 \u001b[1m\u001b[36mrefs\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 saintway  staff    96B Apr 12 14:43 \u001b[1m\u001b[36msnapshots\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah ~/.cache/huggingface/hub/models--bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db213984-9516-43e1-b8d6-bf4921e4a0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "696K\t/Users/saintway/.cache/huggingface/hub/models--bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "du -sh ~/.cache/huggingface/hub/models--bert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454baa",
   "metadata": {},
   "source": [
    "---\n",
    "If you're talking about vector database embeddings, check out [Chroma](https://github.com/chroma-core/chroma).\n",
    "\n",
    "* https://www.trychroma.com/\n",
    "* https://docs.trychroma.com/embeddings\n",
    "\n",
    "---\n",
    "If you're talking about fine tuning, check out [alpaca-lora finetuning](https://github.com/tloen/alpaca-lora/blob/main/finetune.py).\n",
    "\n",
    "I recommend using [gpt4xalpaca](https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml) as a base model as it doesn't have the same level of censorship as vicuna. However, if you're using it purely for ethical tasks, vicuna is definitely better.\n",
    "\n",
    "[GPT4 x Alpaca](https://huggingface.co/chavinlo/gpt4-x-alpaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8f9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma Embedding Database 是一个开源的嵌入式数据库，用于存储人工智能模型的嵌入[^1^]。嵌入是向量的集合。Chroma 的数据库专门用于存储 AI 模型的嵌入[^2^]。它允许您通过最近邻搜索而不是子字符串搜索来搜索嵌入[^3^]。默认情况下，Chroma 使用 Sentence Transformers 进行嵌入，但您也可以使用 OpenAI 嵌入、Cohere（多语言）嵌入或自己的嵌入[^3^]。\n",
      "\n",
      "希望这可以帮助您了解 Chroma Embedding Database。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://docs.trychroma.com/ \"\\uD83C\\uDFE1 Home | Chroma\"\n",
      "[2]: https://siliconangle.com/2023/04/06/chroma-bags-18m-speed-ai-models-embedding-database/ \"Chroma bags $18M to speed up AI models with its embedding database\"\n",
      "[3]: https://pypi.org/project/chromadb/ \"chromadb · PyPI\"\n",
      "[4]: https://www.trychroma.com/ \"Chroma - the AI-native open-source embedding database\"\n",
      "[5]: https://github.com/chroma-core/chroma \"GitHub - chroma-core/chroma: the open source embedding database\"\n",
      "\n",
      "Chroma Embedding Database 是一个开源的嵌入式数据库，用于存储人工智能模型的嵌入[^1^][1]。嵌入是向量的集合。Chroma 的数据库专门用于存储 AI 模型的嵌入[^2^][2]。它允许您通过最近邻搜索而不是子字符串搜索来搜索嵌入[^3^][3]。默认情况下，Chroma 使用 Sentence Transformers 进行嵌入，但您也可以使用 OpenAI 嵌入、Cohere（多语言）嵌入或自己的嵌入[^3^][3]。\n",
      "\n",
      "希望这可以帮助您了解 Chroma Embedding Database。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "Chroma Embedding Database 是什么\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719f661-6e13-44a7-966b-b14d73cdc138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ggml 是一个用于机器学习的张量库。该项目正在开发中，尚未准备好用于生产。它是用 C 语言编写的，支持 16 位浮点数和 4 位整数量化。它还支持自动微分（正在进行中）ADAM 和 L ...[^3^]。\n",
      "\n",
      "希望这可以帮助到您！\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://gitee.com/mirrors/ggml \"ggml: ggml 是一个用于机器学习的张量库\"\n",
      "[2]: https://www.oschina.net/p/ggml \"ggml首页、文档和下载 - 机器学习的张量库 - OSCHINA - 中文开源技术交流社区\"\n",
      "[3]: https://github.com/ggerganov/ggml \"GitHub - ggerganov/ggml: Tensor library for machine learning\"\n",
      "[4]: https://zhuanlan.zhihu.com/p/616037509 \"介绍大模型LLaMa及周边 - 知乎 - 知乎专栏\"\n",
      "[5]: https://github.com/ggerganov/whisper.cpp \"ggerganov/whisper.cpp: Port of OpenAI's Whisper model in C/C++ - Github\"\n",
      "\n",
      "ggml 是一个用于机器学习的张量库。该项目正在开发中，尚未准备好用于生产。它是用 C 语言编写的，支持 16 位浮点数和 4 位整数量化。它还支持自动微分（正在进行中）ADAM 和 L ...[^1^][3]。\n",
      "\n",
      "希望这可以帮助到您！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "ggml 是什么\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cb713c-a69c-4b8b-a2c1-0dd9b151adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACEHUB_API_TOKEN 是一个环境变量，用于验证您的 API 令牌。您可以使用它来访问 Hugging Face Hub API，以便在您的模型中使用预训练模型、上传和下载模型等。[^1^] [^3^]\n",
      "\n",
      "希望这可以帮助到您！\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://sj-langchain.readthedocs.io/en/latest/ecosystem/huggingface.html \"Hugging Face — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.94\"\n",
      "[2]: https://zhuanlan.zhihu.com/p/344173246 \"用户身份验证的令牌——Token - 知乎 - 知乎专栏\"\n",
      "[3]: https://github.com/hwchase17/langchain/blob/master/langchain/llms/huggingface_hub.py \"langchain/huggingface_hub.py at master · hwchase17/langchain\"\n",
      "\n",
      "HUGGINGFACEHUB_API_TOKEN 是一个环境变量，用于验证您的 API 令牌。您可以使用它来访问 Hugging Face Hub API，以便在您的模型中使用预训练模型、上传和下载模型等。[^1^][1] [^2^][3]\n",
      "\n",
      "希望这可以帮助到您！\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "HUGGINGFACEHUB_API_TOKEN 有什么用？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e607257-f8a6-42af-b393-333dadb19fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Hub API 是一个模型和数据集的存储库，可以让您轻松地共享、发现和使用模型和数据集。它提供了一组 API，可以让您从 Python 中上传、下载和管理模型和数据集。您可以使用 Hugging Face Hub API 来存储、共享和管理您的模型和数据集，也可以使用它来发现其他人共享的模型和数据集。[^1^]\n",
      "\n",
      "Hugging Face Hub API 还提供了一些端点，您可以使用这些端点从 Hub 中检索信息，也可以执行某些操作，例如创建模型、数据集或 Space 存储库。[^1^]\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://huggingface.co/docs/hub/api \"https://\\uE000huggingface\\uE001.co/docs/\\uE000hub\\uE001/\\uE000api\\uE001\"\n",
      "\n",
      "Hugging Face Hub API 是一个模型和数据集的存储库，可以让您轻松地共享、发现和使用模型和数据集。它提供了一组 API，可以让您从 Python 中上传、下载和管理模型和数据集。您可以使用 Hugging Face Hub API 来存储、共享和管理您的模型和数据集，也可以使用它来发现其他人共享的模型和数据集。[^1^][1]\n",
      "\n",
      "Hugging Face Hub API 还提供了一些端点，您可以使用这些端点从 Hub 中检索信息，也可以执行某些操作，例如创建模型、数据集或 Space 存储库。[^1^][1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "Hugging Face Hub API 有什么用？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd74c6-e74e-4134-b309-2a1f0c90640d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，Hugging Face Hub API 需要 HUGGINGFACEHUB_API_TOKEN。您需要在 Hugging Face 帐户设置中获取用户访问或 API 令牌。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^] [^3^]\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://api-inference.huggingface.co/docs/python/html/quicktour.html \"Overview - Hugging Face\"\n",
      "[2]: https://huggingface.co/docs/huggingface_hub/how-to-inference \"Access the Inference API - Hugging Face\"\n",
      "[3]: https://python.langchain.com/en/latest/ecosystem/huggingface.html \"Hugging Face — \\uD83E\\uDD9C\\uD83D\\uDD17 LangChain 0.0.137\"\n",
      "[4]: https://docs.fast.ai/huggingface.html \"fastai - Hugging Face Hub\"\n",
      "[5]: https://zhuanlan.zhihu.com/p/619763221 \"HuggingGPT： 用ChatGPT和它的朋友在HuggingFace中解决AI任务 - 知乎\"\n",
      "\n",
      "是的，Hugging Face Hub API 需要 HUGGINGFACEHUB_API_TOKEN。您需要在 Hugging Face 帐户设置中获取用户访问或 API 令牌。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^][1] [^2^][3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "没有 HUGGINGFACEHUB_API_TOKEN 就不能使用 Hugging Face Hub API 吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce34993-add3-41f0-9fc8-eb832931a535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是的，您需要 HUGGINGFACEHUB_API_TOKEN 才能在 HuggingFace 下载模型。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^] [^3^]\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://huggingface.co/docs/hub/security-tokens \"User access tokens - Hugging Face\"\n",
      "[2]: https://blog.csdn.net/qq_41895747/article/details/125232359 \"解决ValueError: You need to pass a valid `token` or login by using ...\"\n",
      "[3]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface \"How to download model from huggingface? - Stack Overflow\"\n",
      "[4]: https://zhuanlan.zhihu.com/p/475260268 \"如何优雅的下载huggingface-transformers模型 - 知乎 - 知乎专栏\"\n",
      "[5]: https://juejin.cn/post/7220623225775095869 \"ILLA Cloud- 调用 Hugging Face Inference Endpoints，开启大模型世界之门 - 掘金\"\n",
      "\n",
      "是的，您需要 HUGGINGFACEHUB_API_TOKEN 才能在 HuggingFace 下载模型。如果您在发送请求到 API 时不提交 API 令牌，则无法在私有模型上运行推理。[^1^][1] [^2^][3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "没有 HUGGINGFACEHUB_API_TOKEN 就不能在 HuggingFace 下载模型吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7fd1f-21e0-4cd1-be53-c1b6ca809fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在机器学习中，训练模型是指使用大量数据来训练模型，以便它可以自动地从新数据中进行预测。在训练期间，模型会根据输入数据进行调整，以便它可以更好地预测输出。训练完成后，模型就可以用于推理，也就是使用新数据进行预测。\n",
      "\n",
      "推理是指使用训练好的模型来进行预测。在推理期间，模型会接收新的输入数据，并输出预测结果。推理通常比训练快得多，因为它不需要对模型进行调整。\n",
      "\n",
      "希望这可以帮到你。如果你有其他问题，请随时问我。\n",
      "\n",
      "---\n",
      "\n",
      "在机器学习中，训练模型是指使用大量数据来训练模型，以便它可以自动地从新数据中进行预测。在训练期间，模型会根据输入数据进行调整，以便它可以更好地预测输出。训练完成后，模型就可以用于推理，也就是使用新数据进行预测。\n",
      "\n",
      "推理是指使用训练好的模型来进行预测。在推理期间，模型会接收新的输入数据，并输出预测结果。推理通常比训练快得多，因为它不需要对模型进行调整。\n",
      "\n",
      "希望这可以帮到你。如果你有其他问题，请随时问我。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "在模型上运行推理是什么意思？训练模型是什么意思？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0002fac-e7f4-4834-bad5-4ac1a8dc60d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace 下载他人公开的模型不需要 HUGGINGFACEHUB_API_TOKEN。你可以通过以下方式下载他人公开的模型：\n",
      "\n",
      "1. 通过 Hugging Face 官网下载，例如：https://huggingface.co/models\n",
      "2. 通过 Transformers 库中的 from_pretrained 方法下载，例如：\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "```\n",
      "\n",
      "希望这可以帮到你。如果你有其他问题，请随时问我。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://zhuanlan.zhihu.com/p/475260268 \"如何优雅的下载huggingface-transformers模型 - 知乎 - 知乎专栏\"\n",
      "[2]: https://huggingface.co/inference-api \"Inference API - Hugging Face\"\n",
      "[3]: https://zhuanlan.zhihu.com/p/448852278 \"【Huggingface Transformers】保姆级使用教程—上 - 知乎\"\n",
      "[4]: https://pypi.org/project/huggingface-hub/ \"huggingface-hub · PyPI\"\n",
      "[5]: https://stackoverflow.com/questions/67595500/how-to-download-model-from-huggingface \"How to download model from huggingface? - Stack Overflow\"\n",
      "\n",
      "HuggingFace 下载他人公开的模型不需要 HUGGINGFACEHUB_API_TOKEN。你可以通过以下方式下载他人公开的模型：\n",
      "\n",
      "1. 通过 Hugging Face 官网下载，例如：https://huggingface.co/models\n",
      "2. 通过 Transformers 库中的 from_pretrained 方法下载，例如：\n",
      "\n",
      "```python\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "```\n",
      "\n",
      "希望这可以帮到你。如果你有其他问题，请随时问我。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "没有 HUGGINGFACEHUB_API_TOKEN 可以在 HuggingFace 下载他人公开的模型吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b9068-a9ca-4c5b-a799-59bfa1eef760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您可以使用以下代码在 HuggingFace 中下载模型：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "```\n",
      "这将下载 BERT 模型和 tokenizer。\n",
      "\n",
      "---\n",
      "\n",
      "您可以使用以下代码在 HuggingFace 中下载模型：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
      "```\n",
      "这将下载 BERT 模型和 tokenizer。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "怎么用 Python 在 HuggingFace 下载模型？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c51e70-7c87-445a-a8cd-52c43528dc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "您可以使用以下代码在 HuggingFace 中断点续传下载模型：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", resume_download=True)\n",
      "```\n",
      "这将下载 BERT 模型和 tokenizer，并在下载过程中断点续传。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://zhuanlan.zhihu.com/p/475260268 \"https://zhuanlan.zhihu.com/p/475260268\"\n",
      "\n",
      "您可以使用以下代码在 HuggingFace 中断点续传下载模型：\n",
      "```\n",
      "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
      "\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", resume_download=True)\n",
      "```\n",
      "这将下载 BERT 模型和 tokenizer，并在下载过程中断点续传。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "在 HuggingFace 下载模型，如何断点续传？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74933778-7caa-4730-9fb8-947cfbc3e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 模型可以分成多个文件，其中包括模型权重和其他模型参数。如果您的模型是由多个子模型组成的，则可以将每个子模型的权重和参数保存在单独的文件中。这些文件可以使用 PyTorch 的 state_dict 和 optimizer 保存在同一个文件中[^9^]. \n",
      "\n",
      "您可以使用以下代码将模型保存到多个文件中：\n",
      "\n",
      "```python\n",
      "torch.save(model.state_dict(), PATH)\n",
      "```\n",
      "\n",
      "其中，`model` 是您要保存的 PyTorch 模型，`PATH` 是您要保存的文件路径。这将会把模型权重和其他参数保存到一个名为 `PATH` 的文件中。\n",
      "\n",
      "如果您想要加载这些文件并恢复模型，则可以使用以下代码：\n",
      "\n",
      "```python\n",
      "model = TheModelClass(*args, **kwargs)\n",
      "model.load_state_dict(torch.load(PATH))\n",
      "```\n",
      "\n",
      "其中，`TheModelClass` 是您要恢复的 PyTorch 模型类，`*args` 和 `**kwargs` 是用于初始化模型的参数。这将会从名为 `PATH` 的文件中加载模型权重和其他参数，并将它们加载到新创建的模型实例中。\n",
      "\n",
      "希望这可以帮助您！请让我知道是否还有其他问题需要解答。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://blog.csdn.net/qq_39686950/article/details/116090826 \"Windows系统下把PyTorch模型转为ncnn模型流程 - CSDN博客\"\n",
      "[2]: https://blog.csdn.net/season77us/article/details/104311195 \"如何下载Hugging Face 模型（pytorch_model.bin, config.json, vocab.txt）以及如何在 ...\"\n",
      "[3]: https://www.cnblogs.com/cxq1126/p/14277134.html \"将tensorflow版本的.ckpt模型转成pytorch的.bin模型 - 最咸的鱼 - 博客园\"\n",
      "[4]: https://zhuanlan.zhihu.com/p/620688513 \"Pytorch格式 .pt .pth .bin 详解 - 知乎 - 知乎专栏\"\n",
      "[5]: https://blog.csdn.net/znevegiveup1/article/details/118093294 \"读取pytorch.bin权重文件解读_pytorch_model.bin_唐僧爱吃唐僧肉的博客-CSDN博客\"\n",
      "[6]: https://blog.csdn.net/A_water_/article/details/128197563 \"Pytorch框架的模型pth文件转换成C++ OpenVINO框架的bin和xml文件并运行_pth转bin_A_water_的博客-CSDN博客\"\n",
      "[7]: https://zhuanlan.zhihu.com/p/113694038 \"PyTorch分布式训练简明教程(2022更新版) - 知乎 - 知乎专栏\"\n",
      "[8]: https://zhuanlan.zhihu.com/p/82038049 \"PyTorch | 保存和加载模型 - 知乎\"\n",
      "[9]: https://pytorch.org/tutorials/recipes/recipes/saving_multiple_models_in_one_file.html \"Saving and loading multiple models in one file using PyTorch\"\n",
      "\n",
      "PyTorch 模型可以分成多个文件，其中包括模型权重和其他模型参数。如果您的模型是由多个子模型组成的，则可以将每个子模型的权重和参数保存在单独的文件中。这些文件可以使用 PyTorch 的 state_dict 和 optimizer 保存在同一个文件中[^1^][9]. \n",
      "\n",
      "您可以使用以下代码将模型保存到多个文件中：\n",
      "\n",
      "```python\n",
      "torch.save(model.state_dict(), PATH)\n",
      "```\n",
      "\n",
      "其中，`model` 是您要保存的 PyTorch 模型，`PATH` 是您要保存的文件路径。这将会把模型权重和其他参数保存到一个名为 `PATH` 的文件中。\n",
      "\n",
      "如果您想要加载这些文件并恢复模型，则可以使用以下代码：\n",
      "\n",
      "```python\n",
      "model = TheModelClass(*args, **kwargs)\n",
      "model.load_state_dict(torch.load(PATH))\n",
      "```\n",
      "\n",
      "其中，`TheModelClass` 是您要恢复的 PyTorch 模型类，`*args` 和 `**kwargs` 是用于初始化模型的参数。这将会从名为 `PATH` 的文件中加载模型权重和其他参数，并将它们加载到新创建的模型实例中。\n",
      "\n",
      "希望这可以帮助您！请让我知道是否还有其他问题需要解答。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "PyTorch 模型可以分成好几个 bin 文件吗？\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d02b7-cee7-4a72-894d-76120fffee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 58231.14 ms /   136 tokens (  428.17 ms per token)\n",
      "llama_print_timings:        eval time =  1743.42 ms /     1 runs   ( 1743.42 ms per run)\n",
      "llama_print_timings:       total time = 60005.23 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 178540.67 ms /   472 tokens (  378.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 178591.84 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 41458.73 ms /   139 tokens (  298.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 41490.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13792.71 ms /    51 tokens (  270.45 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13806.36 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 64205.43 ms /   192 tokens (  334.40 ms per token)\n",
      "llama_print_timings:        eval time =   393.32 ms /     1 runs   (  393.32 ms per run)\n",
      "llama_print_timings:       total time = 64615.16 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 312641.94 ms /   458 tokens (  682.62 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 312700.26 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 428699.27 ms /   478 tokens (  896.86 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 428799.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 268943.46 ms /   309 tokens (  870.37 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 268984.75 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 428036.12 ms /   648 tokens (  660.55 ms per token)\n",
      "llama_print_timings:        eval time =   426.99 ms /     1 runs   (  426.99 ms per run)\n",
      "llama_print_timings:       total time = 428658.90 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 10771.61 ms /    30 tokens (  359.05 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 10808.14 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 77845.98 ms /   109 tokens (  714.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 77920.76 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 31191.36 ms /   103 tokens (  302.83 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 31233.61 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 125583.51 ms /   111 tokens ( 1131.38 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 125624.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 26138.27 ms /    68 tokens (  384.39 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 26169.78 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 12869.16 ms /    35 tokens (  367.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 12877.02 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 349920.02 ms /   504 tokens (  694.29 ms per token)\n",
      "llama_print_timings:        eval time =  1198.45 ms /     1 runs   ( 1198.45 ms per run)\n",
      "llama_print_timings:       total time = 351175.09 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 235384.79 ms /   158 tokens ( 1489.78 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 235445.03 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 540504.96 ms /   542 tokens (  997.24 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 540623.58 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 351235.10 ms /   719 tokens (  488.51 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 351388.48 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 50923.63 ms /   165 tokens (  308.63 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 51015.38 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 184992.04 ms /   272 tokens (  680.12 ms per token)\n",
      "llama_print_timings:        eval time =   411.72 ms /     1 runs   (  411.72 ms per run)\n",
      "llama_print_timings:       total time = 185451.72 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 194429.41 ms /   622 tokens (  312.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 194464.86 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 113118.77 ms /   366 tokens (  309.07 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 113145.92 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 687028.24 ms /   728 tokens (  943.72 ms per token)\n",
      "llama_print_timings:        eval time =   439.94 ms /     1 runs   (  439.94 ms per run)\n",
      "llama_print_timings:       total time = 687586.24 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 189149.69 ms /   533 tokens (  354.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 189193.56 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 191291.91 ms /   560 tokens (  341.59 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 191325.52 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 262665.41 ms /   725 tokens (  362.30 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 262699.98 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 446825.63 ms /   744 tokens (  600.57 ms per token)\n",
      "llama_print_timings:        eval time =  2820.09 ms /     1 runs   ( 2820.09 ms per run)\n",
      "llama_print_timings:       total time = 449699.31 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 370878.55 ms /   717 tokens (  517.26 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 370985.32 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 316657.40 ms /   647 tokens (  489.42 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 316701.04 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 355381.24 ms /   504 tokens (  705.12 ms per token)\n",
      "llama_print_timings:        eval time =   426.32 ms /     1 runs   (  426.32 ms per run)\n",
      "llama_print_timings:       total time = 355866.70 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 306588.42 ms /   559 tokens (  548.46 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 306644.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 388570.23 ms /   565 tokens (  687.73 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 388642.65 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 123319.83 ms /   318 tokens (  387.80 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 123356.07 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 329182.49 ms /   811 tokens (  405.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 329242.86 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 210174.59 ms /   584 tokens (  359.89 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 210206.57 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 81113.52 ms /   286 tokens (  283.61 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 81134.54 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 439253.03 ms /   727 tokens (  604.20 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 439387.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 286354.39 ms /   751 tokens (  381.30 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 286420.33 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 444201.68 ms /   675 tokens (  658.08 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 444267.37 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 607155.44 ms /   620 tokens (  979.28 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 607229.15 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 674110.08 ms /   708 tokens (  952.13 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 674200.48 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 329078.28 ms /   470 tokens (  700.17 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 329136.29 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 774982.77 ms /   734 tokens ( 1055.83 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 775084.90 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1736656.90 ms /   767 tokens ( 2264.22 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1736891.33 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1348035.02 ms /   608 tokens ( 2217.16 ms per token)\n",
      "llama_print_timings:        eval time = 24584.54 ms /     1 runs   (24584.54 ms per run)\n",
      "llama_print_timings:       total time = 1372887.22 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1395140.63 ms /   472 tokens ( 2955.81 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1395354.86 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2404735.92 ms /   808 tokens ( 2976.16 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2405157.08 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1788561.84 ms /   640 tokens ( 2794.63 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1788744.43 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2043317.53 ms /   690 tokens ( 2961.33 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2043549.39 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1295899.29 ms /   402 tokens ( 3223.63 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1296069.46 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1882245.78 ms /   592 tokens ( 3179.47 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1882491.04 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2343689.96 ms /   741 tokens ( 3162.87 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2343993.29 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1234739.82 ms /   490 tokens ( 2519.88 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1234957.50 ms\n",
      "\n",
      "llama_print_timings:        load time =  8850.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 86554.06 ms /    53 tokens ( 1633.10 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 86652.29 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "def get_docs(dir_name):\n",
    "    # (1) Import a series of documents.\n",
    "    loader = DirectoryLoader(dir_name, loader_cls=TextLoader, silent_errors=True)\n",
    "    raw_documents = loader.load()\n",
    "    # (2) Split them into small chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=128,\n",
    "    )\n",
    "    return text_splitter.split_documents(raw_documents)\n",
    "\n",
    "def ingest_docs(dir_name):\n",
    "    documents = get_docs(dir_name)\n",
    "    # (3) Create embeddings for each document (using text-embedding-ada-002).\n",
    "    embeddings = LlamaCppEmbeddings(model_path=os.path.expanduser('~/ggml-model-q4_1.bin'), n_ctx=1024)\n",
    "    return FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vectorstore = ingest_docs('_posts/ultimate-facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609ba07-3ee0-45d6-9174-b078e8b12b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Save vectorstore\n",
    "with open('vectorstore_13B_1024.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d0107-4a76-40aa-8458-87b249098303",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1024\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 1600.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Load vectorstore\n",
    "with open('vectorstore_13B_1024.pkl', 'rb') as f:\n",
    "    vectorstore = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9302b-c6bc-4418-af48-3dcc81a0df74",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '你知道什么？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678ed81-b185-4d76-873a-6978112900de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='态度：认同、欣赏、尊重、重视、轻视、忽视、鄙视、反对\\n客体所做：我对你的态度、你对我的态度、你对他的态度\\n主体所想：我对你的态度、你对我的态度\\n主体所说：我对他的态度、他对我的态度、他对你的态度\\n对于某些次态度的记忆、拟构；怀疑、与、确信；\\n你对我的态度、有正有误；作为某种主体效果的客体态度，对于客体态度的目标、意向；对于态度的态度、有正有误；\\n渴望被重视、被认可、被理解、被公正地对待；\\n虚荣；自负，轻视，反对、有正有误，对于他人的误解；对于自己态度的温和的怀疑；苏格拉底式教学法；偏见、综合比较，是，某种轻视；\\n我对客体的态度：你对我的态度、你的确信，我的确信、我对你的态度；确信、对于确信的态度；我对自己的态度，耻辱之恨、丑陋之恨\\n对于某种经历的认同。对于某种人生概括的认同。省略主语、所有格。怕丑。\\n注意，是，自由的。？我是谁？' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n",
      "page_content='> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='---\\n\\n> \\u3000\\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\\n\\u3000\\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\\n\\u3000\\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15863.30 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 15862.61 ms /     8 tokens ( 1982.83 ms per token)\n",
      "llama_print_timings:        eval time = 23200.83 ms /     1 runs   (23200.83 ms per run)\n",
      "llama_print_timings:       total time = 39067.66 ms\n"
     ]
    }
   ],
   "source": [
    "# Get context related to the question from the embedding model\n",
    "for context in vectorstore.similarity_search(question):\n",
    "    print(f'{context}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a724c2-cc43-496e-a033-4e36d43204a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Callback function to stream answers to stdout.\n",
    "manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "streaming_llm = ChatOpenAI(streaming=True, callback_manager=manager, verbose=True, temperature=0)\n",
    "question_gen_llm = ChatOpenAI(temperature=0, verbose=True, callback_manager=manager)\n",
    "# Prompt to generate independent questions by incorporating chat history and a new question.\n",
    "question_generator = LLMChain(llm=question_gen_llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "# Pass in documents and a standalone prompt to answer questions.\n",
    "doc_chain = load_qa_chain(streaming_llm, chain_type='stuff', prompt=QA_PROMPT)\n",
    "# Generate prompts from embedding model.\n",
    "qa = ConversationalRetrievalChain(retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0cbcf-52f1-4cc4-90d9-20940a65785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4f09f8-3fbc-41ff-aa6c-e7b86c78409f",
   "metadata": {},
   "source": [
    "---\n",
    "Utilizing [IPyMock](https://github.com/seii-saintway/ipymock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284e7c6e-f8bd-4302-9be9-e5f3dd32048a",
   "metadata": {},
   "source": [
    "在 PyTest 中，可以使用 `monkeypatch` 来替换导入的库，从而进行测试。 `monkeypatch` 是一个 pytest 内置的 fixture，用于替换运行时的变量和对象，以便在测试期间使用自定义值。\n",
    "\n",
    "对于要替换的库，可以使用 `pytest_mock` fixture 来自动将其传递给 `monkeypatch`。如果没有使用 `pytest_mock`，则需要手动使用 `monkeypatch.setattr()` 方法来替换导入的库。\n",
    "\n",
    "以下是一个示例，假设我们有一个名为 `example.py` 的模块，它导入了 `requests` 库，并使用了该库的 `get()` 方法发送网络请求：\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def get_example_data():\n",
    "    response = requests.get('https://example.com')\n",
    "    return response.content\n",
    "```\n",
    "\n",
    "要测试这个函数，我们需要使用 Dummy 对象替换 requests 库，以便我们可以模拟网络请求的响应。可以使用以下代码进行测试：\n",
    "\n",
    "```python\n",
    "import pytest\n",
    "import example\n",
    "\n",
    "class DummyResponse:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_requests(monkeypatch):\n",
    "    def mock_get(*args, **kwargs):\n",
    "        return DummyResponse(b'Test data')\n",
    "    monkeypatch.setattr(requests, 'get', mock_get)\n",
    "\n",
    "def test_get_example_data(mock_requests):\n",
    "    data = example.get_example_data()\n",
    "    assert data == b'Test data'\n",
    "```\n",
    "\n",
    "在这个示例中，我们首先定义了一个名为 `DummyResponse` 的类，它代表了一个虚拟的 requests 库的响应。然后我们定义了一个名为 `mock_requests` 的 fixture，它使用 `monkeypatch.setattr()` 方法替换了 requests 库的 `get()` 方法，使其返回我们定义的 DummyResponse 对象。\n",
    "\n",
    "最后，我们定义了一个测试函数 `test_get_example_data`，并将 `mock_requests` fixture 作为参数传递给它。在测试函数内部，我们调用 `example.get_example_data()`，这将调用我们已经用 Dummy 对象替换的 requests 库的 `get()` 方法，最终返回我们定义的虚拟响应。\n",
    "\n",
    "这样我们就可以在测试中使用 Dummy 对象替换任何库，以便更好地控制测试环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921df0fe-c64d-417c-bb06-36b385f48489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e2186-6d1c-45ed-ad49-9f9a76b6dbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method create in module openai.api_resources.chat_completion:\n",
      "\n",
      "create(*args, **kwargs) method of builtins.type instance\n",
      "    Creates a new chat completion for the provided messages and parameters.\n",
      "    \n",
      "    See https://platform.openai.com/docs/api-reference/chat-completions/create\n",
      "    for a list of valid parameters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(openai.ChatCompletion.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a773d9b-a86a-43a3-bc58-bff0b7c010c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "from revChatGPT.V1 import Chatbot, configure\n",
    "\n",
    "# Open the JSON file and read the conversation_id\n",
    "with open(os.path.expanduser('~/.config/revChatGPT/config.json'), 'r') as f:\n",
    "    conversation_id = json.load(f).get('conversation_id', None)\n",
    "\n",
    "bot = Chatbot(\n",
    "    config = configure(),\n",
    "    conversation_id = conversation_id,\n",
    "    lazy_loading = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca023a-1920-47bf-af3e-072902cb9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade git+https://github.com/seii-saintway/ipymock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764d475-39ca-4f22-9874-2e66d74ab17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f4bc4-f8a6-4f09-bdd6-cb057729bc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2833b9f0-d47e-43a7-a661-2eb7eae9cbfd",
   "metadata": {},
   "source": [
    "* [How to stream completions?](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_stream_completions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e3940f-5ddf-4bd1-93fc-1c642dc29324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta(prompt):\n",
    "    res = ''\n",
    "    for response in bot.ask(prompt):\n",
    "        # IPython.display.display(IPython.core.display.Markdown(response['message']))\n",
    "        # IPython.display.clear_output(wait=True)\n",
    "        yield {\n",
    "            'choices': [\n",
    "                {\n",
    "                    'index': 0,\n",
    "                    'delta': {\n",
    "                        'content': response['message'][len(res):],\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "        res = response['message']\n",
    "\n",
    "def mock_create(*args, **kwargs):\n",
    "    for message in kwargs['messages']:\n",
    "        if message['role'] == 'user':\n",
    "            break\n",
    "    else:\n",
    "        return {\n",
    "            'choices': [{}],\n",
    "        }\n",
    "\n",
    "    if kwargs.get('stream', False):\n",
    "        return delta(message['content'])\n",
    "\n",
    "    for response in bot.ask(message['content']):\n",
    "        # IPython.display.display(IPython.core.display.Markdown(response['message']))\n",
    "        # IPython.display.clear_output(wait=True)\n",
    "        pass\n",
    "    return {\n",
    "        'choices': [\n",
    "            {\n",
    "                'finish_reason': 'stop',\n",
    "                'index': 0,\n",
    "                'message': {\n",
    "                    'content': response['message'],\n",
    "                    'role': 'assistant',\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b38c7be-9920-4ee5-86c1-673a727799ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pytest.fixture\n",
    "def mock_openai(monkeypatch):\n",
    "    monkeypatch.setattr(openai.ChatCompletion, 'create', mock_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adddf41-62e7-4a3e-aabb-d3873e7417ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '终极真实是什么？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36ef979-6995-4e60-8425-f876c6b6978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e820588-95fc-4bfd-bc5d-3192f0c2f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_qa(mock_openai):\n",
    "    global answer\n",
    "    answer = qa({'question': question, 'chat_history': []})\n",
    "    print('\\n')\n",
    "    assert isinstance(answer, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74356395-dbcd-492b-93a3-090cc5e8d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymock import do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd3ac66-3ba7-465b-b6e9-640d4700c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> no.0  ::source::test_qa  setup  passed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 14550.38 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 23539.97 ms /    15 tokens ( 1569.33 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 23551.68 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author argues that to study the interconnections between subjective experiences and physical processes, we need to move away from purely introspective methods and embrace physiological psychology. Only through this approach can we achieve a complete understanding of psychological phenomena. The author also suggests that consciousness is inextricably linked to physical processes and can be affected by them.\n",
      "\n",
      "=> no.0  ::source::test_qa  runtest  passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do(\n",
    "    mock_openai=mock_openai,\n",
    "    test_qa=test_qa,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dfaf8e-837c-4fab-83c6-618200b85ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '终极真实是什么？',\n",
       " 'chat_history': [],\n",
       " 'answer': 'The author argues that to study the interconnections between subjective experiences and physical processes, we need to move away from purely introspective methods and embrace physiological psychology. Only through this approach can we achieve a complete understanding of psychological phenomena. The author also suggests that consciousness is inextricably linked to physical processes and can be affected by them.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
