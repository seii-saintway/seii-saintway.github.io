{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e15ed834-72b3-4ede-ad4c-1b452761eb2e",
   "metadata": {},
   "source": [
    "## [LangChain GitHub](https://github.com/hwchase17/langchain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01737f6f-13f5-416a-8a89-cab5b0375c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda72cf-87dd-4c4c-9ff5-658097dc47ab",
   "metadata": {},
   "source": [
    "---\n",
    "* [Agents](https://python.langchain.com/en/latest/modules/agents/tools/getting_started.html)\n",
    "  * [Hierarchical Planning Agent](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/openapi.html)\n",
    "  * [Python Agent](https://python.langchain.com/en/latest/modules/agents/toolkits/examples/python.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6c0e8-42ab-428b-8a89-c5ffa2777757",
   "metadata": {},
   "source": [
    "* [Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback](https://arxiv.org/abs/2302.12813)\n",
    "* [Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f280ac-f7ca-45ee-b07e-d50ef9a201fb",
   "metadata": {},
   "source": [
    "* [Elastic Versatile Agent with LangChain](https://github.com/corca-ai/EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f646fa0",
   "metadata": {},
   "source": [
    "* https://github.com/pHaeusler/micro-agent\n",
    "* https://github.com/SamPink/dev-gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc48289",
   "metadata": {},
   "source": [
    "* https://github.com/yoheinakajima/babyagi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25b615c-ab1b-46e2-8562-ee5526997066",
   "metadata": {},
   "source": [
    "---\n",
    "* [Chatbots](https://python.langchain.com/en/latest/use_cases/chatbots.html)\n",
    "  * [Memory](https://python.langchain.com/en/latest/modules/memory/how_to_guides.html)\n",
    "    * [ConversationBufferMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer.html)\n",
    "    * [ConversationBufferWindowMemory](https://python.langchain.com/en/latest/modules/memory/types/buffer_window.html)\n",
    "  * [Conversation Agent](https://python.langchain.com/en/latest/modules/agents/agents/examples/conversational_agent.html)\n",
    "  * [GPT with Google Search](https://github.com/andylokandy/gpt-4-search)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d4893-7114-4e16-9b8c-ce3a6651ae79",
   "metadata": {},
   "source": [
    "---\n",
    "* [Chatbot for LangChain](https://blog.langchain.dev/langchain-chat/)\n",
    "* [Code of Chatbot for LangChain](https://github.com/hwchase17/chat-langchain)\n",
    "  * [Key Code of Chatbot for LangChain](https://github.com/hwchase17/chat-langchain/blob/master/query_data.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab8a37-b9ac-4aa0-b35f-e2ad609166de",
   "metadata": {},
   "source": [
    "---\n",
    "[ReadTheDocs Documentation Loader](https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/readthedocs_documentation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e88a2f9-2e0b-4ad4-97fd-088495c5f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget -r -A .html -P rtdocs https://langchain.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fabd40d-46e6-4643-98b8-812f0def88bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='.rst\\n.pdf\\nWelcome to LangChain\\n Contents \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nWelcome to LangChain#\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an API, but will also:\\nBe data-aware: connect a language model to other sources of data\\nBe agentic: allow a language model to interact with its environment\\nThe LangChain framework is designed with the above principles in mind.\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\nGetting Started#\\nCheckout the below guide for a walkthrough of how to get started using LangChain to create an Language Model application.\\nGetting Started Documentation\\nModules#\\nThere are several main modules that LangChain provides support for.\\nFor each module we provide some examples to get started, how-to guides, reference docs, and conceptual guides.\\nThese modules are, in increasing order of complexity:\\nModels: The various model types and model integrations LangChain supports.\\nPrompts: This includes prompt management, prompt optimization, and prompt serialization.\\nMemory: Memory is the concept of persisting state between calls of a chain/agent. LangChain provides a standard interface for memory, a collection of memory implementations, and examples of chains/agents that use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nCode Understanding: If you want to understand how to use LLMs to query source code from github, you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously hard to evaluate with traditional metrics. One new way of evaluating them is using language models themselves to do the evaluation. LangChain provides some prompts/chains for assisting in this.\\nReference Docs#\\nAll of LangChain’s reference documentation, in one place. Full documentation on all methods, classes, installation methods, and integration setups for LangChain.\\nReference Documentation\\nLangChain Ecosystem#\\nGuides for how other companies/products can be used with LangChain\\nLangChain Ecosystem\\nAdditional Resources#\\nAdditional collection of resources we think may be useful as you develop your application!\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.\\nGlossary: A glossary of all related terms, papers, methods, etc. Whether implemented in LangChain or not!\\nGallery: A collection of our favorite projects that use LangChain. Useful for finding inspiration or seeing how things were done in other applications.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nTracing: A guide on using tracing in LangChain to visualize the execution of chains and agents.\\nModel Laboratory: Experimenting with different prompts, models, and chains is a big part of developing the best possible application. The ModelLaboratory makes it easy to do so.\\nDiscord: Join us on our Discord to discuss all things LangChain!\\nProduction Support: As you move your LangChains into production, we’d love to offer more comprehensive support. Please fill out this form and we’ll set up a dedicated support Slack channel.\\nnext\\nQuickstart Guide\\n Contents\\n  \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nLangChain Ecosystem\\nAdditional Resources\\nBy Harrison Chase\\n    \\n      © Copyright 2023, Harrison Chase.\\n      \\n  Last updated on Apr 14, 2023.\\n  ', metadata={'source': 'rtdocs/langchain.readthedocs.io/en/latest/index.html'})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import ReadTheDocsLoader\n",
    "ReadTheDocsLoader('rtdocs', features='lxml').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f0a4ee-2322-409c-8a3c-3bd0d0c86350",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf rtdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0887546-ff3e-4ed2-bdd6-103384669ae7",
   "metadata": {},
   "source": [
    "---\n",
    "* [ChatGPT Chatbot for Internal Use](https://zenn.dev/tatsui/articles/langchain-chatbot)\n",
    "  * https://github.com/tatsu-i/chatbot-sample\n",
    "\n",
    "TODO:\n",
    "* 提示模版的中文化\n",
    "* 启示、自我提示、自我认知的模型化？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc799d5c-2ef5-42c5-a180-428e42d0d648",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "Have you ever encountered a problem when using ChatGPT to search for the latest information? The current language model of ChatGPT (gpt-3.5-turbo-0301) was trained on data up until September 2021, so it may not be able to answer questions about the latest information accurately.\n",
    "\n",
    "In this article, we will explain how to create a chatbot that can use chain of thought to respond, by teaching ChatGPT new knowledge.\n",
    "\n",
    "### Preparing and importing training data\n",
    "\n",
    "First, clone a repository as training data.\n",
    "\n",
    "Next, import the repository files as text files using the following code, which contains the OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae0c2d-5c92-4076-8b6c-bc3857cae8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "\n",
    "def get_docs(dir_name):\n",
    "    # (1) Import a series of documents.\n",
    "    loader = DirectoryLoader(dir_name, loader_cls=TextLoader, silent_errors=True)\n",
    "    raw_documents = loader.load()\n",
    "    # (2) Split them into small chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    return text_splitter.split_documents(raw_documents)\n",
    "\n",
    "def ingest_docs(dir_name):\n",
    "    documents = get_docs(dir_name)\n",
    "    # (3) Create embeddings for each document (using text-embedding-ada-002).\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    return FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vectorstore = ingest_docs('_posts/ultimate-facts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9ff8c0-56a2-474a-928c-b48da4b7eb57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678ec6c-2cf0-4d4e-b4d2-d64bdedef2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from EdgeGPT import Chatbot as Bing, ConversationStyle\n",
    "\n",
    "bing = Bing(cookiePath = os.path.expanduser('~/.config/EdgeGPT/cookies.json'))\n",
    "\n",
    "async def ask(prompt):\n",
    "    res = (await bing.ask(\n",
    "        prompt = prompt,\n",
    "        conversation_style = ConversationStyle.balanced,\n",
    "    ))['item']['messages'][1]\n",
    "\n",
    "    print(res['text'])\n",
    "    print('\\n---\\n')\n",
    "    print(res['adaptiveCards'][0]['body'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8310a5a-09f5-47ce-9fdf-7696c6d14b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-embedding-ada-002 是 OpenAI 的一个新的嵌入模型，它替换了五个用于文本搜索、文本相似度和代码搜索的单独模型，并在大多数任务上优于我们以前最强大的模型 Davinci，同时价格降低了 99.8%[^1^]。您可以通过向嵌入 API 端点发送文本字符串以及选择嵌入模型 ID（例如 text-embedding-ada-002）来获取嵌入[^2^]。\n",
      "\n",
      "---\n",
      "\n",
      "[1]: https://openai.com/blog/new-and-improved-embedding-model/ \"New and improved embedding model - openai.com\"\n",
      "[2]: https://platform.openai.com/docs/guides/embeddings \"Embeddings - OpenAI API\"\n",
      "\n",
      "text-embedding-ada-002 是 OpenAI 的一个新的嵌入模型，它替换了五个用于文本搜索、文本相似度和代码搜索的单独模型，并在大多数任务上优于我们以前最强大的模型 Davinci，同时价格降低了 99.8%[^1^][1]。您可以通过向嵌入 API 端点发送文本字符串以及选择嵌入模型 ID（例如 text-embedding-ada-002）来获取嵌入[^2^][2]。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "await ask('''\n",
    "text-embedding-ada-002 是什么？\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa3c23-1aa7-495e-a93c-72d919c1115c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01242e27-5829-41ee-b019-80833c3430f4",
   "metadata": {},
   "source": [
    "### Creating a chatbot\n",
    "\n",
    "Now, we will create a simple chatbot using the LLM chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952bf025-9785-47d3-8e31-eac4550e0a63",
   "metadata": {},
   "source": [
    "```\n",
    "/usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:191: UserWarning: `ChatVectorDBChain` is deprecated - please use `from langchain.chains import ConversationalRetrievalChain`\n",
    "```\n",
    "\n",
    "* [LangChain's Retrieval Plugin](https://blog.langchain.dev/retrieval/)\n",
    "* [Chat Over Documents with Chat History](https://python.langchain.com/en/latest/modules/chains/index_examples/chat_vector_db.html)\n",
    "\n",
    "---\n",
    "LangChain GitHub\n",
    "\n",
    "* [LangChain Document Loaders](https://github.com/hwchase17/langchain/tree/master/langchain/document_loaders)\n",
    "  * [Loader that uses Selenium to load URLs](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/url_selenium.py)\n",
    "  * [Loader that uses Playwright to load URLs](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/url_playwright.py)\n",
    "  * [Loads files from a Git repository](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/git.py)\n",
    "  * [Loader that loads .ipynb notebook files](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/notebook.py)\n",
    "  * [Load HTML files of URLs](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/url.py)\n",
    "    => [from unstructured.partition.auto import partition](https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/partition/auto.py)\n",
    "  * [Read tweets of user twitter handle](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/twitter.py)\n",
    "  * [Load data from Google Drive](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/googledrive.py)\n",
    "  * [Load PDF files](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/pdf.py)\n",
    "    * todo: https://arxiv.org/pdf/2304.03442.pdf\n",
    "  * [Load Python files as text files](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/python.py)\n",
    "  * [Load EPub files as unstructured files](https://github.com/hwchase17/langchain/blob/master/langchain/document_loaders/epub.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793a1d1-95d2-483a-b5c9-ca2c9d72f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.callbacks.base import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains.chat_vector_db.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores.base import VectorStore\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Callback function to stream answers to stdout.\n",
    "manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "streaming_llm = ChatOpenAI(streaming=True, callback_manager=manager, verbose=True, temperature=0)\n",
    "question_gen_llm = ChatOpenAI(temperature=0, verbose=True, callback_manager=manager)\n",
    "# Prompt to generate independent questions by incorporating chat history and a new question.\n",
    "question_generator = LLMChain(llm=question_gen_llm, prompt=CONDENSE_QUESTION_PROMPT)\n",
    "# Pass in documents and a standalone prompt to answer questions.\n",
    "doc_chain = load_qa_chain(streaming_llm, chain_type='stuff', prompt=QA_PROMPT)\n",
    "# Generate prompts from embedding model.\n",
    "qa = ConversationalRetrievalChain(retriever=vectorstore.as_retriever(), combine_docs_chain=doc_chain, question_generator=question_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11a4bf3-d68b-473f-96eb-26cdbd1d4217",
   "metadata": {},
   "source": [
    "The prompt given to ChatGPT's API is created in the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b87ff-a2d6-4424-a1e8-b5711d71bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given context does not provide information about Remix or any existing frameworks, so it is not possible to answer this question."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What makes Remix different from existing frameworks? Please list in bullet points in English.',\n",
       " 'chat_history': [],\n",
       " 'answer': 'The given context does not provide information about Remix or any existing frameworks, so it is not possible to answer this question.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'What makes Remix different from existing frameworks? Please list in bullet points in English.'\n",
    "qa({'question': question, 'chat_history': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c229c0-c0ec-40c6-b554-949f20334e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The context provided is a collection of excerpts from different sources and it is not clear what specific information is being referred to."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '你知道什么？',\n",
       " 'chat_history': [],\n",
       " 'answer': \"I don't know. The context provided is a collection of excerpts from different sources and it is not clear what specific information is being referred to.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '你知道什么？'\n",
    "qa({'question': question, 'chat_history': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b022d1c-6330-4982-a454-1252886ca58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an AI language model and I only have access to the information provided in the context above. From the context, it discusses topics such as the nature of science, psychology, philosophy, and religion. It also touches on concepts such as truth, reality, consciousness, and perception. However, it is difficult to provide a specific answer to your question because it is very broad. Please provide more context or a specific question for me to answer."
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': '你知道什么？',\n",
       " 'chat_history': [],\n",
       " 'answer': 'I am an AI language model and I only have access to the information provided in the context above. From the context, it discusses topics such as the nature of science, psychology, philosophy, and religion. It also touches on concepts such as truth, reality, consciousness, and perception. However, it is difficult to provide a specific answer to your question because it is very broad. Please provide more context or a specific question for me to answer.'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '你知道什么？'\n",
    "qa({'question': question, 'chat_history': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60aae1b-f708-4291-ada7-f5479b0af1d7",
   "metadata": {},
   "source": [
    "### Ask a question\n",
    "\n",
    "```\n",
    "How is Remix different from existing frameworks?\n",
    "Please list in bullet points in Japanese.\n",
    "```\n",
    "\n",
    "The following bullet points will be output as an answer summarizing the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2afac-0775-48a7-966e-ff43b38cda40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\\n> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='真理、真实；神造真实、人造真实；真实，想象；记忆，拟构。\\n如果哲学更像「真理」，那么各类科学就更像「真实」。如果物理学更像「真理」，那么化学就更像「真实」。如果化学更像「真理」，那么生物学、生理学就更像「真实」。如果生理学更像「真理」，那么脑科学、神经科学就更像「真实」。\\n如果理科更像「神造真实」，那么工科就更像「人造真实」。如果生理学更像「神造真实」，那么医学、药学就更像「人造真实」。\\n\\n---\\n\\n> 我只是一个碳族生物；一个土生土长的地球人¹。贵主耶稣是灵族人吧。而木星上的风暴²可以拥有怎样的生命和意识呢？贵主耶稣在木星上能否与木星人一起生活和交往呢？\\n\\n> ¹ 地球人 => 《费曼讲座：宇称不守恒定律和如何与外星人交流》\\n\\n> ² 风暴 =>\\n> 当那一天，到了傍晚，耶稣对他们说：『我们渡到那边去吧。』他们就离开群众，照他在船上的情况把他带走；还有别的船也跟他在一起。当下起了**大暴风**，波浪泼进船内，甚至船简直满了！耶稣竟在船尾上靠着枕头睡觉呢；门徒就叫醒了他，对他说：『老师，我们丧命，你不在意么？』耶稣醒起来，斥责³那风，向海说：『不要作声！噤默罢！』那风不狂吹，便大大平静了。耶稣对他们说：『为什么这么胆怯呢？怎么没有信心呢？』他们就大起了敬畏的心，直彼此说：『这个人到底是谁？连风和海也听从他！』\\n (马可福音 4:35-41 吕振中)\\n\\n> ³ 斥责 => 『ワンパンマン』：サイタマ？キング？\\n\\n↓↓--------------------继续修订--------------------↓↓\\n\\n圣经信仰之神经心理学实证纲领\\n父的自我信息，是指，对于圣灵的表征。\\n纯粹的圣经，含于，父的自我信息。\\n从「纯粹的基督徒」到「超基督徒」「超级赌徒」\\n\\n吾否认圣经中上帝的名，因为那是人们创造的。' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n",
      "page_content='> ³ 斥责 => 『ワンパンマン』：サイタマ？キング？\\n\\n↓↓--------------------继续修订--------------------↓↓\\n\\n圣经信仰之神经心理学实证纲领\\n父的自我信息，是指，对于圣灵的表征。\\n纯粹的圣经，含于，父的自我信息。\\n从「纯粹的基督徒」到「超基督徒」「超级赌徒」\\n\\n吾否认圣经中上帝的名，因为那是人们创造的。\\n\\n超越神论，不可知论；信仰；宁死不屈，抗争到底\\n对于神圣生命的信心？或，亲密关系？\\n坚贞，「甘愿承担自己的罪罚」是《古兰经》的价值所在。\\n真诚、勇敢、坚贞，耶稣的「甘愿承担」是《圣经》的价值所在。\\n\\n吾，若不是因为怯懦，又怎么会祷告呢？\\n所以，吾，应该要，放弃，那种、对于其他的心灵的畏惧、所联结着的祷告。以耶稣为榜样。\\n人子要经受三日地狱之火的洗。罪全部被烧尽了后，第三日复活。\\n我所爱慕的必定是父所喜爱的，因为父从始至终都在吸引我、塑造我的爱慕。\\n我所爱慕的若是父所不喜爱的，父必定会改变我。所以，我总是晓得父的喜爱。\\n人子，与父和好，与父为友，爱父并顺从祂。与父同在，就有勇气。与父同行，就有希望。\\n子永远与父同在，从未分离。\\n「吾要成为超人。」\\n「在吾的生活中显明父的荣耀。」\\n祷告，是，对于子灵的表征。\\n\\n感，分为，虚感、实感。\\n虚感，分为，信（？）、思感、愉快感、位置感。\\n实感，分为，色感、声感、香感、味感、触感、缩紧感、疼痛感、瘙痒感、冷热感。\\n\\n体，是指，广延。\\n\\n感、体，平行地，变化。\\n感、体，分割的，平行性原理\\n感的统合，预示着，体的核心。\\n体的核心：在感的集合中占比最大的体。\\n\\n信，是一种，感。（联结？极深的记忆？）\\n灵，是指，具有自我独特视角的、体。 => “我是谁？”\\n《圣经》说：信、灵，平行地，变化。\\n在苦难中持守坚忍为何能增加信心呢？' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n",
      "page_content='问题，是指，对于某次偶然变化的疑问。\\n解答，是指，对于某种必然变化的概括、对于某种偶然变化的适应措施。\\n\\n技术，是指，问题概括及其解法。\\n程序，是指，数据结构及其算法。\\n模型，是指，对于拟实的技术。\\n建模，是指，对于拟实的计划。\\n解模，是指，对于拟实的实施。\\n软件模型，是指，对于拟实的程序。\\n软件建模，是指，对于拟实的编程。\\n软件解模，是指，对于拟实的进程。\\n\\n模拟，分为，拟实、拟虚。\\n来原，是指，与模型对应的事实。\\n\\n当即行动，增强，对于某种偶然变化的适应力。\\n但，人会拖延、不愿儆醒\\n\\n独立、与、交通，对于联结的交通，更不朽的存在形式\\n更不朽的心思、身体，永远不朽的平安\\n\\n兴趣、快乐、生活情趣；佛学、哲学，作为，一种思维训练\\n\\n弊害、痛苦，错误、误信，有限的价值、终会朽坏；佛学、消极的哲学，作为，一种信仰；\\n忽视、漠视，无私、无我、虚空、无恥、不惭\\n去分别，就是，注视；不去分别，就是，漠视；\\n漠视伤害，导致着，忘记伤害\\n走向虚空，就是，放弃羞耻、光荣、尊贵、荣耀\\n佛学的惊奇性质的信心，导致着，漠视。\\n\\n---\\n\\n> 因为依顺着上帝而有的忧愁能生出不后悔的忏悔来、以至于得救；而世俗的忧愁却能生出死亡。\\n(哥林多后书 7:10 吕振中)\\n\\n「金刚经」的邪灵，完全地，杀死了，吾的心灵。\\n真常唯心系，曾经，在吾的心灵中，孕育，却流产了。\\n\\n忘罪，忘无明。\\n\\n积极的态度；佛教（真常唯心系）唯一的「用处」就是：让人不再惧怕死亡、平安地享受死亡\\n基督教，比，真常唯心系，更加清晰。\\n已成、与、未成；易信、与、难信；注意频次，信心，快乐、爱，恐惧、严肃，惊奇、敬畏；对于实感的表征之信，分别由惊奇（客观）、敬畏（主观）而来的信心\\n某些次表征的联结；「信」，意味着、某种与真实的关系，是一种、「成」' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get context related to the question from the embedding model\n",
    "for context in vectorstore.similarity_search(question):\n",
    "    print(f'{context}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f16280-a2cb-4bc5-86d9-6f6a54464878",
   "metadata": {},
   "source": [
    "```\n",
    "Remix's job is to cross the center of the stack and then get out of your way. We avoid as many \"Remixisms\" as possible and instead make it easier to use the standard APIs the web already has.\n",
    "\n",
    "This one is more for us. We've been educators for the 5 years before Remix. Our tagline is Build Better Websites. We also think of it with a little extra on the end: Build Better Websites, Sometimes with Remix. If you get good at Remix, you will accidentally get good at web development in general.\n",
    "\n",
    "Remix's APIs make it convenient to use the fundamental Browser/HTTP/JavaScript, but those technologies are not hidden from you.\n",
    "\n",
    "Additionally, if Remix doesn't have an adapter for your server already, you can look at the source of one of the adapters and build your own.\n",
    "\n",
    "## Server Framework\n",
    "If you're familiar with server-side MVC web frameworks like Rails and Laravel, Remix is the View and Controller, but it leaves the Model up to you. There are a lot of great databases, ORMs, mailers, etc. in the JavaScript ecosystem to fill that space. Remix also has helpers around the Fetch API for cookie and session management.\n",
    "\n",
    "Instead of having a split between View and Controller, Remix Route modules take on both responsibilities.\n",
    "\n",
    "Most server-side frameworks are \"model focused\". A controller manages multiple URLs for a single model.\n",
    "\n",
    "## Welcome to Remix!\n",
    "We are happy you're here!\n",
    "\n",
    "Remix is a full stack web framework that lets you focus on the user interface and work back through web fundamentals to deliver a fast, slick, and resilient user experience that deploys to any Node.js server and even non-Node.js environments at the edge like Cloudflare Workers.\n",
    "\n",
    "Want to know more? Read the Technical Explanation of Remix\n",
    "\n",
    "This repository contains the Remix source code. This repo is a work in progress, so we appreciate your patience as we figure things out.\n",
    "\n",
    "## Documentation\n",
    "For documentation about Remix, please\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a550ef5-d429-4002-aba9-dec94af445a2",
   "metadata": {},
   "source": [
    "### Final Answers:\n",
    "\n",
    "* Remix aims to make it easy to use standard APIs.\n",
    "* You can learn about web development in general with Remix.\n",
    "* Remix is a framework that plays the role of both the View and Controller.\n",
    "* Remix leaves the model to the user.\n",
    "* Remix provides helpers for the Fetch API.\n",
    "* Remix can be deployed in non-Node.js environments such as Node.js servers and Cloudflare Workers.\n",
    "\n",
    "### Notes on API usage:\n",
    "\n",
    "Regarding ChatGPT on the web, it is currently still an opt-out format (not used for retraining if you apply) as of March 10, 2023, but the ChatGPI API is an opt-in format (not used for retraining unless you apply), and it has been decided that it will not be used for actual model improvement. (It is stored for 30 days for legal monitoring purposes.) Since execution via the API reduces the risk of leakage to third parties other than OpenAI and is not used for retraining, the threshold for using confidential information with ChatGPT seems to have been lowered. The price of the API (gpt-3.5-turbo) is relatively inexpensive at 0.002 dollars per 1000 tokens, while the embedded model (text-embedding-ada-002) is 0.0004 dollars per 1000 tokens. However, if you try to create an embedded model for a large number of files, it will cost more than expected. If the number of tokens cannot be predicted in advance, it is a good idea to calculate the price in advance and decide whether to execute it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389f84a-c4ab-4a0d-87c9-1bdd17271a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated price: 0.03964988 USD\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "text = ''\n",
    "for doc in get_docs('_posts'):\n",
    "    text += doc.page_content.replace(' ', ' ')\n",
    "token_count = len(encoding.encode(text, allowed_special='all'))\n",
    "print(f'Estimated price: {token_count*0.00000004} USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5902a58-398f-40c6-961f-b957d9ef19b7",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "\n",
    "Since ChatGPT learns using past data, it cannot answer questions about the latest information or information that is not publicly available on the internet. This time, by mixing context related to the question content into the prompt, we were able to answer questions about the latest data and files saved locally.\n",
    "\n",
    "If this article has been even a little helpful to you, I would be delighted. If you have any questions or comments, please feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
