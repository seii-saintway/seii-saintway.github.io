{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f924fac6-fa49-458e-9fde-867a7046d53e",
   "metadata": {},
   "source": [
    "## Open Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93d8ac-1d9b-407b-81d3-da4776c9bd33",
   "metadata": {},
   "source": [
    "### [LangChain Embeddings](https://python.langchain.com/en/latest/reference/modules/embeddings.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f085cb-ecbb-4086-8892-b1b851947e92",
   "metadata": {},
   "source": [
    "#### [LLaMA Embeddings](https://huggingface.co/shalomma/llama-7b-embeddings)\n",
    "* https://github.com/ggerganov/llama.cpp\n",
    "* https://github.com/abetlen/llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e6746-7643-4055-b3d2-7872885adfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed llama-cpp-python-0.1.32 typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install --upgrade llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa465a72-29ab-4a0b-b4c1-41bba4c74ab7",
   "metadata": {},
   "source": [
    "---\n",
    "[GPT4 x Alpaca without LoRA ggml](https://huggingface.co/Pi3141/gpt4-x-alpaca-native-13B-ggml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a952b9-af7e-401e-ac15-07132690fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e016f38-6267-4aa1-8372-8cd9bd76b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.13.3 in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (0.13.3)\n",
      "Requirement already satisfied: protobuf==3.20.* in /usr/local/anaconda3/envs/biobot/lib/python3.10/site-packages (3.20.3)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install tokenizers==0.13.3 protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec79ba-280d-41b4-a7eb-c58c41241a56",
   "metadata": {},
   "source": [
    "---\n",
    "* [HuggingFace Welcome](https://huggingface.co/welcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b24c5b1-2f0c-4f7f-8e62-4faea46d02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('chavinlo/gpt4-x-alpaca')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('chavinlo/gpt4-x-alpaca', resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456fc53-daa3-42dd-9226-904c092a4117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  14 saintway  staff   448B Apr 14 14:21 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x   3 saintway  staff    96B Apr 12 21:08 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:34 \u001b[35madded_tokens.json\u001b[m\u001b[m -> ../../blobs/3e03d5f619baf8592fb936d63d05366f9304f7b2\n",
      "lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:43 \u001b[35mconfig.json\u001b[m\u001b[m -> ../../blobs/849ee4b803bc92eb21e60c3946d20e4cbc69eefa\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 13 02:27 \u001b[35mpytorch_model-00001-of-00006.bin\u001b[m\u001b[m -> ../../blobs/c022dd1d22c5ed2501abdb220f8315e6f51a5197026ed72bdbd2fdbac641d27b\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 13 13:32 \u001b[35mpytorch_model-00002-of-00006.bin\u001b[m\u001b[m -> ../../blobs/5481821b5869b58b15c3175e712e41cd6b7b5596557b10aa2c2655a4cf019a7a\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 14 11:53 \u001b[35mpytorch_model-00003-of-00006.bin\u001b[m\u001b[m -> ../../blobs/df46de31831a882cd57c9beefdad97e1ae442fe071871bad60223b23c1a08df9\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 14 13:21 \u001b[35mpytorch_model-00004-of-00006.bin\u001b[m\u001b[m -> ../../blobs/0e5f42d9943bdbc6e12288733a65d6e337c2cc1a3ff90654cdf96df3f43437ee\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 14 14:06 \u001b[35mpytorch_model-00005-of-00006.bin\u001b[m\u001b[m -> ../../blobs/6149b601c773fce7642e3424878c2c8182a221a2723e93d3da10e0f28850d00e\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 14 14:21 \u001b[35mpytorch_model-00006-of-00006.bin\u001b[m\u001b[m -> ../../blobs/1b02c47b8a6151783c6ab90a8e5acba320940d2197cff255cf8f23eab10f8180\n",
      "lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:43 \u001b[35mpytorch_model.bin.index.json\u001b[m\u001b[m -> ../../blobs/eb488e9b33396741832583081e6ca45eb6f4de49\n",
      "lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:34 \u001b[35mspecial_tokens_map.json\u001b[m\u001b[m -> ../../blobs/318f9131477d72be713dcfee9da3a2e43d7ac8ad\n",
      "lrwxr-xr-x   1 saintway  staff    76B Apr 12 21:34 \u001b[35mtokenizer.model\u001b[m\u001b[m -> ../../blobs/9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
      "lrwxr-xr-x   1 saintway  staff    52B Apr 12 21:08 \u001b[35mtokenizer_config.json\u001b[m\u001b[m -> ../../blobs/8edc6b4c1db134f5d717a6a4f271dfa3194f2295\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah ~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/snapshots/6a571f458cab9a23d14324ec63e0abd1744c8353"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7cc6-3ce9-4de6-916f-142afb99d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 101790056\n",
      "drwxr-xr-x  14 saintway  staff   448B Apr 14 22:10 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x   7 saintway  staff   224B Apr 14 22:10 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--   1 saintway  staff   9.2G Apr 14 13:21 0e5f42d9943bdbc6e12288733a65d6e337c2cc1a3ff90654cdf96df3f43437ee\n",
      "-rw-r--r--   1 saintway  staff   2.3G Apr 14 14:21 1b02c47b8a6151783c6ab90a8e5acba320940d2197cff255cf8f23eab10f8180\n",
      "-rw-r--r--   1 saintway  staff    96B Apr 12 21:34 318f9131477d72be713dcfee9da3a2e43d7ac8ad\n",
      "-rw-r--r--   1 saintway  staff    20B Apr 12 21:34 3e03d5f619baf8592fb936d63d05366f9304f7b2\n",
      "-rw-r--r--   1 saintway  staff   9.3G Apr 14 22:10 5481821b5869b58b15c3175e712e41cd6b7b5596557b10aa2c2655a4cf019a7a\n",
      "-rw-r--r--   1 saintway  staff   9.2G Apr 14 14:06 6149b601c773fce7642e3424878c2c8182a221a2723e93d3da10e0f28850d00e\n",
      "-rw-r--r--   1 saintway  staff   535B Apr 12 21:43 849ee4b803bc92eb21e60c3946d20e4cbc69eefa\n",
      "-rw-r--r--   1 saintway  staff   329B Apr 12 21:08 8edc6b4c1db134f5d717a6a4f271dfa3194f2295\n",
      "-rw-r--r--   1 saintway  staff   488K Apr 12 21:34 9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347\n",
      "-rw-r--r--   1 saintway  staff   9.3G Apr 14 20:53 c022dd1d22c5ed2501abdb220f8315e6f51a5197026ed72bdbd2fdbac641d27b\n",
      "-rw-r--r--   1 saintway  staff   9.3G Apr 14 11:53 df46de31831a882cd57c9beefdad97e1ae442fe071871bad60223b23c1a08df9\n",
      "-rw-r--r--   1 saintway  staff    33K Apr 12 21:43 eb488e9b33396741832583081e6ca45eb6f4de49\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah ~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e8714-85e3-4877-ad16-251aa8ae3726",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3ff1f9-8a4d-4fba-b77b-3e5b9dfd1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "alpaca_embeddings = LlamaCppEmbeddings(model_path=os.path.expanduser('~/.cache/huggingface/hub/models--chavinlo--gpt4-x-alpaca/snapshots/6a571f458cab9a23d14324ec63e0abd1744c8353/model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd5941-56d6-4353-825b-1d062a1c894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备文本\n",
    "text = '这是一个测试文档。'\n",
    "\n",
    "# 使用 HuggingFaceEmbeddings 生成文本嵌入\n",
    "query_result = alpaca_embeddings.embed_query(text)\n",
    "doc_result = alpaca_embeddings.embed_documents([text])\n",
    "\n",
    "print(len(query_result))\n",
    "# print(query_result)\n",
    "\n",
    "print(len(doc_result))\n",
    "print(len(doc_result[0]))\n",
    "# print(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9213262d-662e-4568-be46-e1c6e2d2fe8d",
   "metadata": {},
   "source": [
    "---\n",
    "* [Download files from the HuggingFace Hub](https://huggingface.co/docs/huggingface_hub/guides/download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09986f64-bdee-441a-9f9d-a3aa884c8a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9/consolidated.00.pth'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='Pi3141/gpt4-x-alpaca-native-13B-ggml', filename='consolidated.00.pth', resume_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a588de62-8734-40f2-8f32-a961ef0c2e81",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b54c2c-67e7-42ac-acee-98e99c94eff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 Python 去掉文件中最后一个字节\n",
    "import os\n",
    "with open(os.path.expanduser('~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/blobs/8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55.incomplete'), 'rb+') as filehandle:\n",
    "    filehandle.seek(-1, os.SEEK_END)\n",
    "    filehandle.truncate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2699f1a-3f4f-4c14-b2ef-e1fe0ca2ba8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9/ggml-model-q4_1.bin'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='Pi3141/gpt4-x-alpaca-native-13B-ggml', filename='ggml-model-q4_1.bin', resume_download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1f63-a833-4b85-a122-f2356603c865",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1721d-91de-4157-9ff5-8b1b17f7e05c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ce75465b6d423892caccfaf6661d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/Users/saintway/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id='Pi3141/gpt4-x-alpaca-native-13B-ggml', resume_download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ae684-7b43-487a-ad53-fd89bf4285ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxr-xr-x  6 saintway  staff   192B Apr 14 11:20 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  3 saintway  staff    96B Apr 12 23:05 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "lrwxr-xr-x  1 saintway  staff    52B Apr 12 23:05 \u001b[35m.gitattributes\u001b[m\u001b[m -> ../../blobs/c7d9f3332a950355d5a77d85000f05e6f45435ea\n",
      "lrwxr-xr-x  1 saintway  staff    52B Apr 12 23:05 \u001b[35mREADME.md\u001b[m\u001b[m -> ../../blobs/03dbe88acfdc7f800acf2423960468e1c852c9ba\n",
      "lrwxr-xr-x  1 saintway  staff    76B Apr 14 11:20 \u001b[35mconsolidated.00.pth\u001b[m\u001b[m -> ../../blobs/fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499\n",
      "lrwxr-xr-x  1 saintway  staff    76B Apr 13 09:49 \u001b[35mggml-model-q4_1.bin\u001b[m\u001b[m -> ../../blobs/8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah ~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/snapshots/43cce6aab1b95712d83165afafa3c7baad140eb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04215abc-9f10-4da8-af02-358db3b05320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 90434208\n",
      "drwxr-xr-x  9 saintway  staff   288B Apr 14 11:20 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  6 saintway  staff   192B Apr 14 11:20 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "-rw-r--r--  1 saintway  staff   254B Apr 13 09:53 03dbe88acfdc7f800acf2423960468e1c852c9ba\n",
      "-rw-r--r--@ 1 saintway  staff   9.1G Apr 13 09:53 8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55\n",
      "-rw-r--r--  1 saintway  staff   2.1G Apr 13 09:53 8d308284e190467111257950d4e8b34b1e3f19a70636fa6ea51dfa62f4cf5b55.incomplete.retry\n",
      "-rw-r--r--  1 saintway  staff   1.4K Apr 13 09:53 c7d9f3332a950355d5a77d85000f05e6f45435ea\n",
      "-rw-r--r--  1 saintway  staff    24G Apr 14 11:20 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499\n",
      "-rw-------  1 saintway  staff   6.2G Apr 13 09:53 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499.incomplete.retry\n",
      "-rw-r--r--  1 saintway  staff   1.5G Apr 13 09:53 fd8008066e6af8a094d3703b7e3bbcb64cdca43e964288758d3b3a1ba6e41499.incomplete.tempfile\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls -lah ~/.cache/huggingface/hub/models--Pi3141--gpt4-x-alpaca-native-13B-ggml/blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d7475c-a0bd-488c-8463-3935b95d7081",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543db02f-713c-4a0c-a9ad-9ff618db2206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  =  800.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "alpaca_embeddings = LlamaCppEmbeddings(model_path=os.path.expanduser('~/ggml-model-q4_1.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66957993-048e-4ffb-a2a3-bfd822b1aa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15205.17 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 28693.65 ms /    12 tokens ( 2391.14 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 28701.20 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120\n",
      "1\n",
      "5120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time = 15205.17 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  3616.78 ms /    12 tokens (  301.40 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time =  3628.49 ms\n"
     ]
    }
   ],
   "source": [
    "# 准备文本\n",
    "text = '这是一个测试文档。'\n",
    "\n",
    "# 使用 HuggingFaceInstructEmbeddings 生成文本嵌入\n",
    "query_result = alpaca_embeddings.embed_query(text)\n",
    "doc_result = alpaca_embeddings.embed_documents([text])\n",
    "\n",
    "print(len(query_result))\n",
    "# print(query_result)\n",
    "\n",
    "print(len(doc_result))\n",
    "print(len(doc_result[0]))\n",
    "# print(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d47dcf-3dbc-48ff-97ff-297017c65648",
   "metadata": {},
   "source": [
    "* [LangChain with Hugging Face](https://sj-langchain.readthedocs.io/en/latest/ecosystem/huggingface.html)\n",
    "  * [LLM from Hugging Face Hub](https://sj-langchain.readthedocs.io/en/latest/modules/llms/integrations/huggingface_hub.html)\n",
    "  * [LangChain Text Splitter](https://sj-langchain.readthedocs.io/en/latest/modules/indexes/examples/textsplitter.html)\n",
    "  * [Evaluate Models using HuggingFace Datasets](https://sj-langchain.readthedocs.io/en/latest/use_cases/evaluation/huggingface_datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b73f3-18ee-4330-afc5-23d3bbe19d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb2225-5cba-4370-a021-5de8e0ffa7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(dir_name):\n",
    "    # (1) Import a series of documents.\n",
    "    loader = DirectoryLoader(dir_name, loader_cls=TextLoader, silent_errors=True)\n",
    "    raw_documents = loader.load()\n",
    "    # (2) Split them into small chunks.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=64,\n",
    "    )\n",
    "    return text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be058ddd-30bc-4659-b105-a25cdfb5cc7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_docs('_posts/ultimate-facts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68969f-8b01-43d4-9c87-af91e6821056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='title: Neuroscience\\ndate: 2021-10-14 16:30:20\\ntags: Neuro\\n---\\n\\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\\n\\n---\\n\\n> \\u3000\\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\\n\\u3000\\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\\n\\u3000\\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。', metadata={'source': '_posts/ultimate-facts/Neuroscience.md'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs('_posts/ultimate-facts')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa291bbc-965d-4904-90af-9f2bb5486704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='> ² =>\\n\\u3000\\u3000有关文献包括：\\n* Moore, Philosophical Studies (1922)\\n* Grossmann, \"Are current concepts and methods in neuroscience inadequate for studying the neural basis of consciousness and mental activity?\" (1980)\\n* Baars, A Cognitive Theory of Consciousness (1988)\\n* Baars, \"Surprisingly small subcortical structures are needed for the state of waking consciousness, while cortical projection areas seem to provide perceptual contents of consciousness,\" (1995)\\n* Bogen, \"On the neurophysiology of consciousness: I. An overview,\"(1995a)\\n* Searle, \"The Mystery of Consciousness\", (2000)\\n\\n> ³ =>\\n\\u3000\\u3000冥想的技巧就在于排除万念而只集中于一个想法、观念或者知觉。这要经过多年的修炼，才能遏制注意力的不断转换（第9章），把注意力长时间集中在一件事上而又不昏昏入睡。由于神经的适应性无时不在，对单件事的觉知会逐渐消退，使得脑中一片空白，主观上没有任何意识内容，但人还是清醒的。\\n\\n---', metadata={'source': '_posts/ultimate-facts/Neuroscience.md'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs('_posts/ultimate-facts')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83874be-1f1e-4758-9fe7-dbfd9857cb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\\n> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\\n> 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\\n> ……', metadata={'source': '_posts/ultimate-facts/Neuroscience.md'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs('_posts/ultimate-facts')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078731b7-e941-4496-af2d-550dd1227f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='> ……\\n> ……诚然，可以按照某种任意的尺度使一些数与心理的量值相配列。但是这些量值并没有就归结为某种别的东西，因而彼此仍然是互不联系的。所以，我们不能说就知道了某种东西的性质或本质。这种情况与我们上面谈到的物理学上的例子完全相同。如果我们对“温度”本身的测量仅仅依据任意的标尺来对数进行配列，那么我们仍然没有知道“温度”的性质。但是热力学理论引入分子的平均运动能量来代替温度，同时也就为排除任何任意因素的量的处理方法提供了自然的原则。只有当量的关系不是单纯地反映一种任意的约定，而是从事物的本性中产生并且从事物本性中觉察到的时候，这种关系才真的是代表了一种**本质**的知识。正像温度在这里归结为力学的规定性，同样，意识的材料如果要真正地被认知，一般也必须依据自然的原则归结为物理的规定性。就温度的情况来说（也就是热的客观的质），只有通过物质的分子结构的假设才可能把它归结为力学的规定；同样，对主观的心理的质的知识需要有深入研究大脑过程本质的生理学假设。遗憾的是，这种研究的现状还不容许我们以实现心理学最终目标所需要的精确性来构述这种假设。\\nーー《普通认识论》（Ｍ．石里克），31', metadata={'source': '_posts/ultimate-facts/Neuroscience.md'})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_docs('_posts/ultimate-facts')[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e24359-0164-454a-a1a1-3a9ee2eb1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores.faiss import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad73e7d-a0f5-4865-b6e5-9c524f579422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1234747.80 ms /   607 tokens ( 2034.18 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1234897.68 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1014295.96 ms /   384 tokens ( 2641.40 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1014467.79 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 3282788.17 ms /  1245 tokens ( 2636.78 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 3283401.24 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1918145.27 ms /   648 tokens ( 2960.10 ms per token)\n",
      "llama_print_timings:        eval time = 20809.58 ms /     1 runs   (20809.58 ms per run)\n",
      "llama_print_timings:       total time = 1939184.53 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 96082.05 ms /    30 tokens ( 3202.73 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 96150.74 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 507646.90 ms /   208 tokens ( 2440.61 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 507789.71 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 276020.03 ms /   115 tokens ( 2400.17 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 276108.72 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2019785.65 ms /   672 tokens ( 3005.63 ms per token)\n",
      "llama_print_timings:        eval time = 21867.42 ms /     1 runs   (21867.42 ms per run)\n",
      "llama_print_timings:       total time = 2041848.65 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 3602106.63 ms /  1131 tokens ( 3184.89 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 3602439.90 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 3230592.03 ms /  1040 tokens ( 3106.34 ms per token)\n",
      "llama_print_timings:        eval time = 22766.44 ms /     1 runs   (22766.44 ms per run)\n",
      "llama_print_timings:       total time = 3253751.32 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2692559.48 ms /  1530 tokens ( 1759.84 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2692893.27 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2117241.21 ms /  1428 tokens ( 1482.66 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2117414.25 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 1295672.21 ms /  1255 tokens ( 1032.41 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 1295802.91 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2584795.09 ms /  1406 tokens ( 1838.40 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2585014.00 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2833173.90 ms /  1514 tokens ( 1871.32 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2833446.52 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2497129.27 ms /  1459 tokens ( 1711.53 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2497330.28 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2323893.79 ms /  1448 tokens ( 1604.90 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2324101.06 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 424718.68 ms /   549 tokens (  773.62 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 424798.69 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2024465.75 ms /  1456 tokens ( 1390.43 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2024680.15 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 203939.74 ms /   407 tokens (  501.08 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 204026.72 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 2015409.22 ms /  1524 tokens ( 1322.45 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 2015592.15 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 147731.28 ms /   397 tokens (  372.12 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 147780.64 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 477176.94 ms /  1092 tokens (  436.98 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 477262.18 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 900018.45 ms /  1332 tokens (  675.69 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 900122.17 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 133290.49 ms /   490 tokens (  272.02 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 133351.61 ms\n",
      "\n",
      "llama_print_timings:        load time = 12248.83 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time = 13613.44 ms /    53 tokens (  256.86 ms per token)\n",
      "llama_print_timings:        eval time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings:       total time = 13634.46 ms\n"
     ]
    }
   ],
   "source": [
    "def ingest_docs(dir_name):\n",
    "    documents = get_docs(dir_name)\n",
    "    # (3) Create embeddings for each document (using text-embedding-ada-002).\n",
    "    embeddings = LlamaCppEmbeddings(model_path=os.path.expanduser('~/ggml-model-q4_1.bin'), n_ctx=2048)\n",
    "    return FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "vectorstore = ingest_docs('_posts/ultimate-facts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c638f5-b7f0-4243-8454-17ea29339de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbf012-db9b-4ae4-ab38-35470ba75b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore\n",
    "with open('vectorstore_13B_2048.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1edad5-f757-4bea-937c-922243959c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /Users/saintway/ggml-model-q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v1 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: f16        = 3\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =  73.73 KB\n",
      "llama_model_load_internal: mem required  = 11359.03 MB (+ 3216.00 MB per state)\n",
      "llama_init_from_file: kv self size  = 3200.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Load vectorstore\n",
    "with open('vectorstore_13B_2048.pkl', 'rb') as f:\n",
    "    vectorstore = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9112f6-deb9-4f61-a310-19a2130529dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '你知道什么？'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b9f77-dc01-455a-893c-da4368535d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='title: Neuroscience\\ndate: 2021-10-14 16:30:20\\ntags: Neuro\\n---\\n\\nThe [**ventral tegmental area**](https://en.wikipedia.org/wiki/Ventral_tegmental_area) (**VTA**) (**tegmentum** is Latin for covering), also known as the **ventral tegmental area of Tsai**, or simply **ventral tegmentum**, is a group of neurons located close to the midline on the floor of the midbrain.\\n\\n---\\n\\n> \\u3000\\u3000有些权威认为，有必要把意识的内容 (content) 与“有意识状态的特性” (quality of being conscious) 或“意识本身” (consciousness as such) 区分开来²。这一划分与我的分类异曲同工。\\n\\u3000\\u3000要想产生意识，必须先具备某些神经前提条件。我把这些条件称为 NCC_e。任一特定知觉的 NCC 都是局部作用的、高度特化的、转瞬即逝的，相比起来，NCC_e 的作用方式更全局化也更持久。要是没有相关的 NCC_e 的话，机体或许也还能有简单的行为，但在这样做时绝不会有意识（可能发生这种情形的某些病理条件将在第13章讨论）。根据定义可知，如果没有 NCC_e，就不可能形成任何 NCC。\\n\\u3000\\u3000会不会有这样一种状态，即生物体虽然有意识，却意识不到任何具体内容？换句话说，NCC_e 能否脱离 NCC 而单独存在呢？某些冥想的目标就是要进入这种没有具体内容的意识形式³。但是在目前，还很难对它进行严格的分析。' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='有意注意，是指，对于某次效果的注意。\\n无意注意，是指，对于某次非效果的注意。\\n\\n目标，是指，对于某种效果的某些次记忆所联结着的对于此种效果的拟构。\\n意向，是指，对于某些种效果的某些次记忆所联结着的对于某种效果的拟构。\\n\\n懊悔，是指，对于某次弊害效果的某次记忆、对于某次功效的某次记忆所联结着的对于某次功效的拟构。\\n焦虑，是指，对于某次弊害效果的某次记忆、对于某次功效的某次意向所联结着的对于某次弊害效果的拟构。\\n\\n对于某次功效的目标，联结着，对于此次功效的原因。\\n对于某种功效的概括，联结着，对于此种功效的原因。\\n\\n兴趣，是指，联结着某次快乐的识。\\n荒诞，是指，联结着某次乏味的识。\\n苦毒，是指，联结着某次痛苦的识。\\n\\n慾望，是指，对于某次兴趣的表征。\\n妄想，是指，对于某次荒诞的表征。？\\n苦观，是指，对于某次苦毒的表征。\\n\\n苦观，分为，记忆苦观、拟构苦观。弊害，…、…\\n\\n有趣注意，是指，对于某次兴趣的注意。\\n无趣注意，是指，对于某次荒诞的注意。\\n\\n意义，是指，值得的注意。\\n神圣，是指，极其丰富的意义。\\n积极的态度，是指，充满对于某种意义的信心。\\n消极的态度，是指，缺乏对于某种意义的信心。\\n积极的注意，导致着，快乐。\\n消极的注意，导致着，乏味。\\n对于某种意义的怀疑，是指，对于某种意义的信心的减弱。\\n对于某种意义的确定，是指，对于某种意义的信心的增强。\\n对于某种意义的静思，是指，对于某种意义的减弱。对于某种意义的静思，导致着，忧郁。\\n对于某种意义的禅修，是指，对于某种意义的增强。对于某种意义的禅修，导致着，幸福。\\n静思、禅修、祷告，都是，某种定觉练习。\\n\\n---\\n\\n> 因为我们得了救是因着盼望。只是所盼望的若已得看见，便不是盼望了；因为人所看见的、他何必还盼望呢？但我们若盼望所未看见的，就必坚忍切候着。\\n(罗马书 8:24-25 吕振中)\\n\\n> 所以青春性的私欲、你总要逃避；你要跟那些用洁净心呼求主的人一同追求正义、忠信、仁爱、和平。\\n(提摩太后书 2:22 吕振中)\\n\\n向内往最深处去：净心、呼求主名、并且、等待回应。' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n",
      "page_content='> 我们刚刚知道自然科学借以掌握质的方法––形成量的概念的方法。我们必须提出的问题是，这种方法是不是也能够适用于主观的意识的质。按照我们前面所说，为了使这种方法能够加以运用，必须有与这些质充分确定地、唯一地联系着的空间变化。如果情况真的如此，那么这个问题就可以通过空间–时间的重合方法来解决，因而**测量**便是可能的。但是，这种重合的方法本质上就是进行物理的观察，而就内省法来说，却不存在物理的观察这种事情。由此立刻就可以得出结论：心理学沿着内省的途径决不可能达到知识的理想。因此，它必须尽量使用物理的观察方法来达到它的目的。但这是不是可能的呢？是不是有依存于意识的质的空间变化，就像例如在光学中干涉带的宽度依存于颜色，在电学中磁铁的偏转度依存于磁场的强度那样呢？\\n> 现在我们知道，事实上应当承认在主观的质和推断出来的客观世界之间有一种确切规定的、一义的配列关系。大量的经验材料告诉我们，我们可以发现，至少必须假设与所有经验唯一地联系着的“物理的”过程的存在。没有什么意识的质不可能受到作用于身体的力的影响。的确，我们甚至能够用一种简单的物理方法，例如吸进一种气体，就把意识全部消除掉。我们的行动与我们的意志经验相联系，幻觉与身体的疲惫相联系，抑郁症的发作与消化的紊乱相联系。为了研究这类相互联系，心的理论必须抛弃纯粹内省的方法而成为**生理的**心理学。只有这个学科才能在理论上达到对心理的东西的完全的知识。借助于这样一种心理学，我们就可以用概念和所与的主观的质相配列，正如我们能够用概念与推论出来的客观的质相配列一样。这样，主观的质就像客观的质一样成为可知的了。\\n> 我们很早就指出，客观世界中最直接地与自我的主观的质相联系的部分就是由大脑的概念，特别是大脑皮层的概念所表示的那一部分。因而在科学知识的精确的世界图景中，可用数值描述的概念代替的主观质的，只是某些大脑过程。相互依存的分析不可避免要引向这些大脑过程。虽然我们还远没有确切地知道所涉及的是何种个别的过程，但至少指出了一条途径：必须以大脑过程来代替主观的质。这就是我们能够充分认识主观的质所具有的唯一的希望。\\n> ……' metadata={'source': '_posts/ultimate-facts/Neuroscience.md'}\n",
      "\n",
      "page_content='客体方式，导致着、联结着，主体方式、机体状态\\n形体，导致着、联结着，身体、快乐、痛苦\\n轻蔑、轻视他人对自己的态度，损害着，羞耻心\\n羞耻，对于亲密程度的重视；我们在争辩的时候，真正损害着羞耻心的，实际上是，轻视他人对自己的态度，而不是，轻视他人的（由父所创造的）信念？\\n羞耻、光荣，重视他人对自己的态度、敬重\\n恥辱、傲慢，轻视他人对自己的态度、轻蔑\\n羞耻、羞辱，在含义上，有所不同吗？\\n单方的轻视、双方的轻视？\\n一方，是，非吾所显明出来的罪；一方，是，吾所显明出来的罪。\\n狭隘、愚蠢、固执，轻视他人的信念\\n开明、智慧、变通，重视他人的信念' metadata={'source': '_posts/ultimate-facts/终极真实.md'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  6111.23 ms\n",
      "llama_print_timings:      sample time =     0.00 ms /     1 runs   (    0.00 ms per run)\n",
      "llama_print_timings: prompt eval time =  6109.85 ms /     8 tokens (  763.73 ms per token)\n",
      "llama_print_timings:        eval time = 10089.46 ms /     1 runs   (10089.46 ms per run)\n",
      "llama_print_timings:       total time = 16205.01 ms\n"
     ]
    }
   ],
   "source": [
    "# Get context related to the question from the embedding model\n",
    "for context in vectorstore.similarity_search(question):\n",
    "    print(f'{context}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
